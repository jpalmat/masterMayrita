{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, KFold\n",
    "from sklearn import preprocessing, svm\n",
    "from evaluacion_funciones import *\n",
    "\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos.\n",
    "datos = load_iris()\n",
    "X = datos.data\n",
    "y = datos.target\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de evaluación.\n",
    "metricas = {\n",
    "  'ACC':    metrics.accuracy_score,\n",
    "  'PREC':   lambda y_true, y_pred:\n",
    "            metrics.precision_score(y_true, y_pred,\n",
    "              average='micro'),\n",
    "  'RECALL': lambda y_true, y_pred:\n",
    "            metrics.recall_score(y_true, y_pred,\n",
    "              average='micro'),\n",
    "  'F1':     lambda y_true, y_pred:\n",
    "            metrics.f1_score(y_true, y_pred, average='micro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Partición de datos externa\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(np.shape(X_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- TRAINING ---------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Estandarización de los datos de entrenamiento\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "stdr_trained = standardizer.fit(X_training)\n",
    "X_stdr = stdr_trained.transform(X_training)\n",
    "# print(X_stdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Selección de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Construcción del algoritmo de aprendizaje.\n",
    "algoritmos = {'SVM': svm.SVC(C=10, random_state=42, probability=True),\n",
    "             'DUMMY': DummyClassifier(strategy='stratified', random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1) Validación cruzada interna y Optimización de los hiperparámetros\n",
    "# y_pred = {}\n",
    "for nombre, alg in algoritmos.items():\n",
    "#     y_pred[nombre] = cross_val_predict(alg, X_stdr, y_training, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
    "#     results = evaluacion(y_training, y_pred[nombre], metricas)\n",
    "#     print(metrics.confusion_matrix(y_training, y_pred[nombre]))\n",
    "\n",
    "    results = cross_val_score(alg, X_stdr, y_training, cv = KFold(n_splits=10, shuffle=True, random_state=42))\n",
    "    print(\"Accuracy:   %0.4f +/- %0.4f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2) Entrenamiento del modelo definitivo\n",
    "models = {}\n",
    "for nombre, alg in algoritmos.items():\n",
    "    models[nombre] = alg.fit(X_stdr, y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- PREDICTION ---------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Extracción de las características de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Estandarización de las característiacs de test\n",
    "X_test_stdr = stdr_trained.transform(X_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Selección de los atributos de test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Predicción del conjunto de test\n",
    "y_pred = {}\n",
    "for nombre, alg in models.items():\n",
    "    y_pred[nombre] = alg.predict(X_test_stdr)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Evaluación del modelo sobre el conjunto de test\n",
    "for nombre, alg in models.items():\n",
    "    print(metrics.confusion_matrix(y_testing, y_pred[nombre]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos la curva ROC\n",
    "y_proba_svm = models['SVM'].predict_proba(X_test_stdr) # \"predict_proba\" para extraer probabilidades vez de predicciones\n",
    "y_test_bin = preprocessing.label_binarize(y_testing, classes=[0,1,2]) # Usar \"label_binarize\" en el caso de problemas multiclase\n",
    "auc_svm = metrics.roc_auc_score(y_testing, y_proba_svm, multi_class='ovr') # Area Under the ROC curve (AUC)\n",
    "fpr_svm, tpr_svm, th_svm = metrics.roc_curve(y_test_bin[:,1], y_proba_svm[:,1])\n",
    "\n",
    "y_proba_dummy = models['DUMMY'].predict_proba(X_test_stdr) # \"predict_proba\" para extraer probabilidades vez de predicciones\n",
    "auc_dummy = metrics.roc_auc_score(y_testing, y_proba_dummy, multi_class='ovr') # Area Under the ROC curve (AUC)\n",
    "fpr_dummy, tpr_dummy, th = metrics.roc_curve(y_test_bin[:,1], y_proba_dummy[:,1])\n",
    "\n",
    "plt.plot(fpr_svm, tpr_svm)\n",
    "plt.plot(fpr_dummy, tpr_dummy)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('AUC-SVM = ' + str(np.round(auc_svm,4)) + '   AUC-DUMMY = ' + str(np.round(auc_dummy,4)))\n",
    "plt.legend(['SVM', 'DUMMY'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla de resultados\n",
    "from tabulate import tabulate\n",
    "headers = ['','SVM', 'Dummy']\n",
    "P,S,FS,ACC = ['Precision'], ['Recall'], ['F1-score'], ['Accuracy']\n",
    "\n",
    "for nombre, alg in models.items():\n",
    "    P.append(np.round(metrics.precision_score(y_testing, y_pred[nombre], average='macro'),4))\n",
    "    S.append(np.round(metrics.recall_score(y_testing, y_pred[nombre], average='macro'),4))\n",
    "    FS.append(np.round(metrics.f1_score(y_testing, y_pred[nombre], average='macro'),4))\n",
    "    ACC.append(np.round(metrics.accuracy_score(y_testing, y_pred[nombre]),4))\n",
    "\n",
    "my_data = [tuple(P), tuple(S), tuple(FS), tuple(ACC)]\n",
    "print(tabulate(my_data, headers=headers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
