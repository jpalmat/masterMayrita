{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/viu_logo.png\" width=\"200\">\n",
    "\n",
    "# 07MAIR - Redes Neuronales y Deep Learning\n",
    "## Clase 01b: Introducción a Redes Neuronales\n",
    "<img src=\"img/keras_logo.jpg\" width=\"200\">\n",
    "\n",
    "### Profesor: Adrián Colomer Granero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sumario\n",
    "- Perceptrón\n",
    "- Perceptrón multicapa\n",
    "- Ejemplo redes AND y XOR\n",
    "- Entrenamiento con Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptrón Simple\n",
    "<img src=\"img/perceptron.png\" width=\"500\">\n",
    "\n",
    "- Propuesto por Rosenblatt 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametros: \n",
    "- Entrenables, determinan el valor output de la neurona (input*weight + bias): \n",
    " - Bias por cada neurona\n",
    " - Weight por cada conexión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A elegir:\n",
    " - Función de activación (cómo se calcula el output en función del input) -> Hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes neuronales (Perceptrón simple)\n",
    "- Capas de perceptrones, feedforward connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Red neuronal AND\n",
    "- Dos neuronas\n",
    "- Output 1 si ambas reciben input 1\n",
    "\n",
    "<img src=\"img/tabla_and.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Función de activación: signo (1 if > 0, else 0)\n",
    "\n",
    "<img src=\"img/xor_single_layer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W1 = 1, W2 = 1\n",
    "\n",
    "b = -1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Red neuronal XOR\n",
    "<img src=\"img/tabla_xor.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Función de activación: signo (1 if > 0, else 0)\n",
    "\n",
    "<img src=\"img/xor_single_layer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "¡No tiene solución! ¿Y si añadimos una segunda capa oculta? -> Perceptrón Multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/xor_multi_layer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Ejercicio\n",
    "- Averiguar los valores de W1,...,W6 y b1,...,b3 adecuados para XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenando ANNs\n",
    "- __Backpropagation__ en general, descenso del gradiente para minimizar la diferencia entre lo esperado (training label) y lo actual (prediccción de la red en un instante dado).\n",
    "- Inicialización con parámetros aleatorios (https://keras.io/api/layers/initializers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/gradient_descent.jpeg\" width=\"400\">\n",
    "\n",
    "Fuente: Menon, 2018. URL: https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training loop\n",
    "- Escoger X _training samples_ y obtener predicciones\n",
    "- Evaluar la función de pérdida\n",
    "- Utilizar el error para modificar los parámetros W y b para minimizar el error\n",
    "- Parar cuando se llegue a un error aceptable, o tras un numero de iteraciones (épocas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estrategias para actualizar los valores\n",
    "- Naive: modificar uno a uno cada parámetro y ver si disminuye o aumenta el error. Ineficiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Diferenciar la capa con respecto a cada parámetro.\n",
    "  - Se puede computar el gradiente de la perdida (derivada) y modificar el parametro en la direccion contraria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - La diferenciación de las primeras capas depende de las ultimas. Usando la regla de la cadena de cálculo, f(g(x)) == f'(g(x)) * g'(x), el cálculo del gradiente comienza al final de la NN y va hacia atrás (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tipos de descenso del gradiente\n",
    "- Stochastic gradient descent: batch size 1\n",
    "- Mini-batch SGD: batch size > 1 < tamaño del training set\n",
    "- Batch SGD:  batch size == tamaño del training set"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
