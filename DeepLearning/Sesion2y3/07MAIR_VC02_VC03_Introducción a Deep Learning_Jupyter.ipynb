{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/viu_logo.png\" width=\"200\">\n",
    "\n",
    "# 07MAIR - Redes Neuronales y Deep Learning\n",
    "## Clase 02: Redes neuronales artificiales\n",
    "<img src=\"img/keras_logo.jpg\" width=\"200\">\n",
    "\n",
    "### Profesores: Adrián Colomer Granero / Gabriel Enrique Muñoz Ríos\n",
    "### Autor: Carlos Fernández Musoles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sumario\n",
    "- Funciones de activación\n",
    "- Visualización con Tensorflow playground\n",
    "- Ejemplo mnist keras\n",
    "- Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/xor_multi_layer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/xor_multi_layer_solucion.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Diferencias en batch size?\n",
    "- Batch size = 1\n",
    "- Tamaño del training set > Batch size > 1\n",
    "- Batch size = Tamaño del training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Batch size = 1 \n",
    " - Mucho sesgo en la actualización de pesos -> Outliers  \n",
    " - Tantas actualizaciones de pesos como muestras en el trainig set -> Convergencia más lenta (época lenta)\n",
    " - Poca carga en memoria ya que no se almacenan en RAM conjuntos de muestras ni errores por muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Batch size = Tamaño del training set\n",
    " - Una única actualización de pesos retropropagando el error medio de todas las muestras\n",
    " - Necesidad de establecer mayor número de épocas para garantizar convergencia\n",
    " - La época es más rápida ya que solo hay un paso por época\n",
    " - La RAM debe almacenar todas las muestras y errores asociados -> No es permisible en datasets grandes (OOM!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tamaño del training set > Batch size > 1\n",
    " - Solución de compromiso entre las dos anteriores\n",
    " - Tamaño de batch suele establecerse como potencia de dos\n",
    " - Pasos por época = int(Tamaño del dataset/Batch size)\n",
    " - Valor exacto del hiperparámetro: ¿arte? -> Ciencia empírica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hiperparámetros en el entrenamiento de ANNs\n",
    "\n",
    "- Época: Una pasada de todos las training samples\n",
    "- Batch size: Agrupación de training samples considerados para hacer una actualización de los parámetros de la red\n",
    "- Learning rate: Magnitud del cambio en cada actualización\n",
    "- Función de activación: Introduce las no linealidades en el esquema de red neuronal\n",
    "- Función de pérdida: Error entre lo esperado (etiqueta, ground truth) y lo predicho en la época actual\n",
    "- Topología de la red neuronal: Número de capas ocultas y unidades/neuronas por capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tradeoffs: \n",
    "#### Batch size \n",
    " - Alto: Lento en entrenar y mucha memoria RAM consumida, problemático en datasets de grandes dimensiones \n",
    " - Bajo: Poca memoria consumida, errático en bajar el error (mayor sesgo)\n",
    " \n",
    "#### Learning rate\n",
    " - Alto: Problemas para alcanzar minimos globales\n",
    " - Bajo: Lentitud y posible estancamiento en minimo local\n",
    " \n",
    "VIsualización de entrenamiento con TensorFlow playground https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Función de activación \n",
    "\n",
    "Muchas disponibles: https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Umbral\n",
    "<img src=\"img/stepfunc.png\" width=\"300\">\n",
    "\n",
    "Fuente: Wikipedia https://en.wikipedia.org/wiki/Step_function\n",
    "\n",
    "- Input > umbral = 1, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Difícil convergencia durante entrenamiento\n",
    "- Para multiclase, cómo decidir el output correcto (tres 1 y un 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear\n",
    "\n",
    "<img src=\"img/linear.png\" width=\"300\">\n",
    "\n",
    "Fuente: Wikipedia https://en.wikipedia.org/wiki/Activation_function \n",
    "\n",
    "- Input - output linear\n",
    "- Output -inf a +inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combinación de funciones lineales da una funcion lineal\n",
    "- Explosión de valores (dificil entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sigmoide\n",
    "\n",
    "<img src=\"img/sigmoid.png\" width=\"300\">\n",
    "\n",
    "- No linear\n",
    "- Output 0 a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Problema: entrenamiento puede ser lento cuando los valores residen en los extremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tangencial\n",
    "\n",
    "<img src=\"img/tanh.png\" width=\"300\">\n",
    "\n",
    "Fuente: Wolfram https://reference.wolfram.com/language/ref/Tanh.html\n",
    "\n",
    "- No linear\n",
    "- Output -1 a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradiente más fuerte que sigmoide (derivadas son más intensas, por lo que la convergencia puede dificultarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "<img src=\"img/relu.png\" width=\"300\">\n",
    "\n",
    "Fuente: Wikipedia https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "- No linear\n",
    "- Output 0 a +inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cualquier función puede aproximarse con una combinacion de ReLUs\n",
    "- Aunque puede explotar (lado positivo) es más eficiente (menos calculos) que sigmoid o tanh\n",
    " - Sigmoid / tanh el cálculo de la función de activacion es más costoso\n",
    " - Con ReLU, muchas serán 0 (sin calculo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### La no linearidad de la funcion de activación es lo que permite a las ANNs aprender decision boundaries que no son lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variaciones de SGD\n",
    "- Rmsprop: dividir gradiente entre la media de las magnitudes recientes \n",
    "- Momentum: la magnitud del cambio al actualizar parametros depende del gradiente actual y del cambio anterior. velocity = past_vel * momentum + learning_rate * gradient\n",
    "- Otros disponibles en Keras https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instalacion de Keras\n",
    "- Instalacion backend (CPU, GPU)\n",
    "```python\n",
    "pip install tensorflow\n",
    "```\n",
    "```python\n",
    "pip install tensorflow-gpu\n",
    "```\n",
    "- Mas información acerca de la instalación:\n",
    " \n",
    " https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-ubuntu/\n",
    " \n",
    " https://www.tensorflow.org/install/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nuestra primera red neuronal: MNIST dataset\n",
    "Keras y el dataset MNIST para el reconocimiento de digitos escritos a mano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP aplicado a texto: Ejemplo REUTERS\n",
    "Keras y el dataset MNIST para la clasificación de reseñas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot encoding palabras\n",
    "<img src=\"img/hot_encoding.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "Comparar los resultados con otros modelos:\n",
    "- Modificar la arquitectura de la red (numero de neuronas, numero de hidden layers)\n",
    "- Demostrar que con menos hidden units se pierde información que no se puede recuperar\n",
    "- Experimentar con 'tanh' en vez de 'relu' como función de activación\n",
    "- Distinto split training / validation (mayoría validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizacion de redes neuronales\n",
    "- Preprocesado para ANN: \n",
    " - Vectorizacion del input (tensores)\n",
    " - Normalizacion de valores para acelerar entrenamiento (todas las features deben tener misma escala), media 0 y std 1\n",
    " - Valores perdidos: Eliminar, interpolar o utilizar 0 (si 0 no tiene significado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Feature engineering (menos importante en deep learning):\n",
    " - Feature cross\n",
    " - Creacion de features basadas en el dominio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Regularización: \n",
    " - Weight regularization\n",
    " - Dropout\n",
    " - Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight regularization\n",
    "\n",
    "- El modelo más sencillo (menos parametros) que explique los datos es el preferido (Navaja de Ockham)\n",
    "- Lidiar con el problema de overfitting es la tarea más complicada en el paradigma del aprendizaje profundo:\n",
    " - Hace que los parametros tengan valores muy pequeños (coste a cada solución dependiente de la magnitud de los parametros)\n",
    " - Coste proporcional al valor absoluto (L1) o proporcional a la raiz cuadrada del valor (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n",
    "- Desactivar neuronas aleatoriamente (i.e. poner a 0 en output features de una capa)\n",
    "- Dropout rate determina la proporcion de neuronas a desactivar\n",
    "- Funciona al eliminar patrones espurios (evita overfitting)\n",
    "<img src=\"img/dropout.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch normalization\n",
    "\n",
    "- Resta la media y divide entre la desviación típica de los datos del batch -> Estandarización de features\n",
    "<img src=\"img/batch_norm.png\" width=\"600\">\n",
    "- Se suele situar en la salida de la capa (tras la función de activación)\n",
    "- Mejora el entrenamiento de la red -> Mayor eficiencia computacional\n",
    "- Permite el uso de valores de learning rate más altos\n",
    "- Se puede considerar como un método de regularización aunque no es su principal ventaja. Dropout y L1/L2 aportan mayor regularización\n",
    "- Existen variantes como Layer normalization, Instance normalization o Group normalization (CNNs)\n",
    "<img src=\"img/batch_norm_cnn.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización en un MLP: L1/L2, Dropout y Batch Normalization\n",
    "Keras y el dataset Reuters para demostrar el uso de regularización en deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Función de pérdida / Función de activación (Tabla resumen)\n",
    "\n",
    "- Dependiente del problema\n",
    " - Clasificación binaria: activación ultima capa (sigmoide), función de perdida (binary_crossentropy)\n",
    " - Clasificacion multiclase: activación ultima capa (softmax), función de perdida (categorical_crossentropy)\n",
    " - Regresión con valores arbitrarios: activación ultima capa (none), función de perdida (mse)\n",
    " - Regresión valores 0 a 1: activación ultima capa (sigmoide), función de perdida (mse o binary_crossentropy)\n",
    " \n",
    " Información en Keras https://keras.io/losses/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
