{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GANs - 1. Mi primera GAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO6/Xmlcmlehsrl9N65MJXt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2XsGrf4SGApQ"},"source":["**PRÁCTICA**: implementar una GAN que genere parejas de números que cumplan (x, x^2).\n","\n","**NOTA**: escoger TPU."]},{"cell_type":"markdown","metadata":{"id":"qzapMYChWU-E"},"source":["# Mi primera GAN\n","\n","En este Notebook vamos a implementar nuestra primera GAN. Recordad la arquitectura:\n","\n","<center><img src=\"https://miro.medium.com/max/2426/1*XKanAdkjQbg1eDDMF2-4ow.png\"></center>\n","\n","Como ya hemos visto, una GAN consta de un generador (*el falsificador)* y un discriminador (*el policía*).\n","\n","Vamos a comenzar con un ejemplo muy sencillo que iremos complicando conforme avancemos. En esta ocasión no vamos a crear dinero, sino que vamos a generar nuevas muestras que encajen en la función $x^2$. Las muestras reales (nuestro dataset) son las que véis dibujadas en la gráfica siguiente:\n","\n","<center><img src=\"http://mathcentral.uregina.ca/QQ/database/QQ.09.06/mike1.1.gif\"></center>\n","\n","Así que lo primero que vamos a hacer es crear una función que nos permita generar dichas muestras:"]},{"cell_type":"code","metadata":{"id":"LI0-C8N-Y4hF"},"source":["# importamos lo que vamos a necesitar\n","import numpy as np\n","from numpy.random import rand, randn\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blfu5NVyYBH7"},"source":["# generamos muestras reales con sus correspondientes etiquetas (1)\n","def generate_real_samples(n):\n","    # vamos a generar datos entre -0.5 y 0.5\n","    X1 = rand(n) - 0.5\n","    # nuestros datos son x^2\n","    X2 = X1 * X1\n","    # stack arrays\n","    X1 = X1.reshape(n, 1)\n","    X2 = X2.reshape(n, 1)\n","    X = np.hstack((X1, X2))\n","    # generate class labels\n","    y = np.ones((n, 1))\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMRAW2QRY7c6"},"source":["# generamos nuestro dataset\n","X, y = generate_real_samples(n=100)\n","# plot samples\n","plt.scatter(X[:, 0], X[:, 1])\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdbPpUXUZ0gN"},"source":["Nuestra meta en este caso es ser capaces de generar nuevas parejas de datos $(x, x^2)$ lo más similares posibles a los que generaría la función $f(x)=x^2, x \\in [-0.5, 0.5]$.\n","\n","Vamos a crear primero nuestro discriminador. Lo primero es pensar cuales van a ser las entradas y cuales las salidas. Veamos:\n","\n","Las entradas serán parejas $(x, x^2)$ entre -0.5 y 0.5. Por ejemplo: $(0.2, 0.04)$, $(0.4, 0.16)$, $(-0.2, 0.04)$, $(-0.4, -0.16)$, etc.\n","\n","La salida será un valor binario: 1 si la pareja $(x, x^2)$ realmente se parece a la función $f(x)=x^2$, 0 en caso contrario.\n","\n","Vamos a definirlo:"]},{"cell_type":"code","metadata":{"id":"8idxYnPYa5_y"},"source":["# definimos el discriminador\n","def define_discriminator(n_inputs=2):\n","    model = Sequential()\n","    # Nuestro discriminador va a ser muy sencillo: va a tener 2 capas densas.\n","    # la primera, tendrá 25 neuronas, activación relu e inicialización 'he_uniform' (https://keras.io/initializers/)\n","    # además, tendremos que indicarle las dimensiones de entrada (con el atributo input_dim)\n","    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n","    # la segunda, será una densa de 1 única neurona y activación sigmoide: \n","    # queremos que nos de la probabilidad de que la imagen sea real\n","    model.add(Dense(1, activation='sigmoid'))\n","    # utilizamos función de activación final sigmoide y la cross entropía\n","    # binaria para que la red nos devuelva las probabilidades de que la entrada\n","    # sea falsa y real\n","    # el optimizador que usaremos en este caso es el adam con los parámetros por defecto\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Qt1L-2ybrma"},"source":["Perfecto, ya lo tenemos.\n","\n","Ahora necesitamos el generador. ¿Qué es lo que necesitamos que haga el generador? Que genere 2 números que vamos a intentar aproximar a $(x, x^2)$ mediante entrenamiento.\n","\n","Vamos a ello:"]},{"cell_type":"code","metadata":{"id":"uuXmHlE7b7kS"},"source":["# definimos el generador\n","def define_generator(latent_dim, n_outputs=2):\n","    # fijaos en varios detalles:\n","    # - latent_dim: es el tamaño del vector de ruido de la entrada. \n","    # Cuanto mayor sea, más complejos serán los datos que podremos generar\n","    # - la función de activación final ha de ser lineal: esto es muy importante, \n","    # ya que estamos haciendo un trabajo de regresión (estamos tratando de\n","    # llegar a $(x, x^2)$ desde el valor introducido a la red (el vector latente)\n","    # - no compilamos el modelo: esto se debe a que durante el entrenamiento \n","    # del G(z) vamos a necesitar acoplar G(z) a D(x)\n","\n","    # vamos a definir un modelo haciendo uso del modelo Sequential de keras\n","    model = Sequential()\n","    # le añadiremos una capa densa de 15 neuronas, con activación relu e\n","    # inicialización de pesos 'he_uniform', y tendremos que indicarle las \n","    # dimensiones de entrada, igual que en el caso del discriminador\n","    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim)) \n","    # la última capa tendrá tantas neuronas como números esperamos generar\n","    # (en este caso x y x^2) y activación lineal\n","    model.add(Dense(n_outputs, activation='linear'))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbRHRFr9dEgH"},"source":["Generador definido. Ahora vamos a combinar el generador y el discriminador para crear el modelo Generativo Adversarial:"]},{"cell_type":"code","metadata":{"id":"wAt6NsGVdMWX"},"source":["# definimos el modelo GAN combinando generador y discriminador, para entrenar el generador\n","def define_gan(generator, discriminator):\n","    # recordad que durante el entrenamiento del generador necesitamos al\n","    # discriminador para que nos diga si la muestra es real o no, pero\n","    # no debemos actualizar los pesos del discriminador, ya que si no,\n","    # estaríamos haciendo trampas: estaríamos haciendo peor al D(x)\n","    # Así que congelamos el discriminador:\n","    discriminator.trainable = False\n","    # ahora conectamos el G(z) al D(x)\n","    model = Sequential()\n","    # añadimos el generador primero: él es el encargado de generar una muestra\n","    # a partir del espacio latente\n","    model.add(generator)\n","    # y el discriminador después: le introducimos la muestra generada por el \n","    # G(z) para que nos diga si cree que es real o fake\n","    model.add(discriminator)\n","    # y ahora sí, compilamos el modelo con optimizador adam y función de\n","    # pérdidas binary crossentropy\n","    model.compile(loss='binary_crossentropy', optimizer='adam')\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0kx9ocDfd2eO"},"source":["Bueno, ya lo tenemos todo, ¿no?\n","\n","Pues casi, pero no: nos faltan un par de funciones, una que nos devuelva el vector latente que introducirle al G(z) y otra que coja ese vector latente y produzca una muestra fake. \n","\n","Fijaos lo sencillo que es:"]},{"cell_type":"code","metadata":{"id":"q4Rjb3TheBbQ"},"source":["# generamos los vectores latentes que introduciremos al generador\n","def generate_latent_points(latent_dim, batch_size):\n","    # generamos un vector de batch_size * latent_dim números aleatorios\n","    # latent_dim es la dimensión del vector latente\n","    # batch_size es el número de elementos por batch\n","    x_input = randn(latent_dim * batch_size)\n","    # redimensionamos el vector para que tenga un tamaño (batch_size, latent_dim)\n","    x_input = x_input.reshape(batch_size, latent_dim)\n","    return x_input\n","\n","# creamos datos fake con el generador (dinero falsificado)\n","def generate_fake_samples(generator, latent_dim, n): \n","    # usamos la función anterior para generar los vectores latentes que \n","    # necesitamos para generar muestras fake\n","    # queremos generar 'n' códigos latentes de 'latent_dim'\n","    x_input = generate_latent_points(latent_dim, n)\n","    # le introducimos los vectores latentes al generador para obtener\n","    # muestras similares a las reales\n","    X = generator.predict(x_input)\n","    # le asignamos la etiqueta 0 (porque son falsas)\n","    y = np.zeros((n, 1)) \n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIFHKIGWfKks"},"source":["¡Ahora sí! Ya solo nos falta implementar el bucle de entrenamiento. Vamos a ello:"]},{"cell_type":"code","metadata":{"id":"yx9Fe0AlfOqO"},"source":["# función para entrenar la GAN: el discriminador y el generador\n","def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n","    # para entrenar el discriminador, le introduciremos la mitad de los datos\n","    # reales y la otra mitad falsa\n","    half_batch = int(n_batch / 2)\n","    for epoch in tqdm(range(n_epochs)):\n","        \n","        # preparamos los datos reales (generados con f(x)=x^2)\n","        x_real, y_real = generate_real_samples(half_batch)\n","        \n","        # preparamos los datos falsos (creados con el generador)\n","        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","        \n","        # entrenamos el discriminador\n","        d_model.train_on_batch(x_real, y_real) # primero con los datos reales\n","        d_model.train_on_batch(x_fake, y_fake) # luego con los datos fake\n","        \n","        # preparamos los puntos en el espacio latente: serán la entrada al\n","        # modelo GAN con el que entrenaremos el generador\n","        x_gan = generate_latent_points(latent_dim, n_batch)\n","        \n","        # creamos etiquetas invertidas para el generador: utilizamos el D(x) \n","        # para que piense que las muestras que le introducimos son reales, y\n","        # en caso de que diga que no son reales, aprovechamos la información\n","        # de sus gradientes para actualizar el G(z) para que la próxima vez\n","        # los datos generados por G(z) sean más plausibles (parecidos a los \n","        # reales)\n","        y_gan = np.ones((n_batch, 1))\n","        \n","        # como acabamos de ver, entrenamos el generador de forma que actualice\n","        # sus pesos usando los gradientes del discriminador\n","        # tened en cuenta que en este modelo (gan_model) el discriminador está\n","        # congelado, por lo que no se actualizan sus pesos: no queremos \"untar\"\n","        # a nuestro policía, lo que queremos es fabricar dinero más realista.\n","        gan_model.train_on_batch(x_gan, y_gan)\n","\n","        # evaluamos el modelo cada n_eval épocas\n","        if (epoch+1) % n_eval == 0 or epoch == 0:\n","            # preparamos ejemplos reales\n","            x_real, y_real = generate_real_samples(n_batch)\n","            # evaluamos el discriminador con datos reales\n","            _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n","            # preparamos ejemplos fake\n","            x_fake, y_fake = generate_fake_samples(generator, latent_dim, n_batch)\n","            # evaluamos el discriminador con datos fake\n","            _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n","            # mostramos cómo de bueno es nuestro policía\n","            print(epoch, acc_real, acc_fake)\n","            # lo ploteamos\n","            plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n","            plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue') \n","            # lo guardamos para tenerlo disponible más tarde\n","            filename = 'generated_plot_e%03d.png' % (epoch+1) \n","            plt.savefig(filename)\n","            plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQ7top0De_tn"},"source":["# comenzamos con el entrenamiento\n","\n","# tamaño del espacio latente\n","latent_dim = 5\n","# creamos el discriminador\n","discriminator = define_discriminator()\n","# creamos el generador\n","generator = define_generator(latent_dim)\n","# creamos la GAN\n","gan_model = define_gan(generator, discriminator)\n","# entrenamos!\n","train(generator, discriminator, gan_model, latent_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-59Xwidifpe"},"source":["Vamos a ver cómo ha ido el entrenamiento de nuestra red. Para ello, vamos a visualizar la salida del generador al principio, cuando aún no estaba entrenada, y al final, cuando ya estaba entrenada."]},{"cell_type":"code","metadata":{"id":"OxUN_1Jogr1J"},"source":["ls -lah"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0H7eRmCGR01"},"source":["plt.imshow(plt.imread('generated_plot_e001.png'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"We94v8QvhCPx"},"source":["plt.imshow(plt.imread('generated_plot_e2000.png'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OdSU3tQ_huoX"},"source":["plt.imshow(plt.imread('generated_plot_e10000.png'))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JH6r217nkec-"},"source":["¡Fijáos! El generador ha aprendido a generar datos practicamente iguales a los de nuestro dataset original. ¿Os dáis cuenta de la potencia de esto? Y este es el ejemplo más sencillo que he podido poner."]},{"cell_type":"code","metadata":{"id":"bUqEw6gniDn-"},"source":[""],"execution_count":null,"outputs":[]}]}