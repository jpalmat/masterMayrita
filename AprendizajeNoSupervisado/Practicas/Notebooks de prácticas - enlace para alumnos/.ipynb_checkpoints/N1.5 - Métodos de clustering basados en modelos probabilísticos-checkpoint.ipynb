{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnyNgqjZ2J6z"
   },
   "source": [
    "<center><h1>N1: Métodos de clustering</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnI3RyLxLUnP"
   },
   "source": [
    "# N1.5: Métodos de clustering basados en modelos probabilísticos\n",
    "\n",
    "En esta práctica vais a trabajar con los métodos de clustering basados en probabilidad vistos en clase. Concretamente, con el método basado en mixtura de Gaussianas y el algoritmo de EM (esperanza-maximización)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBIckPQrMLaI"
   },
   "source": [
    "# RECUERDA RELLENAR TUS DATOS A CONTINUACIÓN ANTES DE HACER NADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xBNEFw1HJxS"
   },
   "outputs": [],
   "source": [
    "# ===============================================================#\n",
    "# Rellena AQUÍ tu nombre y apellidos antes de hacer nada\n",
    "# ===============================================================#\n",
    "\n",
    "NOMBRE = 'Mayra'\n",
    "APELLIDOS = 'Pullupaxi'\n",
    "\n",
    "# ===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI4fp2sbGUEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Sywu-zLGFYO"
   },
   "source": [
    "Para empezar, cargamos las librerías que vamos a necesitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r5LZeFjPGFYP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS7QeR8FGFYS"
   },
   "source": [
    "\n",
    "Cargamos el dataset con el que vamos a trabajar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGOUm2tmGFYT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
    "\n",
    "#data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_dos_guassianas.csv'\n",
    "data_file_url = 'https://raw.githubusercontent.com/flifuehu/viu-unsupervised-learning/master/datasets/sinteticos/dataset_cuatro_diferente_medida.csv'\n",
    "D = np.array(pd.read_csv(data_file_url,header=0))\n",
    "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False) ,:]\n",
    "Dx = D[:,0:2]\n",
    "Dy = D[:,2]\n",
    "print('El dataset cargado tiene',Dy.size,'instancias.')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0],Dx[:,1], c=Dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcyKxPY1GFYV"
   },
   "source": [
    "\n",
    "El algoritmo EM tiene un único parámetro: el número de clústeres (K). Una vez fijado este valor, el primer paso consiste en inicializar el modelo. Se eligen unos centros iniciales de manera aleatoria, unas matrices de covarianzas fijas y unos pesos iniciales para las diferentes componentes. Sin más información, lo más normal sería asignar a todas las componentes el mismo peso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzC0vyvdGFYW"
   },
   "outputs": [],
   "source": [
    "######################## INICIALIZACION ########################\n",
    "# Elegimos un número de clústeres a buscar\n",
    "K = 4\n",
    "\n",
    "# Asignar unas matrices de covarianzas iniciales\n",
    "sigmas = []\n",
    "for k in np.arange(K):\n",
    "    sigmas.append( np.diag( 0.1 * np.ones( Dx.shape[1] ) ) )\n",
    "\n",
    "x, y = np.mgrid[(np.min(Dx[:,0])-0.1):(np.max(Dx[:,0])+0.1):.01, \n",
    "                (np.min(Dx[:,1])-0.1):(np.max(Dx[:,1])+0.1):.01]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "\n",
    "# Inicialmente consideramos que todas las componentes tienen la misma probabilidad\n",
    "PIs = np.ones(K)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3bUW4ZCGFYY"
   },
   "outputs": [],
   "source": [
    "def random_sample_float(n, mi, ma):\n",
    "    return (ma - mi) * np.random.random_sample(n) + mi\n",
    "\n",
    "# Elegir unos centros (uno para cada componente) de manera aleatoria\n",
    "cDx = np.zeros(K*Dx.shape[1])\n",
    "cDx.shape = (K,Dx.shape[1])\n",
    "\n",
    "for d in np.arange(Dx.shape[1]):\n",
    "    cDx[:,d] = random_sample_float(K, np.min(Dx[:,d]), np.max(Dx[:,d]))\n",
    "\n",
    "print('Los centros iniciales elegidos aleatoriamente son:')\n",
    "print(cDx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hIu_ei2GFYb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mostramos las componentes iniciales\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for k in np.arange(K):\n",
    "    rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
    "    ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
    "    ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
    "\n",
    "ax.scatter(Dx[:,0],Dx[:,1])\n",
    "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZDzTYVmGFYd"
   },
   "source": [
    "\n",
    "Una vez inicializado, el algoritmo EM ejecuta un bucle donde se repiten los pasos E y M hasta que se alcanza la convergencia. \n",
    "\n",
    "En el paso E se (re)calcula la probabilidad de que cada ejemplo pertenezca a cada una de las componentes (los valores $z_{ik}$). \n",
    "\n",
    "En el paso M se (re)calculan los parámetros del modelo: los centros de las distribuciones normales (uno por componente, $\\mu_k$), las matrices de covarianzas de las normales (una por componente, $\\Sigma_k$) y los coeficientes de importancia de las diferentes componentes ($\\{\\pi_k\\}_{k=1}^K$, con $\\sum_k \\pi_k=1$).\n",
    "\n",
    "El algoritmo alcanza la convergencia cuando los parámetros no cambian entre dos iteraciones consecutivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae6LHzt9GFYe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preparamos el vector donde guardamos la asignación probabilística \n",
    "# de cada elemento a un clúster (z_ik)\n",
    "Dy_probs = np.zeros((Dx.shape[0], K))\n",
    "\n",
    "# Flag de convergencia\n",
    "iterando = True\n",
    "# Dibujar los plots intermedios?\n",
    "dibujar = True\n",
    "# Si dibujas, sólo uno de cada 'frec_dibujo'\n",
    "frec_dibujo = 10\n",
    "\n",
    "it = 0\n",
    "\n",
    "while iterando:\n",
    "\n",
    "    # Vector auxiliar para guardar los centros de la iteración pasada\n",
    "    # necesarios para identificar la convergencia\n",
    "    cDx_ant = cDx.copy()\n",
    "    \n",
    "    # PASO E:\n",
    "    # Calcular la asignacion a las componentes (z_ik)\n",
    "    for k in np.arange(K):\n",
    "        Dy_probs[:,k] = PIs[k] * multivariate_normal.pdf(Dx, mean=cDx[k,:], cov=sigmas[k])\n",
    "    Dy_probs = Dy_probs/np.sum(Dy_probs,axis=1)[:,None]\n",
    "    \n",
    "    # PASO M:\n",
    "    # - a: Calcular los nuevos centros de las K componentes\n",
    "    for k in range(K):\n",
    "        cDx[k,:] = np.sum(Dy_probs[:,k,None]*Dx,axis=0)/np.sum(Dy_probs[:,k])\n",
    "\n",
    "    # - b: Calcular la matriz de covarianza (sigma) de las K componentes\n",
    "    for k in range(K):\n",
    "        sigmas[k] = np.sum([Dy_probs[i,k]* np.dot(Dx[i,:,None]-cDx[k,:,None],\n",
    "                                                  (Dx[i,:,None]-cDx[k,:,None]).transpose())\n",
    "                            for i in np.arange(Dx.shape[0])], axis=0) / np.sum(Dy_probs[:,k])\n",
    "\n",
    "    # - c: Calcular los coeficientes de importancia de las diferentes componentes\n",
    "    PIs = np.sum(Dy_probs,axis=0)/Dx.shape[0]\n",
    "\n",
    "    if dibujar and (it % frec_dibujo) == 0:\n",
    "        # Dibujar el plot con el resultado actual\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        for k in np.arange(K):\n",
    "            rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
    "            ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
    "            ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
    "        ax.scatter(Dx[:,0],Dx[:,1])\n",
    "        ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='g')\n",
    "\n",
    "    it += 1\n",
    "    if np.allclose(cDx, cDx_ant):\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNiN7DDgGFYg"
   },
   "source": [
    "\n",
    "El resultado final del algoritmo es una matriz de probabilidades que asigna cada elemento a un clúster (componente) con cierta probabilidad. Si quisiésemos obtener una asignación determinista de cada caso a un único clúster, tomaríamos la componente que mayor probabilidad asigna a cada caso como su clúster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xecVlzRxGFYg"
   },
   "outputs": [],
   "source": [
    "Dyp = np.argmax(Dy_probs,axis=1)\n",
    "\n",
    "# Ver asignaciones finales\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for k in np.arange(K):\n",
    "    rv = multivariate_normal(mean=cDx[k,:], cov=sigmas[k])\n",
    "    ax.contour(x, y, rv.pdf(pos), levels=10, linewidths=1,colors='k',alpha=0.1)\n",
    "    ax.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"RdBu_r\",alpha=0.1)\n",
    "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp)\n",
    "ax.scatter(cDx[:,0],cDx[:,1], marker='*', s=200, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBgjVDbMiUwc"
   },
   "outputs": [],
   "source": [
    "# Como podéis observar, sólo hay un punto que se asigna de manera incorrecta a otro clúster. \n",
    "\n",
    "# P1. ¿Cuales créeis que serán las probabilidades aproximadas de dicho punto para cada una de las componentes?\n",
    "\n",
    "# a) Amarillo=0.02. Verde=0.42. Morado: 0.48. Azul: 0.08.\n",
    "# b) Amarillo=0.02. Verde=0.48. Morado: 0.42. Azul: 0.08.\n",
    "# c) Amarillo=0.02. Verde=0.7. Morado: 0.2. Azul: 0.08.\n",
    "\n",
    "# SE DEBE CONTESTAR EN EL CAMPUS VIRTUAL\n",
    "\n",
    "# P2. En ese caso concreto, ¿a qué puede deberse el error?\n",
    "\n",
    "# a) La (co)varianza de la componente a la que realmente pertenece es más pequeña que la de la otra (a la que al final es asignado)\n",
    "# b) La (co)varianza de la componente a la que realmente pertenece es más grande que la de la otra (a la que al final es asignado)\n",
    "# c) La (co)varianza de las diferentes componentes no influye en la asignación, sino que es la incialización aleatoria lo que propicia este resultado\n",
    "\n",
    "# SE DEBE CONTESTAR EN EL CAMPUS VIRTUAL\n",
    "\n",
    "# P3. ¿Existe alguna forma de obtener un resultado diferente, o el algoritmo siempre encontrará el mismo resultado?\n",
    "\n",
    "# a) El algoritmo es determinista, por lo que siempre obtendrá los mismos resultados\n",
    "# b) Podemos variar los parámetros de las gaussianas para que tengan distintas formas\n",
    "# c) Podemos variar la inicialización para que el resultado final sea diferente\n",
    "\n",
    "# SE DEBE CONTESTAR EN EL CAMPUS VIRTUAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWCXGLS2GFYi"
   },
   "source": [
    "\n",
    "Como en anteriores ocasiones, podemos estudiar la bondad del agrupamiento ya que se conoce la realidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4IPXwG8GFYj"
   },
   "outputs": [],
   "source": [
    "def matriz_confusion(cat_real, cat_pred):\n",
    "    cats = np.unique(cat_real)\n",
    "    clusts = np.unique(cat_pred)\n",
    "    mat = np.array([[np.sum(np.logical_and(cat_real==cats[i], cat_pred==clusts[j])) \n",
    "                     for j in np.arange(clusts.size)] \n",
    "                    for i in np.arange(cats.size)])\n",
    "    return(mat)\n",
    "\n",
    "def medida_error(mat):\n",
    "    assign = np.sum([np.max(mat[l,:]) for l in np.arange(mat.shape[0])])\n",
    "    return 1 - assign / float(np.sum(mat))\n",
    "\n",
    "def medida_pureza(mat):\n",
    "    totales = np.sum(mat,0)/float(np.sum(mat))\n",
    "    return np.sum([totales[k] * np.max(mat[:,k]/float(np.sum(mat[:,k]))) for k in np.arange(mat.shape[1])])\n",
    "\n",
    "def medida_precision(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[:,k]))\n",
    "\n",
    "def medida_recall(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[l,:]))\n",
    "\n",
    "def medida_f1_especifica(mat, l, k):\n",
    "    prec = medida_precision(mat, l, k)\n",
    "    rec = medida_recall(mat, l, k)\n",
    "    if (prec+rec)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2*prec*rec/(prec+rec)\n",
    "\n",
    "def medida_f1(mat):\n",
    "    totales = np.sum(mat,1)/float(np.sum(mat))\n",
    "    assign = np.sum([totales[l] * np.max([medida_f1_especifica(mat, l, k) \n",
    "                                          for k in np.arange(mat.shape[1])]) \n",
    "                     for l in np.arange(mat.shape[0])])\n",
    "    return assign\n",
    "\n",
    "mC = matriz_confusion(Dy,Dyp)\n",
    "\n",
    "print(mC)\n",
    "print('El valor del error cometido es = ', medida_error(mC))\n",
    "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC))\n",
    "print('El valor F1 es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3fFTRsJGFYl"
   },
   "source": [
    "<hr>\n",
    "<h2>Implementaciones en librerías de Python</h2>\n",
    "\n",
    "La librería ScikitLearn ya implementa el algoritmo EM para mixturas de Gaussianas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP65roqdGFYm"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Se inicializa el método con el número de clústeres (componentes) a buscar\n",
    "modelo = GaussianMixture(n_components = 4, max_iter = 200)\n",
    "# Se aprende el modelo\n",
    "modelo = modelo.fit(Dx)\n",
    "# Se predicen las asignaciones a clústeres\n",
    "Dyp_sk = modelo.predict(Dx)\n",
    "\n",
    "# Medimos el rendimiento del algoritmo de ScikitLearn\n",
    "mC_sk = matriz_confusion(Dy,Dyp_sk)\n",
    "\n",
    "print('Matriz de confusión:')\n",
    "print(mC_sk)\n",
    "print('El valor del error cometido es = ', medida_error(mC_sk))\n",
    "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC_sk))\n",
    "print('El valor F1 es = ', medida_f1(mC_sk))\n",
    "\n",
    "# Ver asignaciones finales\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0],Dx[:,1], c=Dyp_sk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xqu3bN3GFYo"
   },
   "source": [
    "\n",
    "El algoritmo anterior, ejecutado a la primera, puede dar un resultado no óptimo: se está eligiendo una inicialización que encalla en un óptimo local. Si se ejecuta en varias ocaciones, eventualmente se obtendrá el resultado óptimo.\n",
    "\n",
    "Podríamos comparar el resultado de nuestro algoritmo y el de la implementación de ScikitLearn para observar si devuelven el mismo resultado:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrcjgveuGFYo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Si comparamos el resultado de ambos algoritmos, el nuestro y el de ScikitLearn\n",
    "mC_comp = matriz_confusion(Dyp,Dyp_sk)\n",
    "\n",
    "print('Matriz de confusión:')\n",
    "print(mC_comp)\n",
    "print('El valor del error cometido es = ', medida_error(mC_comp))\n",
    "print('La pureza del agrupamiento obtenido es = ', medida_pureza(mC_comp))\n",
    "print('El valor F1 es = ', medida_f1(mC_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPUSD74mGFYq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N1.5 - Métodos de clustering basados en modelos probabilísticos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
