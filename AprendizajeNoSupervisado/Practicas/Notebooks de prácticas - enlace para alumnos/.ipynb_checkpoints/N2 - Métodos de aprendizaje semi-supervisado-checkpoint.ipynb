{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnyNgqjZ2J6z"
   },
   "source": [
    "<center><h1>N2: Aprendizaje semi-supervisado</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsc32HMtc6TU"
   },
   "source": [
    "# N2: Métodos de aprendizaje semi-supervisado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAsz003KMbmS"
   },
   "source": [
    "# RECUERDA RELLENAR TUS DATOS A CONTINUACIÓN ANTES DE HACER NADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xBNEFw1HJxS"
   },
   "outputs": [],
   "source": [
    "# ===============================================================#\n",
    "# Rellena AQUÍ tu nombre y apellidos antes de hacer nada\n",
    "# ===============================================================#\n",
    "\n",
    "NOMBRE = 'Mayra'\n",
    "APELLIDOS = 'Pullupaxi'\n",
    "\n",
    "# ===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuhXGlHQLUcK"
   },
   "source": [
    "En esta práctica aprenderemos a llevar a cabo un aprendizaje semi-supervisado. En concreto, estudiaremos el funcionamiento y la utilización del popular algoritmo de EM para aprender un modelo naive Bayes en semi-supervisado, así como la estrategia de aprendizaje conocida como \"co-training\".\n",
    "\n",
    "Empezaremos por programar una implementación del clasificador que usaremos, el clásico Naive Bayes. Este clasificador probabilístico guarda los parámetros relativos a la distribución de probabilidad marginal de clase ($\\{p(C=c)\\}_{c=1}^{|C|}$) y las distribuciones de probabilidad condicional de las variables predictoras dada la clase ($\\{p(X_i=x_i|C=c)\\}_{x_i=1}^{|X_i|}$ para los diferentes valores de la variable clase $C=c$ y para todas las variables predictoras). Con estos parámetros se puede calcular la probabilidad de un caso, $\\mathbf{x}$:\n",
    "$$  p(C=c|\\mathbf{x})\\propto p(C=c)\\prod_{i=1}^{v} p(x_i|C=c)$$\n",
    "normalizándolos para todos los posibles valores de $c$ (todas las clases posibles) y \n",
    "con $v$ siendo el número de variables predictoras \n",
    "(2 en este caso) para que la \n",
    "suma sea $\\sum_{c=1}^{|C|} p(C=c|\\mathbf{x})=1$.\n",
    "\n",
    "Por conveniencia, también guardaremos otras variables como la cardinalidad de las diferentes variables y el índice de la variable clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC1zvNvdGZJ-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class naiveBayes:\n",
    "    def __init__(self, iClass, cardinalities):\n",
    "        self.iClass = iClass\n",
    "        self.cardinalities = cardinalities.copy()\n",
    "        self.Pc = np.zeros(cardinalities[iClass])\n",
    "        self.Pxc = []\n",
    "        for i in np.arange(len(self.cardinalities)):\n",
    "            aux = np.array([])\n",
    "            if i != iClass:\n",
    "                aux = np.zeros((cardinalities[i],cardinalities[iClass]))\n",
    "            self.Pxc.append(aux)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiUdRZl3GZKB"
   },
   "source": [
    "Una vez diseñado, vamos a lo importante. Un clasificador ha de \n",
    "poder ser aprendido y usado para predecir. Por ello, diseñaremos \n",
    "una función para cada procedimiento. \n",
    "Para predecir, se han de calcular las siguiente probabilidades:\n",
    "$$ p(C=c)\\prod_{i=1}^{v} p(x_i|C=c)$$\n",
    "para todos los posibles valores de $c$ (todas las clases posibles) y \n",
    "con $v$ siendo el número de variables predictoras \n",
    "(2 en este caso). Finalmente, normalizamos para que la \n",
    "suma de $\\sum_{c=1}^{|C|} p(C=c|x)=1$.\n",
    "La función de predicción \n",
    "sería así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO-NXt6bGZKB"
   },
   "outputs": [],
   "source": [
    "def predictNB(model, instance):\n",
    "    probs = model.Pc.copy()\n",
    "    for i in np.arange(len(model.cardinalities)):\n",
    "        if i != model.iClass:\n",
    "            probs *= model.Pxc[i][instance[i],:]\n",
    "\n",
    "    return probs/np.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd-hVE2nGZKD"
   },
   "source": [
    "\n",
    "Considerando la estructura definida para guardar los \n",
    "parámetros y lo visto en clase sobre aprendizaje de \n",
    "parámetros de máxima verosimilitud en un entorno de \n",
    "aprendizaje semi-supervisado, diseñaremos la función \n",
    "de aprendizaje:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCJclM_eGZKE"
   },
   "outputs": [],
   "source": [
    "def learnNB(L, U, pesosU, iClass, cardinalities, smoothing=1):\n",
    "    modelo = naiveBayes(iClass, cardinalities)\n",
    "    \n",
    "    # Aprender de casos etiquetados\n",
    "    for i in np.arange(L.shape[0]):\n",
    "        # Actualizamos los parametros de la marginal P(c)\n",
    "        modelo.Pc[L[i,iClass]] += 1\n",
    "        \n",
    "        for j in np.arange(len(cardinalities)):\n",
    "            if j != iClass:\n",
    "                # Actualizamos los parametros de las condicionales P(x_i|c)\n",
    "                modelo.Pxc[j][L[i,j],L[i,iClass]] += 1\n",
    "\n",
    "    # Aprender de casos no etiquetados\n",
    "    for u in np.arange(U.shape[0]):\n",
    "        # Actualizamos los parametros de la marginal de P(c)\n",
    "        modelo.Pc += pesosU[u,:]\n",
    "        \n",
    "        for j in np.arange(len(cardinalities)):\n",
    "            if j != iClass:\n",
    "                # Actualizamos los parametros de las condicionales P(x_i|c)\n",
    "                modelo.Pxc[j][U[u,j],:] += pesosU[u,:]\n",
    "    \n",
    "    modelo.Pc += smoothing # Laplace smoothing\n",
    "    modelo.Pc /= np.sum(modelo.Pc)\n",
    "    for j in np.arange(len(cardinalities)):\n",
    "        if j != iClass:\n",
    "            modelo.Pxc[j] += smoothing # Laplace smoothing\n",
    "            modelo.Pxc[j] /= np.sum(modelo.Pxc[j],0)\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s0gAgKMGZKG"
   },
   "source": [
    "\n",
    "Con esto completamos el diseño del clasificador Naive Bayes para el entorno semi-supervisado.\n",
    "\n",
    "Ahora ya podemos crear el algoritmo EM, que itera los pasos E y M de forma sencilla hasta que converge.\n",
    "\n",
    "**NOTA**: os recomiendo que primero os leáis todo el notebook y luego tratéis de completar el código que falta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E84pZbSlGZKH"
   },
   "outputs": [],
   "source": [
    "def EM(L, U, iClass, cardinalities, epsilon=0.001):\n",
    "    modelo, pesosU = inicializar(L, U, iClass, cardinalities)\n",
    "\n",
    "    convergencia = False\n",
    "    it = 0\n",
    "    while not convergencia:\n",
    "        it += 1\n",
    "        print('Iteracion', it)\n",
    "        \n",
    "        # PASO E\n",
    "        \n",
    "        # P1\n",
    "        pesosU = ## P1. Tu código aquí ##\n",
    "\n",
    "        antModelo = modelo\n",
    "        \n",
    "        # PASO M\n",
    "        \n",
    "        # P2\n",
    "        modelo = ## P2. Tu código aquí ##\n",
    "    \n",
    "        # Comprobacion de convergencia\n",
    "        convergencia = testConvergencia(modelo, antModelo, epsilon)\n",
    "    \n",
    "    return modelo, pesosU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEVt6eblGZKI"
   },
   "source": [
    "Faltarían por programar las tres funciones fundamentales \n",
    "del EM: el paso E (<b>EStep</b>),  el paso M (<b>MStep</b>) \n",
    "y la función que comprueba la convergencia (<b>testConvergencia</b>). \n",
    "Además, falta la inicialización donde asignamos valores iniciales \n",
    "a los pesos de las instancias no etiquetadas para aprender la \n",
    "primera versión del modelo. En concreto, en esta ocasión \n",
    "asignaremos a todos los casos la misma probabilidad de \n",
    "pertenecer a cualquier clase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9TlqyXMGZKJ"
   },
   "outputs": [],
   "source": [
    "def inicializar(L, U, iClass, cardinalities):\n",
    "    # Todos los elementos en U tienen la misma probabilidad de pertenecer a cualquier clase\n",
    "    pesosU = np.ones((U.shape[0],cardinalities[iClass]))\n",
    "    pesosU /= cardinalities[iClass]\n",
    "    \n",
    "    modelo = learnNB(L, U, pesosU, iClass, cardinalities)\n",
    "\n",
    "    return modelo, pesosU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVjlJx2iGZKL"
   },
   "source": [
    "\n",
    "El paso E consiste en recalcular los pesos dado un modelo y el paso M en aprender una nueva versión del modelo:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6HfP-8MGZKM"
   },
   "outputs": [],
   "source": [
    "def EStep(L, U, pesosU, modelo):\n",
    "    nPesosU = np.zeros(pesosU.shape)\n",
    "    for u in np.arange(U.shape[0]):\n",
    "        nPesosU[u,:] = predictNB(modelo, U[u,:])\n",
    "\n",
    "    return nPesosU\n",
    "\n",
    "def MStep(L, U, pesosU, iClass, cardinalities, antModelo):\n",
    "    modelo = learnNB(L, U, pesosU, iClass, cardinalities)\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-Svk_i6GZKN"
   },
   "source": [
    "\n",
    "Mediremos la convergencia atendiendo al criterio de si \n",
    "la distancia euclídea entre los parámetros del paso anterior \n",
    "y del actual es menor que <i>epsilon</i> o no:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvTHXKRaGZKO"
   },
   "outputs": [],
   "source": [
    "def testConvergencia(modeloA, modeloB, epsilon=0.001):\n",
    "    resultado = np.sum((modeloA.Pc-modeloB.Pc)**2)\n",
    "\n",
    "    for j in np.arange(len(modeloA.cardinalities)):\n",
    "        if j != modeloA.iClass:\n",
    "            resultado += np.sum((modeloA.Pxc[j]-modeloB.Pxc[j])**2)\n",
    "  \n",
    "    resultado = np.sqrt(resultado)\n",
    "    \n",
    "    return resultado < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VtmETMBGZKQ"
   },
   "source": [
    "\n",
    "Una vez hemos diseñado completamente el algoritmo EM y el clasificador NB, podemos proceder a su uso. \n",
    "Creamos un dataset y hacemos la llamada al EM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBQyKD3VGZKQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(23)\n",
    "nPredVars = 10\n",
    "iClass = nPredVars\n",
    "nSampleXsubset = 20\n",
    "cardinalities = np.repeat(2,nPredVars+1)\n",
    "\n",
    "# Simulamos un dataset\n",
    "X,y = make_classification(n_samples=nSampleXsubset*3,n_features=nPredVars,n_redundant=0)\n",
    "X[X<0] = 0;X[X>0] = 1 # discretizamos las variables predictoras\n",
    "X = X.astype(int)\n",
    "y.shape=(len(y),1)\n",
    "\n",
    "L = np.concatenate((X[:nSampleXsubset,:],\n",
    "                    y[:nSampleXsubset,:]),axis=1)\n",
    "U = np.concatenate((X[nSampleXsubset:(nSampleXsubset*2),:],\n",
    "                    y[nSampleXsubset:(nSampleXsubset*2),:]),axis=1)\n",
    "U[:,iClass] = np.nan # eliminamos la etiqueta de un subconjunto de elementos\n",
    "\n",
    "modelo, pesos = EM(L, U, iClass, cardinalities, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCx7uOQdGZKS"
   },
   "source": [
    "\n",
    "Como en anteriores ocasiones, podemos estudiar la bondad del agrupamiento ya que se conoce la realidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmDsV_EhGZKT"
   },
   "outputs": [],
   "source": [
    "test = np.concatenate((X[nSampleXsubset*2:,:],\n",
    "                    y[nSampleXsubset*2:,:]),axis=1)\n",
    "\n",
    "realLabels = test[:,iClass]\n",
    "predLabels = np.zeros(realLabels.shape)\n",
    "for i in np.arange(test.shape[0]):\n",
    "    probs = predictNB(modelo, test[i,:])\n",
    "    predLabels[i] = np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzrzqaxFGZKU"
   },
   "outputs": [],
   "source": [
    "def matriz_confusion(cat_real, cat_pred, nClasses):\n",
    "    mat = np.array([[np.sum(np.logical_and(cat_real==i, cat_pred==j)) \n",
    "                     for j in np.arange(nClasses)] \n",
    "                    for i in np.arange(nClasses)])\n",
    "    return(mat)\n",
    "\n",
    "def medida_error(mat):\n",
    "    tot = np.sum(mat)\n",
    "    aux = mat.copy()\n",
    "    np.fill_diagonal(aux, 0)\n",
    "    return float(np.sum(aux))/tot\n",
    "\n",
    "def medida_precision(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[:,k]))\n",
    "\n",
    "def medida_recall(mat, l, k):\n",
    "    return mat[l,k]/float(np.sum(mat[l,:]))\n",
    "\n",
    "def medida_f1(mat):\n",
    "    prec = medida_precision(mat, 1, 1)\n",
    "    rec = medida_recall(mat, 1, 1)\n",
    "    if (prec+rec)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2*prec*rec/(prec+rec)\n",
    "\n",
    "mC = matriz_confusion(realLabels,predLabels,cardinalities[iClass])\n",
    "\n",
    "print(mC)\n",
    "print('El error del clasificador es = ', medida_error(mC))\n",
    "print('El valor F1 del clasificador es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2YP7FF3GZKW"
   },
   "source": [
    "<hr>\n",
    "<h2>Implementaciones en librerías de Python</h2>\n",
    "\n",
    "La librería ScikitLearn implementa diversas versiones del clasificador NB. En el caso de <b>MultinomialNB</b>, la distribución de probabilidad que se modela es diferente: una distribución de probabilidad multinomial donde el valor de cada variable predictora es el conteo de esta distribución de probabilidad:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\Pr(X_{1}=x_{1},\\dots, X_{v}=x_{v})&{}={\\begin{cases}{\\displaystyle {n! \\over x_{1}!\\cdot\\dots\\cdot x_{k}!}p_{1}^{x_{1}}\\cdot \\dots \\cdot p_{v}^{x_{v}}},\\quad &{\\text{when }}\\sum _{i=1}^{k}x_{i}=n\\\\\\\\0&{\\text{otherwise,}}\\end{cases}}\n",
    "\\end{aligned}\n",
    "para cualquier conjunto $\\{x_1, \\dots, x_v\\}$ de valores no negativos.\n",
    "\n",
    "Si quisiésemos incluir esta versión del NB en el algoritmo EM, deberíamos modificar las funciones específicas que hacen llamadas al modelo pero no la general iterativa del EM.\n",
    "\n",
    "En primer lugar, deberíamos preparar una función para el aprendizaje del modelo cuando los datos son semi-supervisados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2OIpOIpGZKX"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def sklearnNB(L, U, pesosU, iClass, cardinalities):\n",
    "    modelo = MultinomialNB()\n",
    "\n",
    "    modelo.partial_fit(np.delete(L, iClass, axis=1),\n",
    "                       L[:,iClass],\n",
    "                       classes=[0,1])\n",
    "\n",
    "    for c in np.arange(cardinalities[iClass]):\n",
    "        impU = U.copy()\n",
    "        impU[:,iClass] = c\n",
    "        modelo.partial_fit(np.delete(impU, iClass, axis=1),\n",
    "                           impU[:,iClass],\n",
    "                           sample_weight = pesosU[:,c])\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmYUnYhcGZKZ"
   },
   "source": [
    "\n",
    "La función de predicción del NB, en este caso, ya está integrada en la definición de la clase de Scikit-learn.\n",
    "\n",
    "Las cuatro funciones auxiliares del EM se deben adecuar a este nuevo modelo: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLJLlG8kGZKZ"
   },
   "outputs": [],
   "source": [
    "def inicializar(L, U, iClass, cardinalities):\n",
    "    pesosU = np.ones((U.shape[0],cardinalities[iClass]))\n",
    "    pesosU /= cardinalities[iClass]\n",
    "    \n",
    "    modelo = sklearnNB(L, U, pesosU, iClass, cardinalities)\n",
    "\n",
    "    return modelo, pesosU\n",
    "\n",
    "def EStep(L, U, pesosU, modelo):\n",
    "\n",
    "    nPesosU = modelo.predict_proba(np.delete(U, iClass, axis=1))\n",
    "\n",
    "    return nPesosU\n",
    "\n",
    "def MStep(L, U, pesosU, iClass, cardinalities, antModelo):\n",
    "    modelo = sklearnNB(L, U, pesosU, iClass, cardinalities)\n",
    "    return modelo\n",
    "\n",
    "def testConvergencia(modeloA, modeloB, epsilon=0.001):\n",
    "    resultado = np.sum((np.exp(modeloA.class_log_prior_) - \n",
    "                        np.exp(modeloB.class_log_prior_))**2)\n",
    "    resultado += np.sum((np.exp(modeloA.feature_log_prob_) - \n",
    "                         np.exp(modeloB.feature_log_prob_))**2)\n",
    "\n",
    "    resultado = np.sqrt(resultado)\n",
    "    \n",
    "    return resultado < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTSRyNpWGZKb"
   },
   "source": [
    "\n",
    "Una vez hemos diseñado completamente las distintas funciones del algoritmo EM y esta nueva versión del NB, podemos proceder a su uso. \n",
    "Creamos un dataset y hacemos la llamada al EM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ILFM6uBGZKb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "nPredVars = 10\n",
    "iClass = nPredVars\n",
    "nSampleXsubset = 20\n",
    "cardinalities = np.repeat(2,nPredVars+1)\n",
    "\n",
    "# Simulamos un dataset\n",
    "X,y = make_classification(n_samples=nSampleXsubset*3,n_features=nPredVars,n_redundant=0)\n",
    "X[X<0] = 0;X[X>0] = 1 # discretizamos las variables predictoras\n",
    "X = X.astype(int)\n",
    "y.shape=(len(y),1)\n",
    "\n",
    "L = np.concatenate((X[:nSampleXsubset,:],\n",
    "                    y[:nSampleXsubset,:]),axis=1)\n",
    "U = np.concatenate((X[nSampleXsubset:(nSampleXsubset*2),:],\n",
    "                    y[nSampleXsubset:(nSampleXsubset*2),:]),axis=1)\n",
    "U[:,iClass] = np.nan\n",
    "\n",
    "skmodelo, skpesos = EM(L, U, iClass, cardinalities, 0.001)\n",
    "print(skpesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMlhIDZRGZKd"
   },
   "outputs": [],
   "source": [
    "test = np.concatenate((X[nSampleXsubset*2:,:],\n",
    "                       y[nSampleXsubset*2:,:]),axis=1)\n",
    "\n",
    "skRealLabels = test[:,iClass]\n",
    "skPredLabels = np.zeros(realLabels.shape)\n",
    "test = np.delete(test,iClass,1)\n",
    "skPredLabels = np.argmax(skmodelo.predict_proba(test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhhYcLL0GZKf"
   },
   "outputs": [],
   "source": [
    "mC = matriz_confusion(skRealLabels,skPredLabels,cardinalities[iClass])\n",
    "\n",
    "print(mC)\n",
    "print('El error del clasificador es = ', medida_error(mC))\n",
    "print('El valor F1 del clasificador es = ', medida_f1(mC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KZccc47GZKg"
   },
   "source": [
    "\n",
    "El algoritmo EM, ejecutado en una única ocasión, puede dar un resultado no óptimo: se está eligiendo una inicialización que lleva al algoritmo a encallar en un óptimo local. Si se ejecuta en varias ocaciones, eventualmente se obtendrá el resultado óptimo.\n",
    "\n",
    "Podríamos comparar el resultado de nuestro algoritmo y el de la implementación de ScikitLearn para observar si devuelven el mismo resultado:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDV-mOjpGZKh"
   },
   "outputs": [],
   "source": [
    "# Si comparamos el resultado de ambos algoritmos, el nuestro y el de ScikitLearn\n",
    "mC_comp = matriz_confusion(predLabels,skPredLabels,cardinalities[iClass])\n",
    "\n",
    "print('Matriz de confusión:')\n",
    "print(mC_comp)\n",
    "print('El valor del error (diferencia) es = ', medida_error(mC_comp))\n",
    "print('El valor F1 del clasificador es = ', medida_f1(mC_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b1oNgb7-iR_"
   },
   "outputs": [],
   "source": [
    "# PREGUNTAS:\n",
    "\n",
    "# P3. Al utilizar Naive Bayes estamos realizando varias asunciones, una de ellas, muy importante, relacionada con nuestras variables. ¿Cuál es esta asunción?\n",
    "\n",
    "# a) Que nuestras variables esten escaladas en el mismo rango\n",
    "# b) Que nuestras variables son independientes unas de otras\n",
    "# c) Que nuestras variables no contengan valores nulos\n",
    "\n",
    "# SE DEBE CONTESTAR EN EL CAMPUS VIRTUAL\n",
    "\n",
    "# P4. ¿Qué ventaja hay en usar EM NB respecto a Naive Bayes?\n",
    "\n",
    "# a) El proceso de aprendizaje es más rápido gracias al algoritmo de EM\n",
    "# b) El algoritmo de EM permite poder entrenar un modelo aún cuando no disponemos de todas las etiquetas\n",
    "# c) Permite relajar la condición de la independencia de las variables\n",
    "\n",
    "# SE DEBE CONTESTAR EN EL CAMPUS VIRTUAL"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N2 - Métodos de aprendizaje semi-supervisado.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
