{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alumnos:\n",
    "Mikel Alberdi,\n",
    "Mayra Pullupaxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "   1) Implementar la red neuronal que se usará en la solución\n",
    "    \n",
    "   2) Implementar las distintas piezas de la solución DQN\n",
    "    \n",
    "   3) Justificar la respuesta en relación a los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANTE:\n",
    "\n",
    "- Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "\n",
    "- Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "\n",
    "- Tened en cuenta que las versiones de librerías recomendadas son Tensorflow==1.13.1, Keras==2.2.4 y keras-rl==0.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:564: UserWarning: \u001b[33mWARN: The environment SpaceInvaders-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute_1 (Permute)         (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1., value_min=.1, value_test=0.1,\n",
    "                              nb_steps=2500000)\n",
    "memory = SequentialMemory(limit=100000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               memory=memory,\n",
    "               policy=policy,\n",
    "               processor=processor,\n",
    "               gamma=0.99,\n",
    "               nb_actions=nb_actions,\n",
    "               nb_steps_warmup=50000,\n",
    "               target_model_update=1000,\n",
    "               train_interval=20\n",
    "                )\n",
    "               \n",
    "dqn.compile(Adam(learning_rate=0.00025),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3750000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     780/3750000: episode: 1, duration: 2.633s, episode steps: 780, steps per second: 296, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1291/3750000: episode: 2, duration: 1.766s, episode steps: 511, steps per second: 289, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1971/3750000: episode: 3, duration: 2.227s, episode steps: 680, steps per second: 305, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2665/3750000: episode: 4, duration: 2.270s, episode steps: 694, steps per second: 306, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3291/3750000: episode: 5, duration: 1.936s, episode steps: 626, steps per second: 323, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3972/3750000: episode: 6, duration: 2.176s, episode steps: 681, steps per second: 313, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4766/3750000: episode: 7, duration: 2.577s, episode steps: 794, steps per second: 308, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5312/3750000: episode: 8, duration: 1.710s, episode steps: 546, steps per second: 319, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6311/3750000: episode: 9, duration: 3.325s, episode steps: 999, steps per second: 300, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6843/3750000: episode: 10, duration: 1.727s, episode steps: 532, steps per second: 308, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7571/3750000: episode: 11, duration: 2.375s, episode steps: 728, steps per second: 306, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8108/3750000: episode: 12, duration: 1.743s, episode steps: 537, steps per second: 308, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8920/3750000: episode: 13, duration: 2.617s, episode steps: 812, steps per second: 310, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9592/3750000: episode: 14, duration: 2.058s, episode steps: 672, steps per second: 327, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10546/3750000: episode: 15, duration: 3.037s, episode steps: 954, steps per second: 314, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10943/3750000: episode: 16, duration: 1.325s, episode steps: 397, steps per second: 300, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11649/3750000: episode: 17, duration: 2.239s, episode steps: 706, steps per second: 315, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12400/3750000: episode: 18, duration: 2.264s, episode steps: 751, steps per second: 332, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13216/3750000: episode: 19, duration: 2.446s, episode steps: 816, steps per second: 334, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13696/3750000: episode: 20, duration: 1.449s, episode steps: 480, steps per second: 331, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14548/3750000: episode: 21, duration: 2.712s, episode steps: 852, steps per second: 314, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14957/3750000: episode: 22, duration: 1.342s, episode steps: 409, steps per second: 305, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15286/3750000: episode: 23, duration: 0.999s, episode steps: 329, steps per second: 329, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15820/3750000: episode: 24, duration: 1.742s, episode steps: 534, steps per second: 307, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16522/3750000: episode: 25, duration: 2.358s, episode steps: 702, steps per second: 298, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17015/3750000: episode: 26, duration: 1.629s, episode steps: 493, steps per second: 303, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17987/3750000: episode: 27, duration: 3.141s, episode steps: 972, steps per second: 309, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18928/3750000: episode: 28, duration: 2.949s, episode steps: 941, steps per second: 319, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19641/3750000: episode: 29, duration: 2.383s, episode steps: 713, steps per second: 299, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20149/3750000: episode: 30, duration: 1.718s, episode steps: 508, steps per second: 296, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21155/3750000: episode: 31, duration: 3.349s, episode steps: 1006, steps per second: 300, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22145/3750000: episode: 32, duration: 3.121s, episode steps: 990, steps per second: 317, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22771/3750000: episode: 33, duration: 2.089s, episode steps: 626, steps per second: 300, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23520/3750000: episode: 34, duration: 2.473s, episode steps: 749, steps per second: 303, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24200/3750000: episode: 35, duration: 2.185s, episode steps: 680, steps per second: 311, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24755/3750000: episode: 36, duration: 1.795s, episode steps: 555, steps per second: 309, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25114/3750000: episode: 37, duration: 1.174s, episode steps: 359, steps per second: 306, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25899/3750000: episode: 38, duration: 2.548s, episode steps: 785, steps per second: 308, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26486/3750000: episode: 39, duration: 1.916s, episode steps: 587, steps per second: 306, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27300/3750000: episode: 40, duration: 2.664s, episode steps: 814, steps per second: 306, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28193/3750000: episode: 41, duration: 2.792s, episode steps: 893, steps per second: 320, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28707/3750000: episode: 42, duration: 1.709s, episode steps: 514, steps per second: 301, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29130/3750000: episode: 43, duration: 1.402s, episode steps: 423, steps per second: 302, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29810/3750000: episode: 44, duration: 2.250s, episode steps: 680, steps per second: 302, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30970/3750000: episode: 45, duration: 3.863s, episode steps: 1160, steps per second: 300, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31353/3750000: episode: 46, duration: 1.219s, episode steps: 383, steps per second: 314, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31972/3750000: episode: 47, duration: 1.901s, episode steps: 619, steps per second: 326, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32592/3750000: episode: 48, duration: 2.053s, episode steps: 620, steps per second: 302, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33476/3750000: episode: 49, duration: 2.919s, episode steps: 884, steps per second: 303, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34606/3750000: episode: 50, duration: 3.723s, episode steps: 1130, steps per second: 304, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35368/3750000: episode: 51, duration: 2.537s, episode steps: 762, steps per second: 300, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35875/3750000: episode: 52, duration: 1.730s, episode steps: 507, steps per second: 293, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36500/3750000: episode: 53, duration: 2.098s, episode steps: 625, steps per second: 298, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37285/3750000: episode: 54, duration: 2.594s, episode steps: 785, steps per second: 303, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37926/3750000: episode: 55, duration: 2.122s, episode steps: 641, steps per second: 302, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38411/3750000: episode: 56, duration: 1.615s, episode steps: 485, steps per second: 300, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39062/3750000: episode: 57, duration: 2.167s, episode steps: 651, steps per second: 300, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39595/3750000: episode: 58, duration: 1.761s, episode steps: 533, steps per second: 303, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40092/3750000: episode: 59, duration: 1.654s, episode steps: 497, steps per second: 300, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40742/3750000: episode: 60, duration: 2.130s, episode steps: 650, steps per second: 305, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41138/3750000: episode: 61, duration: 1.338s, episode steps: 396, steps per second: 296, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41879/3750000: episode: 62, duration: 2.454s, episode steps: 741, steps per second: 302, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42281/3750000: episode: 63, duration: 1.349s, episode steps: 402, steps per second: 298, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42926/3750000: episode: 64, duration: 2.158s, episode steps: 645, steps per second: 299, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43468/3750000: episode: 65, duration: 1.813s, episode steps: 542, steps per second: 299, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44007/3750000: episode: 66, duration: 1.801s, episode steps: 539, steps per second: 299, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44904/3750000: episode: 67, duration: 2.825s, episode steps: 897, steps per second: 317, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45844/3750000: episode: 68, duration: 3.213s, episode steps: 940, steps per second: 293, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46377/3750000: episode: 69, duration: 1.751s, episode steps: 533, steps per second: 304, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47067/3750000: episode: 70, duration: 2.120s, episode steps: 690, steps per second: 325, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47826/3750000: episode: 71, duration: 2.479s, episode steps: 759, steps per second: 306, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48417/3750000: episode: 72, duration: 1.963s, episode steps: 591, steps per second: 301, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48915/3750000: episode: 73, duration: 1.654s, episode steps: 498, steps per second: 301, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49519/3750000: episode: 74, duration: 2.004s, episode steps: 604, steps per second: 301, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikel\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50111/3750000: episode: 75, duration: 2.967s, episode steps: 592, steps per second: 200, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009140, mae: 3.481089, mean_q: 4.196437, mean_eps: 0.981978\n",
      "   51010/3750000: episode: 76, duration: 6.693s, episode steps: 899, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.016818, mae: 3.412151, mean_q: 4.109675, mean_eps: 0.981798\n",
      "   51409/3750000: episode: 77, duration: 3.003s, episode steps: 399, steps per second: 133, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008783, mae: 3.354248, mean_q: 4.036198, mean_eps: 0.981564\n",
      "   52076/3750000: episode: 78, duration: 4.957s, episode steps: 667, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.009815, mae: 3.384423, mean_q: 4.075993, mean_eps: 0.981374\n",
      "   52587/3750000: episode: 79, duration: 3.915s, episode steps: 511, steps per second: 131, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009298, mae: 3.388521, mean_q: 4.086241, mean_eps: 0.981161\n",
      "   53195/3750000: episode: 80, duration: 4.545s, episode steps: 608, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.010612, mae: 3.390408, mean_q: 4.081592, mean_eps: 0.980960\n",
      "   53698/3750000: episode: 81, duration: 3.788s, episode steps: 503, steps per second: 133, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010407, mae: 3.419901, mean_q: 4.115411, mean_eps: 0.980762\n",
      "   54453/3750000: episode: 82, duration: 5.719s, episode steps: 755, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.016614, mae: 3.376054, mean_q: 4.063507, mean_eps: 0.980535\n",
      "   55123/3750000: episode: 83, duration: 5.081s, episode steps: 670, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.010631, mae: 3.439073, mean_q: 4.138282, mean_eps: 0.980276\n",
      "   55650/3750000: episode: 84, duration: 3.914s, episode steps: 527, steps per second: 135, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.018144, mae: 3.406322, mean_q: 4.099652, mean_eps: 0.980060\n",
      "   56492/3750000: episode: 85, duration: 6.302s, episode steps: 842, steps per second: 134, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.028805, mae: 3.372129, mean_q: 4.054923, mean_eps: 0.979815\n",
      "   57212/3750000: episode: 86, duration: 5.393s, episode steps: 720, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.029045, mae: 3.357591, mean_q: 4.046748, mean_eps: 0.979534\n",
      "   57788/3750000: episode: 87, duration: 4.307s, episode steps: 576, steps per second: 134, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014473, mae: 3.350871, mean_q: 4.033452, mean_eps: 0.979300\n",
      "   58516/3750000: episode: 88, duration: 5.398s, episode steps: 728, steps per second: 135, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.016889, mae: 3.377778, mean_q: 4.064259, mean_eps: 0.979066\n",
      "   59441/3750000: episode: 89, duration: 7.030s, episode steps: 925, steps per second: 132, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.016422, mae: 3.377636, mean_q: 4.067738, mean_eps: 0.978767\n",
      "   59862/3750000: episode: 90, duration: 3.142s, episode steps: 421, steps per second: 134, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.012414, mae: 3.377058, mean_q: 4.064408, mean_eps: 0.978522\n",
      "   60401/3750000: episode: 91, duration: 4.109s, episode steps: 539, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011028, mae: 3.340593, mean_q: 4.024132, mean_eps: 0.978350\n",
      "   61202/3750000: episode: 92, duration: 6.091s, episode steps: 801, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.013620, mae: 3.354896, mean_q: 4.040784, mean_eps: 0.978108\n",
      "   61848/3750000: episode: 93, duration: 4.773s, episode steps: 646, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.011155, mae: 3.378723, mean_q: 4.067897, mean_eps: 0.977849\n",
      "   62380/3750000: episode: 94, duration: 3.997s, episode steps: 532, steps per second: 133, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.012244, mae: 3.421622, mean_q: 4.121405, mean_eps: 0.977640\n",
      "   62992/3750000: episode: 95, duration: 4.658s, episode steps: 612, steps per second: 131, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.017207, mae: 3.314571, mean_q: 3.989221, mean_eps: 0.977435\n",
      "   63727/3750000: episode: 96, duration: 5.481s, episode steps: 735, steps per second: 134, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.008696, mae: 3.357379, mean_q: 4.046031, mean_eps: 0.977190\n",
      "   64270/3750000: episode: 97, duration: 4.084s, episode steps: 543, steps per second: 133, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.019732, mae: 3.391493, mean_q: 4.085063, mean_eps: 0.976960\n",
      "   64668/3750000: episode: 98, duration: 2.976s, episode steps: 398, steps per second: 134, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.008010, mae: 3.359460, mean_q: 4.041839, mean_eps: 0.976791\n",
      "   65479/3750000: episode: 99, duration: 5.973s, episode steps: 811, steps per second: 136, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.018244, mae: 3.438025, mean_q: 4.139919, mean_eps: 0.976575\n",
      "   66511/3750000: episode: 100, duration: 7.733s, episode steps: 1032, steps per second: 133, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.019307, mae: 3.421129, mean_q: 4.116119, mean_eps: 0.976244\n",
      "   67208/3750000: episode: 101, duration: 5.310s, episode steps: 697, steps per second: 131, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.010238, mae: 3.406391, mean_q: 4.103907, mean_eps: 0.975930\n",
      "   67761/3750000: episode: 102, duration: 4.198s, episode steps: 553, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.018676, mae: 3.440384, mean_q: 4.141194, mean_eps: 0.975704\n",
      "   68504/3750000: episode: 103, duration: 5.570s, episode steps: 743, steps per second: 133, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.016715, mae: 3.418841, mean_q: 4.117264, mean_eps: 0.975470\n",
      "   69223/3750000: episode: 104, duration: 5.466s, episode steps: 719, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.013060, mae: 3.407664, mean_q: 4.097075, mean_eps: 0.975207\n",
      "   69609/3750000: episode: 105, duration: 2.894s, episode steps: 386, steps per second: 133, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010861, mae: 3.455509, mean_q: 4.154393, mean_eps: 0.975009\n",
      "   70240/3750000: episode: 106, duration: 4.758s, episode steps: 631, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012995, mae: 3.367729, mean_q: 4.054407, mean_eps: 0.974829\n",
      "   70750/3750000: episode: 107, duration: 3.889s, episode steps: 510, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.012451, mae: 3.357004, mean_q: 4.040234, mean_eps: 0.974624\n",
      "   71554/3750000: episode: 108, duration: 6.063s, episode steps: 804, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.017614, mae: 3.393402, mean_q: 4.084813, mean_eps: 0.974386\n",
      "   71933/3750000: episode: 109, duration: 2.825s, episode steps: 379, steps per second: 134, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.013643, mae: 3.422192, mean_q: 4.121914, mean_eps: 0.974174\n",
      "   72437/3750000: episode: 110, duration: 3.750s, episode steps: 504, steps per second: 134, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.012546, mae: 3.438181, mean_q: 4.140483, mean_eps: 0.974015\n",
      "   73653/3750000: episode: 111, duration: 9.178s, episode steps: 1216, steps per second: 132, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.017628, mae: 3.463855, mean_q: 4.166274, mean_eps: 0.973706\n",
      "   74043/3750000: episode: 112, duration: 2.964s, episode steps: 390, steps per second: 132, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010251, mae: 3.416348, mean_q: 4.110447, mean_eps: 0.973414\n",
      "   74427/3750000: episode: 113, duration: 2.850s, episode steps: 384, steps per second: 135, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.016721, mae: 3.407454, mean_q: 4.101666, mean_eps: 0.973274\n",
      "   74958/3750000: episode: 114, duration: 3.921s, episode steps: 531, steps per second: 135, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.017926, mae: 3.440273, mean_q: 4.144754, mean_eps: 0.973112\n",
      "   75647/3750000: episode: 115, duration: 5.243s, episode steps: 689, steps per second: 131, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.015831, mae: 3.451868, mean_q: 4.163115, mean_eps: 0.972892\n",
      "   76467/3750000: episode: 116, duration: 6.149s, episode steps: 820, steps per second: 133, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.010457, mae: 3.454848, mean_q: 4.158854, mean_eps: 0.972618\n",
      "   77246/3750000: episode: 117, duration: 5.831s, episode steps: 779, steps per second: 134, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.014816, mae: 3.473891, mean_q: 4.178677, mean_eps: 0.972330\n",
      "   77915/3750000: episode: 118, duration: 5.013s, episode steps: 669, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.010013, mae: 3.474900, mean_q: 4.179314, mean_eps: 0.972071\n",
      "   78463/3750000: episode: 119, duration: 4.154s, episode steps: 548, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.009884, mae: 3.420244, mean_q: 4.111477, mean_eps: 0.971852\n",
      "   79303/3750000: episode: 120, duration: 6.292s, episode steps: 840, steps per second: 133, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.017033, mae: 3.358411, mean_q: 4.043597, mean_eps: 0.971600\n",
      "   79803/3750000: episode: 121, duration: 3.788s, episode steps: 500, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.012102, mae: 3.410836, mean_q: 4.100520, mean_eps: 0.971358\n",
      "   80275/3750000: episode: 122, duration: 3.539s, episode steps: 472, steps per second: 133, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014285, mae: 3.369630, mean_q: 4.059689, mean_eps: 0.971186\n",
      "   81205/3750000: episode: 123, duration: 7.001s, episode steps: 930, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.010360, mae: 3.369477, mean_q: 4.058920, mean_eps: 0.970934\n",
      "   82024/3750000: episode: 124, duration: 6.224s, episode steps: 819, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.007858, mae: 3.378785, mean_q: 4.070376, mean_eps: 0.970617\n",
      "   82813/3750000: episode: 125, duration: 5.887s, episode steps: 789, steps per second: 134, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.008660, mae: 3.389628, mean_q: 4.078201, mean_eps: 0.970329\n",
      "   83491/3750000: episode: 126, duration: 5.113s, episode steps: 678, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.015918, mae: 3.370317, mean_q: 4.057278, mean_eps: 0.970066\n",
      "   84324/3750000: episode: 127, duration: 6.331s, episode steps: 833, steps per second: 132, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.012418, mae: 3.425089, mean_q: 4.124649, mean_eps: 0.969792\n",
      "   85085/3750000: episode: 128, duration: 5.748s, episode steps: 761, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012833, mae: 3.377303, mean_q: 4.065765, mean_eps: 0.969504\n",
      "   85710/3750000: episode: 129, duration: 4.665s, episode steps: 625, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.011717, mae: 3.344996, mean_q: 4.031256, mean_eps: 0.969256\n",
      "   86146/3750000: episode: 130, duration: 3.292s, episode steps: 436, steps per second: 132, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.015564, mae: 3.392837, mean_q: 4.086676, mean_eps: 0.969065\n",
      "   87224/3750000: episode: 131, duration: 8.144s, episode steps: 1078, steps per second: 132, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013228, mae: 3.383696, mean_q: 4.076304, mean_eps: 0.968792\n",
      "   88018/3750000: episode: 132, duration: 5.901s, episode steps: 794, steps per second: 135, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011025, mae: 3.367954, mean_q: 4.057417, mean_eps: 0.968457\n",
      "   88417/3750000: episode: 133, duration: 3.078s, episode steps: 399, steps per second: 130, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.007816, mae: 3.324819, mean_q: 4.008640, mean_eps: 0.968244\n",
      "   89289/3750000: episode: 134, duration: 6.628s, episode steps: 872, steps per second: 132, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.009416, mae: 3.367020, mean_q: 4.058543, mean_eps: 0.968014\n",
      "   89932/3750000: episode: 135, duration: 4.821s, episode steps: 643, steps per second: 133, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.010631, mae: 3.354627, mean_q: 4.040052, mean_eps: 0.967740\n",
      "   90589/3750000: episode: 136, duration: 5.006s, episode steps: 657, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.007548, mae: 3.334721, mean_q: 4.016876, mean_eps: 0.967506\n",
      "   90970/3750000: episode: 137, duration: 2.861s, episode steps: 381, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015854, mae: 3.369388, mean_q: 4.053504, mean_eps: 0.967319\n",
      "   91469/3750000: episode: 138, duration: 3.780s, episode steps: 499, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.025190, mae: 3.388528, mean_q: 4.078833, mean_eps: 0.967161\n",
      "   92136/3750000: episode: 139, duration: 4.968s, episode steps: 667, steps per second: 134, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.024505, mae: 3.330248, mean_q: 4.022515, mean_eps: 0.966952\n",
      "   92915/3750000: episode: 140, duration: 5.886s, episode steps: 779, steps per second: 132, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.026188, mae: 3.343316, mean_q: 4.028848, mean_eps: 0.966693\n",
      "   93315/3750000: episode: 141, duration: 3.058s, episode steps: 400, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.012544, mae: 3.345155, mean_q: 4.028064, mean_eps: 0.966480\n",
      "   94543/3750000: episode: 142, duration: 9.295s, episode steps: 1228, steps per second: 132, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.011039, mae: 3.379429, mean_q: 4.073502, mean_eps: 0.966185\n",
      "   94991/3750000: episode: 143, duration: 3.395s, episode steps: 448, steps per second: 132, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016243, mae: 3.334070, mean_q: 4.012844, mean_eps: 0.965883\n",
      "   95600/3750000: episode: 144, duration: 4.635s, episode steps: 609, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.008365, mae: 3.330868, mean_q: 4.008570, mean_eps: 0.965696\n",
      "   96425/3750000: episode: 145, duration: 6.240s, episode steps: 825, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013337, mae: 3.360260, mean_q: 4.053300, mean_eps: 0.965436\n",
      "   97171/3750000: episode: 146, duration: 5.621s, episode steps: 746, steps per second: 133, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.018268, mae: 3.375015, mean_q: 4.066817, mean_eps: 0.965152\n",
      "   97620/3750000: episode: 147, duration: 3.416s, episode steps: 449, steps per second: 131, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.018105, mae: 3.416499, mean_q: 4.113950, mean_eps: 0.964940\n",
      "   98419/3750000: episode: 148, duration: 6.172s, episode steps: 799, steps per second: 129, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.017614, mae: 3.428201, mean_q: 4.137217, mean_eps: 0.964716\n",
      "   99335/3750000: episode: 149, duration: 6.990s, episode steps: 916, steps per second: 131, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.017123, mae: 3.357341, mean_q: 4.044980, mean_eps: 0.964407\n",
      "  100189/3750000: episode: 150, duration: 6.447s, episode steps: 854, steps per second: 132, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.013174, mae: 3.375262, mean_q: 4.065556, mean_eps: 0.964086\n",
      "  101015/3750000: episode: 151, duration: 6.251s, episode steps: 826, steps per second: 132, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.016882, mae: 3.372075, mean_q: 4.055797, mean_eps: 0.963784\n",
      "  101593/3750000: episode: 152, duration: 4.388s, episode steps: 578, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.014906, mae: 3.306627, mean_q: 3.978535, mean_eps: 0.963532\n",
      "  102395/3750000: episode: 153, duration: 5.995s, episode steps: 802, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013718, mae: 3.346477, mean_q: 4.026628, mean_eps: 0.963284\n",
      "  102979/3750000: episode: 154, duration: 4.354s, episode steps: 584, steps per second: 134, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.015020, mae: 3.295988, mean_q: 3.965389, mean_eps: 0.963035\n",
      "  103569/3750000: episode: 155, duration: 4.537s, episode steps: 590, steps per second: 130, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.015349, mae: 3.307316, mean_q: 3.980501, mean_eps: 0.962823\n",
      "  104575/3750000: episode: 156, duration: 7.498s, episode steps: 1006, steps per second: 134, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017629, mae: 3.321276, mean_q: 4.006323, mean_eps: 0.962535\n",
      "  105103/3750000: episode: 157, duration: 4.014s, episode steps: 528, steps per second: 132, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.019008, mae: 3.313250, mean_q: 3.992467, mean_eps: 0.962258\n",
      "  105587/3750000: episode: 158, duration: 3.666s, episode steps: 484, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011742, mae: 3.338633, mean_q: 4.025025, mean_eps: 0.962074\n",
      "  106277/3750000: episode: 159, duration: 5.121s, episode steps: 690, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.029727, mae: 3.390729, mean_q: 4.084984, mean_eps: 0.961865\n",
      "  106874/3750000: episode: 160, duration: 4.483s, episode steps: 597, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.011991, mae: 3.380543, mean_q: 4.072787, mean_eps: 0.961635\n",
      "  107506/3750000: episode: 161, duration: 4.844s, episode steps: 632, steps per second: 130, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.013143, mae: 3.355150, mean_q: 4.037182, mean_eps: 0.961412\n",
      "  108242/3750000: episode: 162, duration: 5.638s, episode steps: 736, steps per second: 131, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.009855, mae: 3.304388, mean_q: 3.976363, mean_eps: 0.961163\n",
      "  109183/3750000: episode: 163, duration: 6.935s, episode steps: 941, steps per second: 136, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.022280, mae: 3.310188, mean_q: 3.983525, mean_eps: 0.960861\n",
      "  110363/3750000: episode: 164, duration: 8.867s, episode steps: 1180, steps per second: 133, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014055, mae: 3.307904, mean_q: 3.983978, mean_eps: 0.960479\n",
      "  111150/3750000: episode: 165, duration: 5.815s, episode steps: 787, steps per second: 135, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.014317, mae: 3.298035, mean_q: 3.971695, mean_eps: 0.960126\n",
      "  111665/3750000: episode: 166, duration: 3.887s, episode steps: 515, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.016421, mae: 3.368783, mean_q: 4.053343, mean_eps: 0.959892\n",
      "  112044/3750000: episode: 167, duration: 2.891s, episode steps: 379, steps per second: 131, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.013812, mae: 3.307645, mean_q: 3.983023, mean_eps: 0.959730\n",
      "  112732/3750000: episode: 168, duration: 5.072s, episode steps: 688, steps per second: 136, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.017042, mae: 3.299843, mean_q: 3.974105, mean_eps: 0.959540\n",
      "  113370/3750000: episode: 169, duration: 4.784s, episode steps: 638, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.009560, mae: 3.294568, mean_q: 3.968621, mean_eps: 0.959302\n",
      "  114193/3750000: episode: 170, duration: 6.203s, episode steps: 823, steps per second: 133, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.013483, mae: 3.301984, mean_q: 3.974970, mean_eps: 0.959039\n",
      "  114979/3750000: episode: 171, duration: 5.829s, episode steps: 786, steps per second: 135, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.011158, mae: 3.300038, mean_q: 3.969137, mean_eps: 0.958751\n",
      "  115654/3750000: episode: 172, duration: 5.133s, episode steps: 675, steps per second: 131, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.014671, mae: 3.294140, mean_q: 3.968584, mean_eps: 0.958488\n",
      "  116026/3750000: episode: 173, duration: 2.915s, episode steps: 372, steps per second: 128, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.011577, mae: 3.271852, mean_q: 3.941565, mean_eps: 0.958298\n",
      "  116667/3750000: episode: 174, duration: 4.801s, episode steps: 641, steps per second: 134, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.010385, mae: 3.330137, mean_q: 4.006409, mean_eps: 0.958114\n",
      "  117211/3750000: episode: 175, duration: 4.143s, episode steps: 544, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014857, mae: 3.294361, mean_q: 3.972067, mean_eps: 0.957902\n",
      "  117767/3750000: episode: 176, duration: 4.230s, episode steps: 556, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014770, mae: 3.275081, mean_q: 3.949278, mean_eps: 0.957704\n",
      "  118273/3750000: episode: 177, duration: 3.830s, episode steps: 506, steps per second: 132, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015856, mae: 3.351916, mean_q: 4.040868, mean_eps: 0.957513\n",
      "  118655/3750000: episode: 178, duration: 2.855s, episode steps: 382, steps per second: 134, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.012954, mae: 3.266095, mean_q: 3.930537, mean_eps: 0.957354\n",
      "  120100/3750000: episode: 179, duration: 10.833s, episode steps: 1445, steps per second: 133, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.019553, mae: 3.353416, mean_q: 4.037682, mean_eps: 0.957027\n",
      "  120659/3750000: episode: 180, duration: 4.213s, episode steps: 559, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.017970, mae: 3.364203, mean_q: 4.050896, mean_eps: 0.956667\n",
      "  121239/3750000: episode: 181, duration: 4.346s, episode steps: 580, steps per second: 133, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.018499, mae: 3.318580, mean_q: 3.996560, mean_eps: 0.956462\n",
      "  122071/3750000: episode: 182, duration: 6.247s, episode steps: 832, steps per second: 133, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.017091, mae: 3.320581, mean_q: 3.999076, mean_eps: 0.956206\n",
      "  122807/3750000: episode: 183, duration: 5.557s, episode steps: 736, steps per second: 132, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.018302, mae: 3.323490, mean_q: 4.007253, mean_eps: 0.955922\n",
      "  124033/3750000: episode: 184, duration: 9.108s, episode steps: 1226, steps per second: 135, episode reward: 12.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.013045, mae: 3.360652, mean_q: 4.047785, mean_eps: 0.955569\n",
      "  124575/3750000: episode: 185, duration: 4.116s, episode steps: 542, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010698, mae: 3.367556, mean_q: 4.054406, mean_eps: 0.955252\n",
      "  125338/3750000: episode: 186, duration: 5.669s, episode steps: 763, steps per second: 135, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.012762, mae: 3.328445, mean_q: 4.008264, mean_eps: 0.955018\n",
      "  126146/3750000: episode: 187, duration: 6.087s, episode steps: 808, steps per second: 133, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.018298, mae: 3.337913, mean_q: 4.016031, mean_eps: 0.954734\n",
      "  127022/3750000: episode: 188, duration: 6.616s, episode steps: 876, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.014617, mae: 3.301611, mean_q: 3.974993, mean_eps: 0.954428\n",
      "  127835/3750000: episode: 189, duration: 6.007s, episode steps: 813, steps per second: 135, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.012076, mae: 3.305260, mean_q: 3.976933, mean_eps: 0.954125\n",
      "  128281/3750000: episode: 190, duration: 3.397s, episode steps: 446, steps per second: 131, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.020107, mae: 3.305285, mean_q: 3.979240, mean_eps: 0.953898\n",
      "  129059/3750000: episode: 191, duration: 5.767s, episode steps: 778, steps per second: 135, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.012279, mae: 3.299205, mean_q: 3.974158, mean_eps: 0.953679\n",
      "  129663/3750000: episode: 192, duration: 4.584s, episode steps: 604, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.017138, mae: 3.351608, mean_q: 4.044981, mean_eps: 0.953430\n",
      "  130048/3750000: episode: 193, duration: 2.850s, episode steps: 385, steps per second: 135, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.025934, mae: 3.328121, mean_q: 4.026079, mean_eps: 0.953250\n",
      "  130449/3750000: episode: 194, duration: 2.990s, episode steps: 401, steps per second: 134, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.014182, mae: 3.310752, mean_q: 3.991422, mean_eps: 0.953110\n",
      "  131015/3750000: episode: 195, duration: 4.292s, episode steps: 566, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013274, mae: 3.376041, mean_q: 4.070737, mean_eps: 0.952937\n",
      "  132375/3750000: episode: 196, duration: 10.138s, episode steps: 1360, steps per second: 134, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.013142, mae: 3.370590, mean_q: 4.060942, mean_eps: 0.952592\n",
      "  133127/3750000: episode: 197, duration: 5.841s, episode steps: 752, steps per second: 129, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.009308, mae: 3.375423, mean_q: 4.065475, mean_eps: 0.952210\n",
      "  133726/3750000: episode: 198, duration: 4.475s, episode steps: 599, steps per second: 134, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.015277, mae: 3.343297, mean_q: 4.024340, mean_eps: 0.951965\n",
      "  134252/3750000: episode: 199, duration: 3.936s, episode steps: 526, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.012662, mae: 3.358694, mean_q: 4.045839, mean_eps: 0.951764\n",
      "  134768/3750000: episode: 200, duration: 3.901s, episode steps: 516, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.022938, mae: 3.421826, mean_q: 4.135697, mean_eps: 0.951576\n",
      "  135544/3750000: episode: 201, duration: 5.879s, episode steps: 776, steps per second: 132, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.017656, mae: 3.413045, mean_q: 4.113109, mean_eps: 0.951342\n",
      "  136265/3750000: episode: 202, duration: 5.464s, episode steps: 721, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.012104, mae: 3.337855, mean_q: 4.016884, mean_eps: 0.951072\n",
      "  136805/3750000: episode: 203, duration: 4.032s, episode steps: 540, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015057, mae: 3.382491, mean_q: 4.070955, mean_eps: 0.950846\n",
      "  137298/3750000: episode: 204, duration: 3.690s, episode steps: 493, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.007792, mae: 3.360114, mean_q: 4.047886, mean_eps: 0.950662\n",
      "  138405/3750000: episode: 205, duration: 8.310s, episode steps: 1107, steps per second: 133, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011283, mae: 3.357755, mean_q: 4.043905, mean_eps: 0.950374\n",
      "  138977/3750000: episode: 206, duration: 4.208s, episode steps: 572, steps per second: 136, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.008090, mae: 3.299614, mean_q: 3.972984, mean_eps: 0.950072\n",
      "  139432/3750000: episode: 207, duration: 3.491s, episode steps: 455, steps per second: 130, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.011987, mae: 3.342441, mean_q: 4.027356, mean_eps: 0.949888\n",
      "  140204/3750000: episode: 208, duration: 5.816s, episode steps: 772, steps per second: 133, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.021223, mae: 3.378794, mean_q: 4.076278, mean_eps: 0.949665\n",
      "  141070/3750000: episode: 209, duration: 6.432s, episode steps: 866, steps per second: 135, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.013297, mae: 3.354610, mean_q: 4.039298, mean_eps: 0.949370\n",
      "  142023/3750000: episode: 210, duration: 7.232s, episode steps: 953, steps per second: 132, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.015791, mae: 3.341363, mean_q: 4.036544, mean_eps: 0.949042\n",
      "  142754/3750000: episode: 211, duration: 5.385s, episode steps: 731, steps per second: 136, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.011434, mae: 3.362660, mean_q: 4.063989, mean_eps: 0.948740\n",
      "  143336/3750000: episode: 212, duration: 4.340s, episode steps: 582, steps per second: 134, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.010688, mae: 3.394179, mean_q: 4.090406, mean_eps: 0.948506\n",
      "  143872/3750000: episode: 213, duration: 4.117s, episode steps: 536, steps per second: 130, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.011502, mae: 3.341612, mean_q: 4.026782, mean_eps: 0.948304\n",
      "  144758/3750000: episode: 214, duration: 6.607s, episode steps: 886, steps per second: 134, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.015081, mae: 3.317448, mean_q: 3.992163, mean_eps: 0.948048\n",
      "  145465/3750000: episode: 215, duration: 5.351s, episode steps: 707, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012584, mae: 3.357291, mean_q: 4.041872, mean_eps: 0.947760\n",
      "  146092/3750000: episode: 216, duration: 4.726s, episode steps: 627, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.019096, mae: 3.295095, mean_q: 3.966888, mean_eps: 0.947519\n",
      "  146754/3750000: episode: 217, duration: 4.955s, episode steps: 662, steps per second: 134, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.020001, mae: 3.311292, mean_q: 3.995614, mean_eps: 0.947289\n",
      "  147221/3750000: episode: 218, duration: 3.559s, episode steps: 467, steps per second: 131, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.027671, mae: 3.306047, mean_q: 3.987364, mean_eps: 0.947084\n",
      "  147890/3750000: episode: 219, duration: 4.992s, episode steps: 669, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.016116, mae: 3.374401, mean_q: 4.076312, mean_eps: 0.946878\n",
      "  148780/3750000: episode: 220, duration: 6.768s, episode steps: 890, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.013905, mae: 3.346668, mean_q: 4.031850, mean_eps: 0.946601\n",
      "  149444/3750000: episode: 221, duration: 5.101s, episode steps: 664, steps per second: 130, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013934, mae: 3.371403, mean_q: 4.061678, mean_eps: 0.946320\n",
      "  149845/3750000: episode: 222, duration: 3.007s, episode steps: 401, steps per second: 133, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017592, mae: 3.421444, mean_q: 4.117462, mean_eps: 0.946126\n",
      "  150425/3750000: episode: 223, duration: 4.425s, episode steps: 580, steps per second: 131, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.011552, mae: 3.420194, mean_q: 4.121230, mean_eps: 0.945950\n",
      "  151140/3750000: episode: 224, duration: 5.348s, episode steps: 715, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.015216, mae: 3.379527, mean_q: 4.073221, mean_eps: 0.945719\n",
      "  151947/3750000: episode: 225, duration: 6.113s, episode steps: 807, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.017935, mae: 3.357892, mean_q: 4.046470, mean_eps: 0.945446\n",
      "  152458/3750000: episode: 226, duration: 3.880s, episode steps: 511, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.023471, mae: 3.485014, mean_q: 4.197601, mean_eps: 0.945208\n",
      "  153050/3750000: episode: 227, duration: 4.432s, episode steps: 592, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.013430, mae: 3.458391, mean_q: 4.165853, mean_eps: 0.945010\n",
      "  154056/3750000: episode: 228, duration: 7.482s, episode steps: 1006, steps per second: 134, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.017058, mae: 3.434049, mean_q: 4.137396, mean_eps: 0.944722\n",
      "  154728/3750000: episode: 229, duration: 5.096s, episode steps: 672, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.018994, mae: 3.416680, mean_q: 4.115537, mean_eps: 0.944420\n",
      "  155406/3750000: episode: 230, duration: 5.100s, episode steps: 678, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.023731, mae: 3.451493, mean_q: 4.160885, mean_eps: 0.944175\n",
      "  155822/3750000: episode: 231, duration: 3.172s, episode steps: 416, steps per second: 131, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.010236, mae: 3.433561, mean_q: 4.136391, mean_eps: 0.943977\n",
      "  157009/3750000: episode: 232, duration: 8.934s, episode steps: 1187, steps per second: 133, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012639, mae: 3.452351, mean_q: 4.168823, mean_eps: 0.943689\n",
      "  157528/3750000: episode: 233, duration: 3.899s, episode steps: 519, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.009341, mae: 3.495292, mean_q: 4.210404, mean_eps: 0.943383\n",
      "  157923/3750000: episode: 234, duration: 2.964s, episode steps: 395, steps per second: 133, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.016464, mae: 3.480155, mean_q: 4.188514, mean_eps: 0.943217\n",
      "  158530/3750000: episode: 235, duration: 4.538s, episode steps: 607, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.012814, mae: 3.419950, mean_q: 4.115968, mean_eps: 0.943037\n",
      "  159176/3750000: episode: 236, duration: 4.848s, episode steps: 646, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.014117, mae: 3.422849, mean_q: 4.121941, mean_eps: 0.942814\n",
      "  159913/3750000: episode: 237, duration: 5.577s, episode steps: 737, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.013792, mae: 3.429666, mean_q: 4.132492, mean_eps: 0.942566\n",
      "  160583/3750000: episode: 238, duration: 5.106s, episode steps: 670, steps per second: 131, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.018464, mae: 3.510371, mean_q: 4.232318, mean_eps: 0.942310\n",
      "  161082/3750000: episode: 239, duration: 3.783s, episode steps: 499, steps per second: 132, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.019958, mae: 3.487703, mean_q: 4.202441, mean_eps: 0.942098\n",
      "  161731/3750000: episode: 240, duration: 4.833s, episode steps: 649, steps per second: 134, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016694, mae: 3.411329, mean_q: 4.109935, mean_eps: 0.941892\n",
      "  162239/3750000: episode: 241, duration: 3.775s, episode steps: 508, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.011796, mae: 3.445598, mean_q: 4.155181, mean_eps: 0.941687\n",
      "  163358/3750000: episode: 242, duration: 8.444s, episode steps: 1119, steps per second: 133, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.014446, mae: 3.461840, mean_q: 4.165663, mean_eps: 0.941396\n",
      "  163848/3750000: episode: 243, duration: 3.724s, episode steps: 490, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.011216, mae: 3.451424, mean_q: 4.158334, mean_eps: 0.941104\n",
      "  164254/3750000: episode: 244, duration: 3.036s, episode steps: 406, steps per second: 134, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.015224, mae: 3.470576, mean_q: 4.185661, mean_eps: 0.940942\n",
      "  165096/3750000: episode: 245, duration: 6.422s, episode steps: 842, steps per second: 131, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.017065, mae: 3.462993, mean_q: 4.179035, mean_eps: 0.940719\n",
      "  165848/3750000: episode: 246, duration: 5.667s, episode steps: 752, steps per second: 133, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014321, mae: 3.419895, mean_q: 4.120341, mean_eps: 0.940431\n",
      "  166589/3750000: episode: 247, duration: 5.516s, episode steps: 741, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011402, mae: 3.462281, mean_q: 4.169085, mean_eps: 0.940161\n",
      "  167235/3750000: episode: 248, duration: 4.886s, episode steps: 646, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016418, mae: 3.450518, mean_q: 4.161596, mean_eps: 0.939912\n",
      "  167909/3750000: episode: 249, duration: 5.078s, episode steps: 674, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.020980, mae: 3.435699, mean_q: 4.135629, mean_eps: 0.939675\n",
      "  168640/3750000: episode: 250, duration: 5.524s, episode steps: 731, steps per second: 132, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.011267, mae: 3.469113, mean_q: 4.183002, mean_eps: 0.939423\n",
      "  169248/3750000: episode: 251, duration: 4.648s, episode steps: 608, steps per second: 131, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013042, mae: 3.488391, mean_q: 4.198720, mean_eps: 0.939182\n",
      "  169663/3750000: episode: 252, duration: 3.124s, episode steps: 415, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.023691, mae: 3.450745, mean_q: 4.156494, mean_eps: 0.938994\n",
      "  170036/3750000: episode: 253, duration: 2.745s, episode steps: 373, steps per second: 136, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.031724, mae: 3.473236, mean_q: 4.191766, mean_eps: 0.938854\n",
      "  170501/3750000: episode: 254, duration: 3.550s, episode steps: 465, steps per second: 131, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.031250, mae: 3.440536, mean_q: 4.147257, mean_eps: 0.938703\n",
      "  171154/3750000: episode: 255, duration: 4.855s, episode steps: 653, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.018406, mae: 3.478532, mean_q: 4.189491, mean_eps: 0.938501\n",
      "  171801/3750000: episode: 256, duration: 4.961s, episode steps: 647, steps per second: 130, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.015941, mae: 3.489946, mean_q: 4.200220, mean_eps: 0.938267\n",
      "  172346/3750000: episode: 257, duration: 4.077s, episode steps: 545, steps per second: 134, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.022332, mae: 3.473276, mean_q: 4.181243, mean_eps: 0.938051\n",
      "  173111/3750000: episode: 258, duration: 5.739s, episode steps: 765, steps per second: 133, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012908, mae: 3.421146, mean_q: 4.115889, mean_eps: 0.937817\n",
      "  173990/3750000: episode: 259, duration: 6.634s, episode steps: 879, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012703, mae: 3.356297, mean_q: 4.038175, mean_eps: 0.937522\n",
      "  174546/3750000: episode: 260, duration: 4.210s, episode steps: 556, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.017489, mae: 3.425505, mean_q: 4.121885, mean_eps: 0.937263\n",
      "  175197/3750000: episode: 261, duration: 4.830s, episode steps: 651, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.019252, mae: 3.376971, mean_q: 4.062478, mean_eps: 0.937047\n",
      "  175927/3750000: episode: 262, duration: 5.766s, episode steps: 730, steps per second: 127, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.012170, mae: 3.385828, mean_q: 4.076617, mean_eps: 0.936798\n",
      "  176448/3750000: episode: 263, duration: 3.924s, episode steps: 521, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.015923, mae: 3.451360, mean_q: 4.153287, mean_eps: 0.936572\n",
      "  177339/3750000: episode: 264, duration: 6.620s, episode steps: 891, steps per second: 135, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.010381, mae: 3.418335, mean_q: 4.117363, mean_eps: 0.936320\n",
      "  177939/3750000: episode: 265, duration: 4.550s, episode steps: 600, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.016062, mae: 3.436096, mean_q: 4.134575, mean_eps: 0.936053\n",
      "  178451/3750000: episode: 266, duration: 3.895s, episode steps: 512, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.019823, mae: 3.393062, mean_q: 4.078654, mean_eps: 0.935852\n",
      "  179173/3750000: episode: 267, duration: 5.576s, episode steps: 722, steps per second: 129, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.018577, mae: 3.377002, mean_q: 4.064650, mean_eps: 0.935628\n",
      "  179790/3750000: episode: 268, duration: 4.714s, episode steps: 617, steps per second: 131, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.012625, mae: 3.407613, mean_q: 4.103772, mean_eps: 0.935387\n",
      "  180488/3750000: episode: 269, duration: 5.603s, episode steps: 698, steps per second: 125, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.016877, mae: 3.446996, mean_q: 4.148550, mean_eps: 0.935150\n",
      "  181137/3750000: episode: 270, duration: 4.932s, episode steps: 649, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.012410, mae: 3.408616, mean_q: 4.100925, mean_eps: 0.934908\n",
      "  181749/3750000: episode: 271, duration: 4.646s, episode steps: 612, steps per second: 132, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.014381, mae: 3.407352, mean_q: 4.103730, mean_eps: 0.934682\n",
      "  182394/3750000: episode: 272, duration: 4.857s, episode steps: 645, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.024908, mae: 3.401412, mean_q: 4.098002, mean_eps: 0.934455\n",
      "  183028/3750000: episode: 273, duration: 4.763s, episode steps: 634, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.012181, mae: 3.382838, mean_q: 4.083626, mean_eps: 0.934224\n",
      "  183787/3750000: episode: 274, duration: 5.669s, episode steps: 759, steps per second: 134, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.025250, mae: 3.476238, mean_q: 4.182758, mean_eps: 0.933972\n",
      "  184421/3750000: episode: 275, duration: 4.817s, episode steps: 634, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.014530, mae: 3.408380, mean_q: 4.105452, mean_eps: 0.933720\n",
      "  184866/3750000: episode: 276, duration: 3.316s, episode steps: 445, steps per second: 134, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.016416, mae: 3.350521, mean_q: 4.035164, mean_eps: 0.933526\n",
      "  185405/3750000: episode: 277, duration: 4.020s, episode steps: 539, steps per second: 134, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.013417, mae: 3.440389, mean_q: 4.146386, mean_eps: 0.933350\n",
      "  186171/3750000: episode: 278, duration: 5.773s, episode steps: 766, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.015172, mae: 3.461461, mean_q: 4.167810, mean_eps: 0.933116\n",
      "  186676/3750000: episode: 279, duration: 3.755s, episode steps: 505, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.018646, mae: 3.457010, mean_q: 4.173914, mean_eps: 0.932889\n",
      "  187394/3750000: episode: 280, duration: 5.372s, episode steps: 718, steps per second: 134, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.014815, mae: 3.455373, mean_q: 4.164375, mean_eps: 0.932669\n",
      "  188283/3750000: episode: 281, duration: 6.723s, episode steps: 889, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.019695, mae: 3.506168, mean_q: 4.226597, mean_eps: 0.932378\n",
      "  188990/3750000: episode: 282, duration: 5.235s, episode steps: 707, steps per second: 135, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.013689, mae: 3.427886, mean_q: 4.134881, mean_eps: 0.932090\n",
      "  189362/3750000: episode: 283, duration: 2.795s, episode steps: 372, steps per second: 133, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.022914, mae: 3.409964, mean_q: 4.103139, mean_eps: 0.931895\n",
      "  189872/3750000: episode: 284, duration: 3.746s, episode steps: 510, steps per second: 136, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.021310, mae: 3.480420, mean_q: 4.188499, mean_eps: 0.931737\n",
      "  190443/3750000: episode: 285, duration: 4.400s, episode steps: 571, steps per second: 130, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013547, mae: 3.453463, mean_q: 4.166209, mean_eps: 0.931542\n",
      "  191316/3750000: episode: 286, duration: 6.496s, episode steps: 873, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014050, mae: 3.430078, mean_q: 4.126766, mean_eps: 0.931283\n",
      "  192341/3750000: episode: 287, duration: 7.685s, episode steps: 1025, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011209, mae: 3.470624, mean_q: 4.177322, mean_eps: 0.930941\n",
      "  193480/3750000: episode: 288, duration: 8.594s, episode steps: 1139, steps per second: 133, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.021364, mae: 3.459235, mean_q: 4.165086, mean_eps: 0.930552\n",
      "  194136/3750000: episode: 289, duration: 4.919s, episode steps: 656, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.017063, mae: 3.466702, mean_q: 4.175564, mean_eps: 0.930232\n",
      "  194967/3750000: episode: 290, duration: 6.280s, episode steps: 831, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.019344, mae: 3.467149, mean_q: 4.171336, mean_eps: 0.929962\n",
      "  195548/3750000: episode: 291, duration: 4.333s, episode steps: 581, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.018298, mae: 3.432658, mean_q: 4.134373, mean_eps: 0.929706\n",
      "  196186/3750000: episode: 292, duration: 4.834s, episode steps: 638, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.019112, mae: 3.430063, mean_q: 4.124891, mean_eps: 0.929487\n",
      "  197069/3750000: episode: 293, duration: 6.644s, episode steps: 883, steps per second: 133, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.011684, mae: 3.445505, mean_q: 4.148844, mean_eps: 0.929213\n",
      "  197725/3750000: episode: 294, duration: 4.920s, episode steps: 656, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.031573, mae: 3.421560, mean_q: 4.124057, mean_eps: 0.928936\n",
      "  198222/3750000: episode: 295, duration: 3.756s, episode steps: 497, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.019377, mae: 3.383130, mean_q: 4.070666, mean_eps: 0.928727\n",
      "  198622/3750000: episode: 296, duration: 2.968s, episode steps: 400, steps per second: 135, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.013419, mae: 3.456439, mean_q: 4.166926, mean_eps: 0.928565\n",
      "  199285/3750000: episode: 297, duration: 5.006s, episode steps: 663, steps per second: 132, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.013758, mae: 3.436329, mean_q: 4.143543, mean_eps: 0.928374\n",
      "  200399/3750000: episode: 298, duration: 8.180s, episode steps: 1114, steps per second: 136, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.015753, mae: 3.434355, mean_q: 4.142154, mean_eps: 0.928058\n",
      "  201034/3750000: episode: 299, duration: 4.833s, episode steps: 635, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.016224, mae: 3.530028, mean_q: 4.249936, mean_eps: 0.927744\n",
      "  201513/3750000: episode: 300, duration: 3.569s, episode steps: 479, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.017978, mae: 3.502982, mean_q: 4.216669, mean_eps: 0.927543\n",
      "  201943/3750000: episode: 301, duration: 3.255s, episode steps: 430, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.015872, mae: 3.460982, mean_q: 4.174318, mean_eps: 0.927377\n",
      "  202318/3750000: episode: 302, duration: 2.780s, episode steps: 375, steps per second: 135, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.016334, mae: 3.565800, mean_q: 4.297179, mean_eps: 0.927233\n",
      "  202737/3750000: episode: 303, duration: 3.155s, episode steps: 419, steps per second: 133, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.014835, mae: 3.537026, mean_q: 4.262885, mean_eps: 0.927093\n",
      "  203600/3750000: episode: 304, duration: 6.559s, episode steps: 863, steps per second: 132, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.017747, mae: 3.470717, mean_q: 4.181045, mean_eps: 0.926862\n",
      "  204010/3750000: episode: 305, duration: 3.129s, episode steps: 410, steps per second: 131, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.020674, mae: 3.407502, mean_q: 4.108039, mean_eps: 0.926632\n",
      "  204666/3750000: episode: 306, duration: 4.905s, episode steps: 656, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.020235, mae: 3.576763, mean_q: 4.313615, mean_eps: 0.926438\n",
      "  205005/3750000: episode: 307, duration: 2.536s, episode steps: 339, steps per second: 134, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.014574, mae: 3.489076, mean_q: 4.205378, mean_eps: 0.926258\n",
      "  205546/3750000: episode: 308, duration: 4.094s, episode steps: 541, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.018801, mae: 3.535318, mean_q: 4.263143, mean_eps: 0.926099\n",
      "  206041/3750000: episode: 309, duration: 3.747s, episode steps: 495, steps per second: 132, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.025569, mae: 3.456633, mean_q: 4.163676, mean_eps: 0.925912\n",
      "  206657/3750000: episode: 310, duration: 4.529s, episode steps: 616, steps per second: 136, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.017391, mae: 3.524731, mean_q: 4.255046, mean_eps: 0.925714\n",
      "  207301/3750000: episode: 311, duration: 4.918s, episode steps: 644, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.018575, mae: 3.581006, mean_q: 4.321991, mean_eps: 0.925487\n",
      "  207696/3750000: episode: 312, duration: 2.923s, episode steps: 395, steps per second: 135, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.013000, mae: 3.524313, mean_q: 4.247346, mean_eps: 0.925300\n",
      "  208578/3750000: episode: 313, duration: 6.563s, episode steps: 882, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.024031, mae: 3.533012, mean_q: 4.249569, mean_eps: 0.925073\n",
      "  209359/3750000: episode: 314, duration: 5.823s, episode steps: 781, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015638, mae: 3.600952, mean_q: 4.336098, mean_eps: 0.924774\n",
      "  209776/3750000: episode: 315, duration: 3.191s, episode steps: 417, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.015912, mae: 3.531008, mean_q: 4.249997, mean_eps: 0.924558\n",
      "  210458/3750000: episode: 316, duration: 5.106s, episode steps: 682, steps per second: 134, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.013970, mae: 3.550197, mean_q: 4.272504, mean_eps: 0.924360\n",
      "  211080/3750000: episode: 317, duration: 4.735s, episode steps: 622, steps per second: 131, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.028305, mae: 3.600907, mean_q: 4.334789, mean_eps: 0.924126\n",
      "  211676/3750000: episode: 318, duration: 4.527s, episode steps: 596, steps per second: 132, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.018280, mae: 3.533442, mean_q: 4.253655, mean_eps: 0.923907\n",
      "  212382/3750000: episode: 319, duration: 5.411s, episode steps: 706, steps per second: 130, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014363, mae: 3.524635, mean_q: 4.245220, mean_eps: 0.923669\n",
      "  212906/3750000: episode: 320, duration: 3.984s, episode steps: 524, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.029147, mae: 3.496987, mean_q: 4.207926, mean_eps: 0.923446\n",
      "  213390/3750000: episode: 321, duration: 3.627s, episode steps: 484, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.020935, mae: 3.588452, mean_q: 4.320081, mean_eps: 0.923266\n",
      "  213951/3750000: episode: 322, duration: 4.239s, episode steps: 561, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.025982, mae: 3.569779, mean_q: 4.302763, mean_eps: 0.923079\n",
      "  214354/3750000: episode: 323, duration: 3.021s, episode steps: 403, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.021175, mae: 3.632787, mean_q: 4.381829, mean_eps: 0.922906\n",
      "  214910/3750000: episode: 324, duration: 4.174s, episode steps: 556, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.011373, mae: 3.556625, mean_q: 4.291594, mean_eps: 0.922733\n",
      "  215562/3750000: episode: 325, duration: 4.932s, episode steps: 652, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.021563, mae: 3.542129, mean_q: 4.274992, mean_eps: 0.922514\n",
      "  216059/3750000: episode: 326, duration: 3.723s, episode steps: 497, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.018300, mae: 3.474624, mean_q: 4.186623, mean_eps: 0.922308\n",
      "  216560/3750000: episode: 327, duration: 3.841s, episode steps: 501, steps per second: 130, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.017059, mae: 3.537394, mean_q: 4.267820, mean_eps: 0.922132\n",
      "  217211/3750000: episode: 328, duration: 4.909s, episode steps: 651, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.025565, mae: 3.601862, mean_q: 4.336574, mean_eps: 0.921923\n",
      "  217824/3750000: episode: 329, duration: 4.583s, episode steps: 613, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.023523, mae: 3.488930, mean_q: 4.216222, mean_eps: 0.921693\n",
      "  218436/3750000: episode: 330, duration: 4.568s, episode steps: 612, steps per second: 134, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.021439, mae: 3.531546, mean_q: 4.264648, mean_eps: 0.921473\n",
      "  219527/3750000: episode: 331, duration: 8.207s, episode steps: 1091, steps per second: 133, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.023582, mae: 3.544368, mean_q: 4.268644, mean_eps: 0.921167\n",
      "  220134/3750000: episode: 332, duration: 4.526s, episode steps: 607, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.015610, mae: 3.572314, mean_q: 4.303789, mean_eps: 0.920861\n",
      "  220579/3750000: episode: 333, duration: 3.353s, episode steps: 445, steps per second: 133, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.018982, mae: 3.598434, mean_q: 4.335668, mean_eps: 0.920674\n",
      "  221239/3750000: episode: 334, duration: 4.958s, episode steps: 660, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.021550, mae: 3.573849, mean_q: 4.303214, mean_eps: 0.920476\n",
      "  221845/3750000: episode: 335, duration: 4.601s, episode steps: 606, steps per second: 132, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.016055, mae: 3.497649, mean_q: 4.209199, mean_eps: 0.920246\n",
      "  222720/3750000: episode: 336, duration: 6.613s, episode steps: 875, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.017164, mae: 3.592596, mean_q: 4.335240, mean_eps: 0.919979\n",
      "  223297/3750000: episode: 337, duration: 4.332s, episode steps: 577, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.027091, mae: 3.547261, mean_q: 4.283496, mean_eps: 0.919720\n",
      "  223889/3750000: episode: 338, duration: 4.505s, episode steps: 592, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.015259, mae: 3.547174, mean_q: 4.279820, mean_eps: 0.919508\n",
      "  224372/3750000: episode: 339, duration: 3.629s, episode steps: 483, steps per second: 133, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.025364, mae: 3.529003, mean_q: 4.250601, mean_eps: 0.919313\n",
      "  225259/3750000: episode: 340, duration: 6.582s, episode steps: 887, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.015323, mae: 3.526794, mean_q: 4.246887, mean_eps: 0.919068\n",
      "  225896/3750000: episode: 341, duration: 4.799s, episode steps: 637, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.013433, mae: 3.509525, mean_q: 4.224569, mean_eps: 0.918795\n",
      "  226556/3750000: episode: 342, duration: 4.978s, episode steps: 660, steps per second: 133, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.016703, mae: 3.635977, mean_q: 4.384310, mean_eps: 0.918561\n",
      "  227125/3750000: episode: 343, duration: 4.311s, episode steps: 569, steps per second: 132, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.019825, mae: 3.543512, mean_q: 4.280448, mean_eps: 0.918338\n",
      "  227795/3750000: episode: 344, duration: 5.011s, episode steps: 670, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.025747, mae: 3.556069, mean_q: 4.294038, mean_eps: 0.918114\n",
      "  228405/3750000: episode: 345, duration: 4.695s, episode steps: 610, steps per second: 130, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.016052, mae: 3.504534, mean_q: 4.220661, mean_eps: 0.917884\n",
      "  229227/3750000: episode: 346, duration: 6.194s, episode steps: 822, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.017255, mae: 3.528354, mean_q: 4.248232, mean_eps: 0.917625\n",
      "  229861/3750000: episode: 347, duration: 4.813s, episode steps: 634, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.019424, mae: 3.500953, mean_q: 4.216894, mean_eps: 0.917362\n",
      "  230465/3750000: episode: 348, duration: 4.499s, episode steps: 604, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.011782, mae: 3.545503, mean_q: 4.274787, mean_eps: 0.917139\n",
      "  230867/3750000: episode: 349, duration: 3.074s, episode steps: 402, steps per second: 131, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.019200, mae: 3.554629, mean_q: 4.288777, mean_eps: 0.916959\n",
      "  231265/3750000: episode: 350, duration: 3.063s, episode steps: 398, steps per second: 130, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.014367, mae: 3.541334, mean_q: 4.268230, mean_eps: 0.916815\n",
      "  231853/3750000: episode: 351, duration: 4.404s, episode steps: 588, steps per second: 134, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.028591, mae: 3.501425, mean_q: 4.220526, mean_eps: 0.916638\n",
      "  232479/3750000: episode: 352, duration: 4.741s, episode steps: 626, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.020253, mae: 3.506090, mean_q: 4.224609, mean_eps: 0.916422\n",
      "  233015/3750000: episode: 353, duration: 4.125s, episode steps: 536, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.018891, mae: 3.524561, mean_q: 4.253644, mean_eps: 0.916214\n",
      "  233904/3750000: episode: 354, duration: 6.856s, episode steps: 889, steps per second: 130, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.017343, mae: 3.566553, mean_q: 4.297749, mean_eps: 0.915954\n",
      "  234477/3750000: episode: 355, duration: 4.262s, episode steps: 573, steps per second: 134, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.023958, mae: 3.513932, mean_q: 4.231087, mean_eps: 0.915692\n",
      "  234992/3750000: episode: 356, duration: 3.942s, episode steps: 515, steps per second: 131, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.016531, mae: 3.516746, mean_q: 4.235729, mean_eps: 0.915497\n",
      "  235531/3750000: episode: 357, duration: 4.076s, episode steps: 539, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.016127, mae: 3.526677, mean_q: 4.250345, mean_eps: 0.915306\n",
      "  236240/3750000: episode: 358, duration: 5.365s, episode steps: 709, steps per second: 132, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.018988, mae: 3.551627, mean_q: 4.280086, mean_eps: 0.915083\n",
      "  237061/3750000: episode: 359, duration: 6.312s, episode steps: 821, steps per second: 130, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.020856, mae: 3.496531, mean_q: 4.221486, mean_eps: 0.914806\n",
      "  237727/3750000: episode: 360, duration: 5.013s, episode steps: 666, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.018111, mae: 3.496638, mean_q: 4.216435, mean_eps: 0.914536\n",
      "  238620/3750000: episode: 361, duration: 6.672s, episode steps: 893, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.017304, mae: 3.527618, mean_q: 4.251196, mean_eps: 0.914259\n",
      "  240236/3750000: episode: 362, duration: 12.109s, episode steps: 1616, steps per second: 133, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.013852, mae: 3.480681, mean_q: 4.194804, mean_eps: 0.913809\n",
      "  240730/3750000: episode: 363, duration: 3.755s, episode steps: 494, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.012264, mae: 3.493860, mean_q: 4.209255, mean_eps: 0.913427\n",
      "  241383/3750000: episode: 364, duration: 4.965s, episode steps: 653, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.017590, mae: 3.558976, mean_q: 4.288558, mean_eps: 0.913218\n",
      "  241895/3750000: episode: 365, duration: 3.780s, episode steps: 512, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.019096, mae: 3.480037, mean_q: 4.189844, mean_eps: 0.913010\n",
      "  242840/3750000: episode: 366, duration: 7.188s, episode steps: 945, steps per second: 131, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.019245, mae: 3.523208, mean_q: 4.253084, mean_eps: 0.912750\n",
      "  243365/3750000: episode: 367, duration: 4.013s, episode steps: 525, steps per second: 131, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.010230, mae: 3.479632, mean_q: 4.193955, mean_eps: 0.912484\n",
      "  244237/3750000: episode: 368, duration: 6.495s, episode steps: 872, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.019120, mae: 3.481201, mean_q: 4.200287, mean_eps: 0.912232\n",
      "  244910/3750000: episode: 369, duration: 5.067s, episode steps: 673, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016230, mae: 3.471609, mean_q: 4.185222, mean_eps: 0.911955\n",
      "  245739/3750000: episode: 370, duration: 6.242s, episode steps: 829, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.014731, mae: 3.508645, mean_q: 4.228760, mean_eps: 0.911685\n",
      "  246471/3750000: episode: 371, duration: 5.499s, episode steps: 732, steps per second: 133, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.018681, mae: 3.505101, mean_q: 4.222880, mean_eps: 0.911404\n",
      "  247170/3750000: episode: 372, duration: 5.231s, episode steps: 699, steps per second: 134, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.021749, mae: 3.495791, mean_q: 4.210779, mean_eps: 0.911145\n",
      "  247623/3750000: episode: 373, duration: 3.471s, episode steps: 453, steps per second: 131, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.015802, mae: 3.463499, mean_q: 4.170588, mean_eps: 0.910936\n",
      "  248518/3750000: episode: 374, duration: 6.702s, episode steps: 895, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.017434, mae: 3.455951, mean_q: 4.160773, mean_eps: 0.910695\n",
      "  249196/3750000: episode: 375, duration: 5.096s, episode steps: 678, steps per second: 133, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.021165, mae: 3.456180, mean_q: 4.167121, mean_eps: 0.910414\n",
      "  249657/3750000: episode: 376, duration: 3.458s, episode steps: 461, steps per second: 133, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.011095, mae: 3.523058, mean_q: 4.243704, mean_eps: 0.910209\n",
      "  250278/3750000: episode: 377, duration: 4.817s, episode steps: 621, steps per second: 129, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.021936, mae: 3.470552, mean_q: 4.180416, mean_eps: 0.910014\n",
      "  250913/3750000: episode: 378, duration: 4.833s, episode steps: 635, steps per second: 131, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.026434, mae: 3.443449, mean_q: 4.152923, mean_eps: 0.909788\n",
      "  251598/3750000: episode: 379, duration: 5.110s, episode steps: 685, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.019352, mae: 3.494446, mean_q: 4.213571, mean_eps: 0.909550\n",
      "  252274/3750000: episode: 380, duration: 5.168s, episode steps: 676, steps per second: 131, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.017259, mae: 3.480459, mean_q: 4.195296, mean_eps: 0.909305\n",
      "  252799/3750000: episode: 381, duration: 3.908s, episode steps: 525, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.023901, mae: 3.473880, mean_q: 4.189619, mean_eps: 0.909089\n",
      "  253871/3750000: episode: 382, duration: 8.031s, episode steps: 1072, steps per second: 133, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.017546, mae: 3.423162, mean_q: 4.129789, mean_eps: 0.908801\n",
      "  254366/3750000: episode: 383, duration: 3.819s, episode steps: 495, steps per second: 130, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.018462, mae: 3.352045, mean_q: 4.047949, mean_eps: 0.908517\n",
      "  254896/3750000: episode: 384, duration: 3.912s, episode steps: 530, steps per second: 135, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.023686, mae: 3.336046, mean_q: 4.018677, mean_eps: 0.908333\n",
      "  255439/3750000: episode: 385, duration: 4.046s, episode steps: 543, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.019059, mae: 3.425386, mean_q: 4.132643, mean_eps: 0.908142\n",
      "  256344/3750000: episode: 386, duration: 6.901s, episode steps: 905, steps per second: 131, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.013428, mae: 3.395620, mean_q: 4.099404, mean_eps: 0.907880\n",
      "  257328/3750000: episode: 387, duration: 7.380s, episode steps: 984, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.018039, mae: 3.431664, mean_q: 4.140261, mean_eps: 0.907538\n",
      "  257725/3750000: episode: 388, duration: 3.100s, episode steps: 397, steps per second: 128, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.020208, mae: 3.341513, mean_q: 4.029208, mean_eps: 0.907289\n",
      "  258211/3750000: episode: 389, duration: 3.676s, episode steps: 486, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.017005, mae: 3.500252, mean_q: 4.224681, mean_eps: 0.907131\n",
      "  259099/3750000: episode: 390, duration: 6.611s, episode steps: 888, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.015689, mae: 3.422121, mean_q: 4.125617, mean_eps: 0.906886\n",
      "  259484/3750000: episode: 391, duration: 2.955s, episode steps: 385, steps per second: 130, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.016858, mae: 3.471767, mean_q: 4.182571, mean_eps: 0.906656\n",
      "  259870/3750000: episode: 392, duration: 2.909s, episode steps: 386, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.016491, mae: 3.452277, mean_q: 4.167860, mean_eps: 0.906515\n",
      "  260438/3750000: episode: 393, duration: 4.280s, episode steps: 568, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.021978, mae: 3.479343, mean_q: 4.198047, mean_eps: 0.906346\n",
      "  261083/3750000: episode: 394, duration: 4.877s, episode steps: 645, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.022202, mae: 3.455705, mean_q: 4.172795, mean_eps: 0.906126\n",
      "  262022/3750000: episode: 395, duration: 7.029s, episode steps: 939, steps per second: 134, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.026982, mae: 3.477236, mean_q: 4.189755, mean_eps: 0.905838\n",
      "  262511/3750000: episode: 396, duration: 3.655s, episode steps: 489, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.023307, mae: 3.443472, mean_q: 4.148610, mean_eps: 0.905583\n",
      "  263134/3750000: episode: 397, duration: 4.661s, episode steps: 623, steps per second: 134, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.017429, mae: 3.527885, mean_q: 4.255903, mean_eps: 0.905385\n",
      "  263784/3750000: episode: 398, duration: 4.898s, episode steps: 650, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.023251, mae: 3.510176, mean_q: 4.235837, mean_eps: 0.905154\n",
      "  264296/3750000: episode: 399, duration: 3.799s, episode steps: 512, steps per second: 135, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.017734, mae: 3.523674, mean_q: 4.257058, mean_eps: 0.904946\n",
      "  265134/3750000: episode: 400, duration: 6.327s, episode steps: 838, steps per second: 132, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.018146, mae: 3.454283, mean_q: 4.171075, mean_eps: 0.904704\n",
      "  265585/3750000: episode: 401, duration: 3.413s, episode steps: 451, steps per second: 132, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014575, mae: 3.545844, mean_q: 4.278290, mean_eps: 0.904470\n",
      "  265931/3750000: episode: 402, duration: 2.639s, episode steps: 346, steps per second: 131, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.020410, mae: 3.443624, mean_q: 4.152713, mean_eps: 0.904326\n",
      "  266836/3750000: episode: 403, duration: 6.827s, episode steps: 905, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014967, mae: 3.442104, mean_q: 4.149884, mean_eps: 0.904103\n",
      "  267223/3750000: episode: 404, duration: 2.967s, episode steps: 387, steps per second: 130, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.019927, mae: 3.476953, mean_q: 4.192524, mean_eps: 0.903869\n",
      "  267769/3750000: episode: 405, duration: 4.057s, episode steps: 546, steps per second: 135, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.013237, mae: 3.479595, mean_q: 4.200964, mean_eps: 0.903700\n",
      "  268167/3750000: episode: 406, duration: 2.976s, episode steps: 398, steps per second: 134, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014778, mae: 3.441223, mean_q: 4.153086, mean_eps: 0.903531\n",
      "  269247/3750000: episode: 407, duration: 8.095s, episode steps: 1080, steps per second: 133, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.019551, mae: 3.433367, mean_q: 4.142434, mean_eps: 0.903264\n",
      "  269624/3750000: episode: 408, duration: 2.846s, episode steps: 377, steps per second: 132, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.019935, mae: 3.457391, mean_q: 4.165592, mean_eps: 0.903002\n",
      "  270161/3750000: episode: 409, duration: 4.036s, episode steps: 537, steps per second: 133, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015641, mae: 3.443799, mean_q: 4.157724, mean_eps: 0.902836\n",
      "  270608/3750000: episode: 410, duration: 3.330s, episode steps: 447, steps per second: 134, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.018312, mae: 3.491302, mean_q: 4.213742, mean_eps: 0.902660\n",
      "  271451/3750000: episode: 411, duration: 6.318s, episode steps: 843, steps per second: 133, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016772, mae: 3.489842, mean_q: 4.209950, mean_eps: 0.902429\n",
      "  271891/3750000: episode: 412, duration: 3.293s, episode steps: 440, steps per second: 134, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.021655, mae: 3.451210, mean_q: 4.158056, mean_eps: 0.902199\n",
      "  273235/3750000: episode: 413, duration: 10.039s, episode steps: 1344, steps per second: 134, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.018475, mae: 3.501632, mean_q: 4.224115, mean_eps: 0.901878\n",
      "  274143/3750000: episode: 414, duration: 6.851s, episode steps: 908, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.021409, mae: 3.559370, mean_q: 4.290933, mean_eps: 0.901472\n",
      "  274757/3750000: episode: 415, duration: 4.557s, episode steps: 614, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.016729, mae: 3.473957, mean_q: 4.187138, mean_eps: 0.901198\n",
      "  275374/3750000: episode: 416, duration: 4.674s, episode steps: 617, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.019763, mae: 3.524405, mean_q: 4.249802, mean_eps: 0.900978\n",
      "  275991/3750000: episode: 417, duration: 4.772s, episode steps: 617, steps per second: 129, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.017591, mae: 3.477579, mean_q: 4.190962, mean_eps: 0.900755\n",
      "  276670/3750000: episode: 418, duration: 5.100s, episode steps: 679, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.020968, mae: 3.450076, mean_q: 4.160093, mean_eps: 0.900521\n",
      "  277436/3750000: episode: 419, duration: 5.735s, episode steps: 766, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.020033, mae: 3.458115, mean_q: 4.163220, mean_eps: 0.900262\n",
      "  277857/3750000: episode: 420, duration: 3.203s, episode steps: 421, steps per second: 131, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.022266, mae: 3.521659, mean_q: 4.246734, mean_eps: 0.900050\n",
      "  278628/3750000: episode: 421, duration: 5.825s, episode steps: 771, steps per second: 132, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.014472, mae: 3.543986, mean_q: 4.268478, mean_eps: 0.899834\n",
      "  279329/3750000: episode: 422, duration: 5.264s, episode steps: 701, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.016618, mae: 3.496915, mean_q: 4.214895, mean_eps: 0.899567\n",
      "  279868/3750000: episode: 423, duration: 4.169s, episode steps: 539, steps per second: 129, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.021190, mae: 3.534500, mean_q: 4.272839, mean_eps: 0.899344\n",
      "  280362/3750000: episode: 424, duration: 3.729s, episode steps: 494, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017415, mae: 3.463964, mean_q: 4.176045, mean_eps: 0.899157\n",
      "  280874/3750000: episode: 425, duration: 3.769s, episode steps: 512, steps per second: 136, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.017851, mae: 3.569838, mean_q: 4.305448, mean_eps: 0.898977\n",
      "  281287/3750000: episode: 426, duration: 3.120s, episode steps: 413, steps per second: 132, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.016699, mae: 3.471340, mean_q: 4.184048, mean_eps: 0.898811\n",
      "  282470/3750000: episode: 427, duration: 8.881s, episode steps: 1183, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.016588, mae: 3.508867, mean_q: 4.235839, mean_eps: 0.898523\n",
      "  283473/3750000: episode: 428, duration: 7.455s, episode steps: 1003, steps per second: 135, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.018201, mae: 3.510139, mean_q: 4.233234, mean_eps: 0.898131\n",
      "  284269/3750000: episode: 429, duration: 6.024s, episode steps: 796, steps per second: 132, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.019948, mae: 3.566943, mean_q: 4.312906, mean_eps: 0.897807\n",
      "  285599/3750000: episode: 430, duration: 9.905s, episode steps: 1330, steps per second: 134, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.018844, mae: 3.565344, mean_q: 4.299350, mean_eps: 0.897425\n",
      "  286548/3750000: episode: 431, duration: 7.220s, episode steps: 949, steps per second: 131, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.018273, mae: 3.579150, mean_q: 4.316682, mean_eps: 0.897015\n",
      "  287523/3750000: episode: 432, duration: 7.314s, episode steps: 975, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.021556, mae: 3.580541, mean_q: 4.315074, mean_eps: 0.896666\n",
      "  288295/3750000: episode: 433, duration: 5.774s, episode steps: 772, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.021158, mae: 3.660241, mean_q: 4.410912, mean_eps: 0.896352\n",
      "  288674/3750000: episode: 434, duration: 2.852s, episode steps: 379, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.029924, mae: 3.590790, mean_q: 4.323643, mean_eps: 0.896147\n",
      "  289224/3750000: episode: 435, duration: 4.149s, episode steps: 550, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.021738, mae: 3.648475, mean_q: 4.394249, mean_eps: 0.895978\n",
      "  289634/3750000: episode: 436, duration: 3.030s, episode steps: 410, steps per second: 135, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.015960, mae: 3.648455, mean_q: 4.396995, mean_eps: 0.895805\n",
      "  290265/3750000: episode: 437, duration: 4.822s, episode steps: 631, steps per second: 131, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.023247, mae: 3.650617, mean_q: 4.399702, mean_eps: 0.895618\n",
      "  290784/3750000: episode: 438, duration: 3.914s, episode steps: 519, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.016971, mae: 3.620513, mean_q: 4.379498, mean_eps: 0.895409\n",
      "  291383/3750000: episode: 439, duration: 4.469s, episode steps: 599, steps per second: 134, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.019687, mae: 3.684672, mean_q: 4.448998, mean_eps: 0.895208\n",
      "  292035/3750000: episode: 440, duration: 4.908s, episode steps: 652, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.022556, mae: 3.678561, mean_q: 4.431916, mean_eps: 0.894984\n",
      "  292852/3750000: episode: 441, duration: 6.194s, episode steps: 817, steps per second: 132, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.015265, mae: 3.650471, mean_q: 4.410140, mean_eps: 0.894722\n",
      "  293323/3750000: episode: 442, duration: 3.569s, episode steps: 471, steps per second: 132, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.020188, mae: 3.587033, mean_q: 4.325374, mean_eps: 0.894488\n",
      "  293697/3750000: episode: 443, duration: 2.758s, episode steps: 374, steps per second: 136, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.018503, mae: 3.652731, mean_q: 4.398429, mean_eps: 0.894336\n",
      "  294510/3750000: episode: 444, duration: 6.134s, episode steps: 813, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.015601, mae: 3.635420, mean_q: 4.388459, mean_eps: 0.894124\n",
      "  295143/3750000: episode: 445, duration: 4.778s, episode steps: 633, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014157, mae: 3.630380, mean_q: 4.374927, mean_eps: 0.893861\n",
      "  295638/3750000: episode: 446, duration: 3.633s, episode steps: 495, steps per second: 136, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.024167, mae: 3.612897, mean_q: 4.364671, mean_eps: 0.893660\n",
      "  296370/3750000: episode: 447, duration: 5.498s, episode steps: 732, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.020452, mae: 3.614214, mean_q: 4.354169, mean_eps: 0.893440\n",
      "  296940/3750000: episode: 448, duration: 4.371s, episode steps: 570, steps per second: 130, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.018188, mae: 3.659059, mean_q: 4.406406, mean_eps: 0.893206\n",
      "  297455/3750000: episode: 449, duration: 3.906s, episode steps: 515, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.012820, mae: 3.655211, mean_q: 4.405161, mean_eps: 0.893012\n",
      "  297985/3750000: episode: 450, duration: 4.006s, episode steps: 530, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014972, mae: 3.630828, mean_q: 4.384780, mean_eps: 0.892821\n",
      "  299001/3750000: episode: 451, duration: 7.681s, episode steps: 1016, steps per second: 132, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.018354, mae: 3.613740, mean_q: 4.351532, mean_eps: 0.892540\n",
      "  299756/3750000: episode: 452, duration: 5.586s, episode steps: 755, steps per second: 135, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.024662, mae: 3.635452, mean_q: 4.372924, mean_eps: 0.892223\n",
      "  300601/3750000: episode: 453, duration: 6.391s, episode steps: 845, steps per second: 132, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.026878, mae: 3.583827, mean_q: 4.322166, mean_eps: 0.891935\n",
      "  301090/3750000: episode: 454, duration: 3.744s, episode steps: 489, steps per second: 131, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.022704, mae: 3.648200, mean_q: 4.391736, mean_eps: 0.891694\n",
      "  301687/3750000: episode: 455, duration: 4.471s, episode steps: 597, steps per second: 134, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013042, mae: 3.603903, mean_q: 4.341931, mean_eps: 0.891500\n",
      "  302212/3750000: episode: 456, duration: 3.898s, episode steps: 525, steps per second: 135, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.018491, mae: 3.628007, mean_q: 4.368923, mean_eps: 0.891298\n",
      "  302714/3750000: episode: 457, duration: 3.759s, episode steps: 502, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.020562, mae: 3.647676, mean_q: 4.389659, mean_eps: 0.891114\n",
      "  303237/3750000: episode: 458, duration: 3.958s, episode steps: 523, steps per second: 132, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015270, mae: 3.559129, mean_q: 4.283823, mean_eps: 0.890931\n",
      "  303752/3750000: episode: 459, duration: 3.912s, episode steps: 515, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.019115, mae: 3.561622, mean_q: 4.287746, mean_eps: 0.890744\n",
      "  304643/3750000: episode: 460, duration: 6.692s, episode steps: 891, steps per second: 133, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.019362, mae: 3.633723, mean_q: 4.384357, mean_eps: 0.890488\n",
      "  305315/3750000: episode: 461, duration: 4.994s, episode steps: 672, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.023168, mae: 3.652215, mean_q: 4.397924, mean_eps: 0.890207\n",
      "  306255/3750000: episode: 462, duration: 7.018s, episode steps: 940, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.022954, mae: 3.676872, mean_q: 4.432957, mean_eps: 0.889919\n",
      "  306922/3750000: episode: 463, duration: 5.038s, episode steps: 667, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.018528, mae: 3.684495, mean_q: 4.443929, mean_eps: 0.889628\n",
      "  307597/3750000: episode: 464, duration: 5.043s, episode steps: 675, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.017319, mae: 3.659238, mean_q: 4.406107, mean_eps: 0.889386\n",
      "  308501/3750000: episode: 465, duration: 6.895s, episode steps: 904, steps per second: 131, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.015145, mae: 3.649269, mean_q: 4.393388, mean_eps: 0.889102\n",
      "  309355/3750000: episode: 466, duration: 6.350s, episode steps: 854, steps per second: 134, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.022526, mae: 3.623149, mean_q: 4.363601, mean_eps: 0.888785\n",
      "  309994/3750000: episode: 467, duration: 4.822s, episode steps: 639, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.028864, mae: 3.637379, mean_q: 4.379118, mean_eps: 0.888519\n",
      "  310528/3750000: episode: 468, duration: 4.019s, episode steps: 534, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.031338, mae: 3.619975, mean_q: 4.358258, mean_eps: 0.888306\n",
      "  311265/3750000: episode: 469, duration: 5.512s, episode steps: 737, steps per second: 134, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.024874, mae: 3.634081, mean_q: 4.380990, mean_eps: 0.888076\n",
      "  311819/3750000: episode: 470, duration: 4.162s, episode steps: 554, steps per second: 133, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.015891, mae: 3.614758, mean_q: 4.359430, mean_eps: 0.887846\n",
      "  312789/3750000: episode: 471, duration: 7.283s, episode steps: 970, steps per second: 133, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.027318, mae: 3.610007, mean_q: 4.342461, mean_eps: 0.887572\n",
      "  313547/3750000: episode: 472, duration: 5.677s, episode steps: 758, steps per second: 134, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.027349, mae: 3.634535, mean_q: 4.379665, mean_eps: 0.887259\n",
      "  314047/3750000: episode: 473, duration: 3.801s, episode steps: 500, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.014830, mae: 3.575665, mean_q: 4.312555, mean_eps: 0.887032\n",
      "  314768/3750000: episode: 474, duration: 5.368s, episode steps: 721, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.018745, mae: 3.588228, mean_q: 4.321602, mean_eps: 0.886812\n",
      "  315441/3750000: episode: 475, duration: 5.043s, episode steps: 673, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.019957, mae: 3.573108, mean_q: 4.305250, mean_eps: 0.886560\n",
      "  316492/3750000: episode: 476, duration: 7.867s, episode steps: 1051, steps per second: 134, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.022144, mae: 3.616125, mean_q: 4.359859, mean_eps: 0.886251\n",
      "  317165/3750000: episode: 477, duration: 5.033s, episode steps: 673, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.024041, mae: 3.589362, mean_q: 4.331806, mean_eps: 0.885941\n",
      "  317855/3750000: episode: 478, duration: 5.136s, episode steps: 690, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.020453, mae: 3.648316, mean_q: 4.398713, mean_eps: 0.885696\n",
      "  319179/3750000: episode: 479, duration: 9.933s, episode steps: 1324, steps per second: 133, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.026201, mae: 3.607338, mean_q: 4.341433, mean_eps: 0.885336\n",
      "  319961/3750000: episode: 480, duration: 5.914s, episode steps: 782, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.018329, mae: 3.619853, mean_q: 4.357358, mean_eps: 0.884955\n",
      "  320477/3750000: episode: 481, duration: 3.862s, episode steps: 516, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.021603, mae: 3.598984, mean_q: 4.341234, mean_eps: 0.884721\n",
      "  321071/3750000: episode: 482, duration: 4.487s, episode steps: 594, steps per second: 132, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.018020, mae: 3.660613, mean_q: 4.410253, mean_eps: 0.884523\n",
      "  321907/3750000: episode: 483, duration: 6.248s, episode steps: 836, steps per second: 134, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.027257, mae: 3.559240, mean_q: 4.288122, mean_eps: 0.884264\n",
      "  322489/3750000: episode: 484, duration: 4.454s, episode steps: 582, steps per second: 131, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.016921, mae: 3.557140, mean_q: 4.289004, mean_eps: 0.884008\n",
      "  323453/3750000: episode: 485, duration: 7.144s, episode steps: 964, steps per second: 135, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.021683, mae: 3.650810, mean_q: 4.408286, mean_eps: 0.883731\n",
      "  324208/3750000: episode: 486, duration: 5.725s, episode steps: 755, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.019727, mae: 3.674111, mean_q: 4.432761, mean_eps: 0.883421\n",
      "  325414/3750000: episode: 487, duration: 9.034s, episode steps: 1206, steps per second: 133, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.029727, mae: 3.644987, mean_q: 4.386194, mean_eps: 0.883068\n",
      "  326135/3750000: episode: 488, duration: 5.394s, episode steps: 721, steps per second: 134, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.021225, mae: 3.637079, mean_q: 4.384373, mean_eps: 0.882723\n",
      "  326802/3750000: episode: 489, duration: 5.092s, episode steps: 667, steps per second: 131, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.016220, mae: 3.646863, mean_q: 4.393235, mean_eps: 0.882471\n",
      "  327440/3750000: episode: 490, duration: 4.791s, episode steps: 638, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.021309, mae: 3.642424, mean_q: 4.391002, mean_eps: 0.882237\n",
      "  328333/3750000: episode: 491, duration: 6.704s, episode steps: 893, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.019857, mae: 3.646632, mean_q: 4.389176, mean_eps: 0.881963\n",
      "  329269/3750000: episode: 492, duration: 7.048s, episode steps: 936, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.024567, mae: 3.631124, mean_q: 4.376458, mean_eps: 0.881632\n",
      "  330169/3750000: episode: 493, duration: 6.697s, episode steps: 900, steps per second: 134, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.020692, mae: 3.631482, mean_q: 4.374099, mean_eps: 0.881301\n",
      "  330782/3750000: episode: 494, duration: 4.672s, episode steps: 613, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.016829, mae: 3.656765, mean_q: 4.406509, mean_eps: 0.881027\n",
      "  331593/3750000: episode: 495, duration: 6.024s, episode steps: 811, steps per second: 135, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.016770, mae: 3.652962, mean_q: 4.400746, mean_eps: 0.880772\n",
      "  332133/3750000: episode: 496, duration: 4.086s, episode steps: 540, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.017950, mae: 3.643967, mean_q: 4.393475, mean_eps: 0.880530\n",
      "  332887/3750000: episode: 497, duration: 5.737s, episode steps: 754, steps per second: 131, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.017360, mae: 3.659440, mean_q: 4.407961, mean_eps: 0.880296\n",
      "  333834/3750000: episode: 498, duration: 7.048s, episode steps: 947, steps per second: 134, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.022471, mae: 3.684927, mean_q: 4.438906, mean_eps: 0.879990\n",
      "  334282/3750000: episode: 499, duration: 3.410s, episode steps: 448, steps per second: 131, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.033358, mae: 3.761895, mean_q: 4.545258, mean_eps: 0.879738\n",
      "  335395/3750000: episode: 500, duration: 8.291s, episode steps: 1113, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.019906, mae: 3.732640, mean_q: 4.498515, mean_eps: 0.879458\n",
      "  336082/3750000: episode: 501, duration: 5.176s, episode steps: 687, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.021660, mae: 3.707716, mean_q: 4.460204, mean_eps: 0.879134\n",
      "  336530/3750000: episode: 502, duration: 3.346s, episode steps: 448, steps per second: 134, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.015282, mae: 3.737946, mean_q: 4.501303, mean_eps: 0.878928\n",
      "  337333/3750000: episode: 503, duration: 6.034s, episode steps: 803, steps per second: 133, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.021077, mae: 3.725948, mean_q: 4.487805, mean_eps: 0.878705\n",
      "  337911/3750000: episode: 504, duration: 4.308s, episode steps: 578, steps per second: 134, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.022121, mae: 3.771974, mean_q: 4.541938, mean_eps: 0.878457\n",
      "  338270/3750000: episode: 505, duration: 2.694s, episode steps: 359, steps per second: 133, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.023930, mae: 3.826088, mean_q: 4.602567, mean_eps: 0.878288\n",
      "  338808/3750000: episode: 506, duration: 4.034s, episode steps: 538, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.030997, mae: 3.662114, mean_q: 4.405775, mean_eps: 0.878126\n",
      "  339982/3750000: episode: 507, duration: 8.980s, episode steps: 1174, steps per second: 131, episode reward: 11.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.034522, mae: 3.745278, mean_q: 4.512601, mean_eps: 0.877816\n",
      "  340814/3750000: episode: 508, duration: 6.157s, episode steps: 832, steps per second: 135, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.023953, mae: 3.848852, mean_q: 4.639387, mean_eps: 0.877456\n",
      "  341424/3750000: episode: 509, duration: 4.673s, episode steps: 610, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015950, mae: 3.753968, mean_q: 4.519494, mean_eps: 0.877197\n",
      "  341991/3750000: episode: 510, duration: 4.218s, episode steps: 567, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.021732, mae: 3.807582, mean_q: 4.580776, mean_eps: 0.876984\n",
      "  342681/3750000: episode: 511, duration: 5.209s, episode steps: 690, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.023975, mae: 3.818358, mean_q: 4.595049, mean_eps: 0.876758\n",
      "  343405/3750000: episode: 512, duration: 5.413s, episode steps: 724, steps per second: 134, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.026065, mae: 3.755095, mean_q: 4.515738, mean_eps: 0.876502\n",
      "  343884/3750000: episode: 513, duration: 3.643s, episode steps: 479, steps per second: 131, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.031597, mae: 3.735090, mean_q: 4.500397, mean_eps: 0.876286\n",
      "  344261/3750000: episode: 514, duration: 2.844s, episode steps: 377, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.017297, mae: 3.788215, mean_q: 4.559969, mean_eps: 0.876131\n",
      "  345056/3750000: episode: 515, duration: 5.858s, episode steps: 795, steps per second: 136, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.026464, mae: 3.777652, mean_q: 4.553021, mean_eps: 0.875922\n",
      "  345524/3750000: episode: 516, duration: 3.533s, episode steps: 468, steps per second: 132, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.022472, mae: 3.819614, mean_q: 4.597393, mean_eps: 0.875696\n",
      "  346499/3750000: episode: 517, duration: 7.271s, episode steps: 975, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.020536, mae: 3.803078, mean_q: 4.572664, mean_eps: 0.875436\n",
      "  347034/3750000: episode: 518, duration: 4.031s, episode steps: 535, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.031778, mae: 3.817577, mean_q: 4.589414, mean_eps: 0.875166\n",
      "  347533/3750000: episode: 519, duration: 3.737s, episode steps: 499, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.017276, mae: 3.784835, mean_q: 4.553455, mean_eps: 0.874979\n",
      "  348184/3750000: episode: 520, duration: 4.952s, episode steps: 651, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.031771, mae: 3.742985, mean_q: 4.505444, mean_eps: 0.874770\n",
      "  349161/3750000: episode: 521, duration: 7.294s, episode steps: 977, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.024080, mae: 3.719432, mean_q: 4.475134, mean_eps: 0.874475\n",
      "  349918/3750000: episode: 522, duration: 5.704s, episode steps: 757, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.032799, mae: 3.753878, mean_q: 4.519835, mean_eps: 0.874166\n",
      "  350538/3750000: episode: 523, duration: 4.764s, episode steps: 620, steps per second: 130, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013710, mae: 3.677971, mean_q: 4.431763, mean_eps: 0.873921\n",
      "  351351/3750000: episode: 524, duration: 6.166s, episode steps: 813, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.020276, mae: 3.688382, mean_q: 4.441008, mean_eps: 0.873662\n",
      "  352366/3750000: episode: 525, duration: 7.647s, episode steps: 1015, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.025231, mae: 3.724850, mean_q: 4.488852, mean_eps: 0.873330\n",
      "  353117/3750000: episode: 526, duration: 5.614s, episode steps: 751, steps per second: 134, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.851 [0.000, 5.000],  loss: 0.017186, mae: 3.743878, mean_q: 4.525214, mean_eps: 0.873014\n",
      "  354097/3750000: episode: 527, duration: 7.313s, episode steps: 980, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.024315, mae: 3.737938, mean_q: 4.502862, mean_eps: 0.872704\n",
      "  355460/3750000: episode: 528, duration: 10.344s, episode steps: 1363, steps per second: 132, episode reward: 22.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.023840, mae: 3.676343, mean_q: 4.427487, mean_eps: 0.872283\n",
      "  356460/3750000: episode: 529, duration: 7.695s, episode steps: 1000, steps per second: 130, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.016632, mae: 3.652658, mean_q: 4.393315, mean_eps: 0.871858\n",
      "  356886/3750000: episode: 530, duration: 3.226s, episode steps: 426, steps per second: 132, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.017627, mae: 3.679651, mean_q: 4.427264, mean_eps: 0.871599\n",
      "  357778/3750000: episode: 531, duration: 6.613s, episode steps: 892, steps per second: 135, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.024202, mae: 3.617898, mean_q: 4.357812, mean_eps: 0.871361\n",
      "  358279/3750000: episode: 532, duration: 3.764s, episode steps: 501, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.015972, mae: 3.561769, mean_q: 4.298890, mean_eps: 0.871113\n",
      "  359052/3750000: episode: 533, duration: 5.854s, episode steps: 773, steps per second: 132, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.021103, mae: 3.630363, mean_q: 4.375547, mean_eps: 0.870882\n",
      "  359458/3750000: episode: 534, duration: 3.026s, episode steps: 406, steps per second: 134, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.021022, mae: 3.629930, mean_q: 4.372028, mean_eps: 0.870670\n",
      "  360094/3750000: episode: 535, duration: 4.772s, episode steps: 636, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.022945, mae: 3.643444, mean_q: 4.390681, mean_eps: 0.870483\n",
      "  360764/3750000: episode: 536, duration: 5.091s, episode steps: 670, steps per second: 132, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.034959, mae: 3.673831, mean_q: 4.417215, mean_eps: 0.870245\n",
      "  361431/3750000: episode: 537, duration: 4.975s, episode steps: 667, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.027498, mae: 3.651147, mean_q: 4.392025, mean_eps: 0.870004\n",
      "  361812/3750000: episode: 538, duration: 2.843s, episode steps: 381, steps per second: 134, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.033403, mae: 3.630095, mean_q: 4.362712, mean_eps: 0.869817\n",
      "  362303/3750000: episode: 539, duration: 3.698s, episode steps: 491, steps per second: 133, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.028841, mae: 3.668972, mean_q: 4.412256, mean_eps: 0.869658\n",
      "  362816/3750000: episode: 540, duration: 3.836s, episode steps: 513, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.021252, mae: 3.688324, mean_q: 4.444170, mean_eps: 0.869478\n",
      "  363424/3750000: episode: 541, duration: 4.607s, episode steps: 608, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.023389, mae: 3.663472, mean_q: 4.410350, mean_eps: 0.869277\n",
      "  364298/3750000: episode: 542, duration: 6.463s, episode steps: 874, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.019085, mae: 3.736498, mean_q: 4.504568, mean_eps: 0.869010\n",
      "  364932/3750000: episode: 543, duration: 4.839s, episode steps: 634, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.017339, mae: 3.704810, mean_q: 4.454487, mean_eps: 0.868740\n",
      "  365470/3750000: episode: 544, duration: 4.073s, episode steps: 538, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.017675, mae: 3.684351, mean_q: 4.432230, mean_eps: 0.868528\n",
      "  366143/3750000: episode: 545, duration: 5.051s, episode steps: 673, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.015203, mae: 3.658520, mean_q: 4.409335, mean_eps: 0.868308\n",
      "  367126/3750000: episode: 546, duration: 7.380s, episode steps: 983, steps per second: 133, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.024817, mae: 3.657539, mean_q: 4.403058, mean_eps: 0.868010\n",
      "  367601/3750000: episode: 547, duration: 3.588s, episode steps: 475, steps per second: 132, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.020522, mae: 3.699832, mean_q: 4.468005, mean_eps: 0.867747\n",
      "  368722/3750000: episode: 548, duration: 8.377s, episode steps: 1121, steps per second: 134, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.018565, mae: 3.635024, mean_q: 4.380054, mean_eps: 0.867459\n",
      "  369322/3750000: episode: 549, duration: 4.562s, episode steps: 600, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.018375, mae: 3.642360, mean_q: 4.381061, mean_eps: 0.867149\n",
      "  370312/3750000: episode: 550, duration: 7.343s, episode steps: 990, steps per second: 135, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.018826, mae: 3.612170, mean_q: 4.342399, mean_eps: 0.866865\n",
      "  371077/3750000: episode: 551, duration: 5.711s, episode steps: 765, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.017646, mae: 3.573433, mean_q: 4.297399, mean_eps: 0.866552\n",
      "  371886/3750000: episode: 552, duration: 6.173s, episode steps: 809, steps per second: 131, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.023886, mae: 3.586669, mean_q: 4.312300, mean_eps: 0.866267\n",
      "  372279/3750000: episode: 553, duration: 2.976s, episode steps: 393, steps per second: 132, episode reward: 10.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.012253, mae: 3.622947, mean_q: 4.358479, mean_eps: 0.866051\n",
      "  372955/3750000: episode: 554, duration: 5.050s, episode steps: 676, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.019261, mae: 3.607123, mean_q: 4.339526, mean_eps: 0.865860\n",
      "  374260/3750000: episode: 555, duration: 9.896s, episode steps: 1305, steps per second: 132, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.018149, mae: 3.629976, mean_q: 4.372884, mean_eps: 0.865504\n",
      "  374900/3750000: episode: 556, duration: 4.892s, episode steps: 640, steps per second: 131, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.018535, mae: 3.582436, mean_q: 4.318560, mean_eps: 0.865155\n",
      "  375349/3750000: episode: 557, duration: 3.458s, episode steps: 449, steps per second: 130, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.018794, mae: 3.577161, mean_q: 4.308731, mean_eps: 0.864957\n",
      "  375982/3750000: episode: 558, duration: 4.798s, episode steps: 633, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.024975, mae: 3.562896, mean_q: 4.281894, mean_eps: 0.864759\n",
      "  376582/3750000: episode: 559, duration: 4.498s, episode steps: 600, steps per second: 133, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.030791, mae: 3.551061, mean_q: 4.277076, mean_eps: 0.864536\n",
      "  377354/3750000: episode: 560, duration: 5.721s, episode steps: 772, steps per second: 135, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.017655, mae: 3.601535, mean_q: 4.338151, mean_eps: 0.864291\n",
      "  377837/3750000: episode: 561, duration: 3.654s, episode steps: 483, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.017307, mae: 3.625076, mean_q: 4.361782, mean_eps: 0.864068\n",
      "  378675/3750000: episode: 562, duration: 6.268s, episode steps: 838, steps per second: 134, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.018702, mae: 3.549541, mean_q: 4.272598, mean_eps: 0.863830\n",
      "  379981/3750000: episode: 563, duration: 9.839s, episode steps: 1306, steps per second: 133, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.021837, mae: 3.570723, mean_q: 4.298256, mean_eps: 0.863441\n",
      "  380551/3750000: episode: 564, duration: 4.232s, episode steps: 570, steps per second: 135, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.025224, mae: 3.531166, mean_q: 4.262257, mean_eps: 0.863103\n",
      "  381124/3750000: episode: 565, duration: 4.319s, episode steps: 573, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.017069, mae: 3.555161, mean_q: 4.284864, mean_eps: 0.862898\n",
      "  381564/3750000: episode: 566, duration: 3.281s, episode steps: 440, steps per second: 134, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.018626, mae: 3.525601, mean_q: 4.257328, mean_eps: 0.862714\n",
      "  382204/3750000: episode: 567, duration: 4.796s, episode steps: 640, steps per second: 133, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.022819, mae: 3.536960, mean_q: 4.261483, mean_eps: 0.862520\n",
      "  382846/3750000: episode: 568, duration: 4.888s, episode steps: 642, steps per second: 131, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.019152, mae: 3.562839, mean_q: 4.288091, mean_eps: 0.862289\n",
      "  383966/3750000: episode: 569, duration: 8.444s, episode steps: 1120, steps per second: 133, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.023558, mae: 3.565453, mean_q: 4.289085, mean_eps: 0.861972\n",
      "  384891/3750000: episode: 570, duration: 6.900s, episode steps: 925, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.020268, mae: 3.566810, mean_q: 4.290178, mean_eps: 0.861605\n",
      "  385827/3750000: episode: 571, duration: 6.982s, episode steps: 936, steps per second: 134, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.018569, mae: 3.523034, mean_q: 4.235701, mean_eps: 0.861270\n",
      "  386563/3750000: episode: 572, duration: 5.596s, episode steps: 736, steps per second: 132, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.018322, mae: 3.572761, mean_q: 4.301511, mean_eps: 0.860968\n",
      "  387234/3750000: episode: 573, duration: 4.981s, episode steps: 671, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.021491, mae: 3.488899, mean_q: 4.202172, mean_eps: 0.860716\n",
      "  387908/3750000: episode: 574, duration: 5.214s, episode steps: 674, steps per second: 129, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.025712, mae: 3.516304, mean_q: 4.227219, mean_eps: 0.860475\n",
      "  388446/3750000: episode: 575, duration: 4.095s, episode steps: 538, steps per second: 131, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.019012, mae: 3.585380, mean_q: 4.312818, mean_eps: 0.860255\n",
      "  389426/3750000: episode: 576, duration: 7.320s, episode steps: 980, steps per second: 134, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.024583, mae: 3.560661, mean_q: 4.287405, mean_eps: 0.859982\n",
      "  390494/3750000: episode: 577, duration: 8.012s, episode steps: 1068, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.016253, mae: 3.533871, mean_q: 4.256332, mean_eps: 0.859614\n",
      "  392055/3750000: episode: 578, duration: 11.647s, episode steps: 1561, steps per second: 134, episode reward: 17.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.017708, mae: 3.546950, mean_q: 4.275384, mean_eps: 0.859143\n",
      "  392417/3750000: episode: 579, duration: 2.719s, episode steps: 362, steps per second: 133, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.018207, mae: 3.495067, mean_q: 4.217340, mean_eps: 0.858797\n",
      "  393036/3750000: episode: 580, duration: 4.665s, episode steps: 619, steps per second: 133, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.015549, mae: 3.545014, mean_q: 4.271772, mean_eps: 0.858621\n",
      "  393435/3750000: episode: 581, duration: 2.977s, episode steps: 399, steps per second: 134, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.025388, mae: 3.550220, mean_q: 4.272619, mean_eps: 0.858437\n",
      "  393934/3750000: episode: 582, duration: 3.724s, episode steps: 499, steps per second: 134, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.019538, mae: 3.514350, mean_q: 4.229164, mean_eps: 0.858275\n",
      "  394811/3750000: episode: 583, duration: 6.616s, episode steps: 877, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.017478, mae: 3.569188, mean_q: 4.297334, mean_eps: 0.858027\n",
      "  395271/3750000: episode: 584, duration: 3.444s, episode steps: 460, steps per second: 134, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.030635, mae: 3.581214, mean_q: 4.308636, mean_eps: 0.857786\n",
      "  395861/3750000: episode: 585, duration: 4.484s, episode steps: 590, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.016230, mae: 3.583083, mean_q: 4.318610, mean_eps: 0.857595\n",
      "  396817/3750000: episode: 586, duration: 7.141s, episode steps: 956, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.019225, mae: 3.554196, mean_q: 4.283369, mean_eps: 0.857318\n",
      "  397403/3750000: episode: 587, duration: 4.468s, episode steps: 586, steps per second: 131, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.020396, mae: 3.649250, mean_q: 4.397607, mean_eps: 0.857040\n",
      "  398277/3750000: episode: 588, duration: 6.475s, episode steps: 874, steps per second: 135, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.020392, mae: 3.610477, mean_q: 4.341556, mean_eps: 0.856778\n",
      "  398923/3750000: episode: 589, duration: 4.957s, episode steps: 646, steps per second: 130, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.019394, mae: 3.514962, mean_q: 4.231212, mean_eps: 0.856504\n",
      "  399614/3750000: episode: 590, duration: 5.128s, episode steps: 691, steps per second: 135, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.016534, mae: 3.542559, mean_q: 4.263447, mean_eps: 0.856263\n",
      "  400439/3750000: episode: 591, duration: 6.160s, episode steps: 825, steps per second: 134, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.024898, mae: 3.586574, mean_q: 4.317272, mean_eps: 0.855993\n",
      "  401317/3750000: episode: 592, duration: 6.637s, episode steps: 878, steps per second: 132, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.017502, mae: 3.588574, mean_q: 4.324548, mean_eps: 0.855687\n",
      "  401899/3750000: episode: 593, duration: 4.334s, episode steps: 582, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.019961, mae: 3.609873, mean_q: 4.343120, mean_eps: 0.855424\n",
      "  402640/3750000: episode: 594, duration: 5.608s, episode steps: 741, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.017030, mae: 3.574054, mean_q: 4.303383, mean_eps: 0.855186\n",
      "  403516/3750000: episode: 595, duration: 6.717s, episode steps: 876, steps per second: 130, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.018295, mae: 3.588791, mean_q: 4.320221, mean_eps: 0.854895\n",
      "  404410/3750000: episode: 596, duration: 6.750s, episode steps: 894, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014867, mae: 3.615562, mean_q: 4.350074, mean_eps: 0.854574\n",
      "  405148/3750000: episode: 597, duration: 5.551s, episode steps: 738, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.022585, mae: 3.621321, mean_q: 4.359478, mean_eps: 0.854279\n",
      "  405537/3750000: episode: 598, duration: 2.928s, episode steps: 389, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.022696, mae: 3.562590, mean_q: 4.284952, mean_eps: 0.854078\n",
      "  406457/3750000: episode: 599, duration: 6.893s, episode steps: 920, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.017867, mae: 3.588637, mean_q: 4.320614, mean_eps: 0.853844\n",
      "  407318/3750000: episode: 600, duration: 6.554s, episode steps: 861, steps per second: 131, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.023097, mae: 3.527812, mean_q: 4.241463, mean_eps: 0.853523\n",
      "  408133/3750000: episode: 601, duration: 6.164s, episode steps: 815, steps per second: 132, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.017864, mae: 3.501900, mean_q: 4.209312, mean_eps: 0.853221\n",
      "  409018/3750000: episode: 602, duration: 6.600s, episode steps: 885, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.020495, mae: 3.503764, mean_q: 4.214977, mean_eps: 0.852915\n",
      "  409728/3750000: episode: 603, duration: 5.420s, episode steps: 710, steps per second: 131, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.016852, mae: 3.515678, mean_q: 4.233472, mean_eps: 0.852627\n",
      "  410486/3750000: episode: 604, duration: 5.709s, episode steps: 758, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015744, mae: 3.505103, mean_q: 4.218873, mean_eps: 0.852360\n",
      "  411180/3750000: episode: 605, duration: 5.236s, episode steps: 694, steps per second: 133, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014395, mae: 3.485705, mean_q: 4.197972, mean_eps: 0.852101\n",
      "  411887/3750000: episode: 606, duration: 5.425s, episode steps: 707, steps per second: 130, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.023339, mae: 3.525882, mean_q: 4.243207, mean_eps: 0.851849\n",
      "  412536/3750000: episode: 607, duration: 4.821s, episode steps: 649, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.019064, mae: 3.497561, mean_q: 4.211628, mean_eps: 0.851604\n",
      "  413143/3750000: episode: 608, duration: 4.634s, episode steps: 607, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.017699, mae: 3.530720, mean_q: 4.256073, mean_eps: 0.851378\n",
      "  413992/3750000: episode: 609, duration: 6.372s, episode steps: 849, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.019035, mae: 3.491507, mean_q: 4.201312, mean_eps: 0.851115\n",
      "  414609/3750000: episode: 610, duration: 4.628s, episode steps: 617, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014734, mae: 3.516788, mean_q: 4.231761, mean_eps: 0.850852\n",
      "  415273/3750000: episode: 611, duration: 4.962s, episode steps: 664, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016664, mae: 3.541841, mean_q: 4.261191, mean_eps: 0.850622\n",
      "  415990/3750000: episode: 612, duration: 5.452s, episode steps: 717, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014804, mae: 3.482839, mean_q: 4.187179, mean_eps: 0.850373\n",
      "  416564/3750000: episode: 613, duration: 4.322s, episode steps: 574, steps per second: 133, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.016537, mae: 3.549292, mean_q: 4.268761, mean_eps: 0.850139\n",
      "  417222/3750000: episode: 614, duration: 5.108s, episode steps: 658, steps per second: 129, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.019541, mae: 3.498874, mean_q: 4.215607, mean_eps: 0.849916\n",
      "  417669/3750000: episode: 615, duration: 3.367s, episode steps: 447, steps per second: 133, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.017821, mae: 3.507089, mean_q: 4.233284, mean_eps: 0.849718\n",
      "  418394/3750000: episode: 616, duration: 5.485s, episode steps: 725, steps per second: 132, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.020323, mae: 3.510176, mean_q: 4.225336, mean_eps: 0.849509\n",
      "  419320/3750000: episode: 617, duration: 7.030s, episode steps: 926, steps per second: 132, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.017033, mae: 3.521444, mean_q: 4.236241, mean_eps: 0.849214\n",
      "  420233/3750000: episode: 618, duration: 6.914s, episode steps: 913, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.019852, mae: 3.537265, mean_q: 4.253637, mean_eps: 0.848883\n",
      "  421295/3750000: episode: 619, duration: 8.034s, episode steps: 1062, steps per second: 132, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.016762, mae: 3.524815, mean_q: 4.240076, mean_eps: 0.848526\n",
      "  421785/3750000: episode: 620, duration: 3.735s, episode steps: 490, steps per second: 131, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.014153, mae: 3.547443, mean_q: 4.272201, mean_eps: 0.848246\n",
      "  422555/3750000: episode: 621, duration: 5.781s, episode steps: 770, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.017410, mae: 3.541519, mean_q: 4.256092, mean_eps: 0.848019\n",
      "  423245/3750000: episode: 622, duration: 5.256s, episode steps: 690, steps per second: 131, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.012918, mae: 3.582194, mean_q: 4.308514, mean_eps: 0.847756\n",
      "  424448/3750000: episode: 623, duration: 9.026s, episode steps: 1203, steps per second: 133, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.023730, mae: 3.589688, mean_q: 4.318633, mean_eps: 0.847414\n",
      "  424839/3750000: episode: 624, duration: 2.882s, episode steps: 391, steps per second: 136, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.011273, mae: 3.519227, mean_q: 4.242666, mean_eps: 0.847130\n",
      "  425946/3750000: episode: 625, duration: 8.311s, episode steps: 1107, steps per second: 133, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016072, mae: 3.567092, mean_q: 4.295870, mean_eps: 0.846860\n",
      "  427040/3750000: episode: 626, duration: 8.214s, episode steps: 1094, steps per second: 133, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.021310, mae: 3.534221, mean_q: 4.254838, mean_eps: 0.846464\n",
      "  428084/3750000: episode: 627, duration: 7.872s, episode steps: 1044, steps per second: 133, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.020692, mae: 3.616626, mean_q: 4.358875, mean_eps: 0.846078\n",
      "  428648/3750000: episode: 628, duration: 4.307s, episode steps: 564, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.014424, mae: 3.585639, mean_q: 4.320889, mean_eps: 0.845787\n",
      "  429224/3750000: episode: 629, duration: 4.356s, episode steps: 576, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.014372, mae: 3.574385, mean_q: 4.303445, mean_eps: 0.845582\n",
      "  429808/3750000: episode: 630, duration: 4.341s, episode steps: 584, steps per second: 135, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.017237, mae: 3.618861, mean_q: 4.357807, mean_eps: 0.845373\n",
      "  430380/3750000: episode: 631, duration: 4.309s, episode steps: 572, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.012423, mae: 3.565963, mean_q: 4.295195, mean_eps: 0.845168\n",
      "  430960/3750000: episode: 632, duration: 4.465s, episode steps: 580, steps per second: 130, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.013725, mae: 3.613007, mean_q: 4.348159, mean_eps: 0.844962\n",
      "  431348/3750000: episode: 633, duration: 2.976s, episode steps: 388, steps per second: 130, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.020490, mae: 3.633493, mean_q: 4.391028, mean_eps: 0.844786\n",
      "  432359/3750000: episode: 634, duration: 7.473s, episode steps: 1011, steps per second: 135, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.020392, mae: 3.661033, mean_q: 4.405015, mean_eps: 0.844534\n",
      "  432932/3750000: episode: 635, duration: 4.354s, episode steps: 573, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.018398, mae: 3.641256, mean_q: 4.381469, mean_eps: 0.844250\n",
      "  433378/3750000: episode: 636, duration: 3.331s, episode steps: 446, steps per second: 134, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.020461, mae: 3.666711, mean_q: 4.411002, mean_eps: 0.844066\n",
      "  433774/3750000: episode: 637, duration: 2.979s, episode steps: 396, steps per second: 133, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.021478, mae: 3.650413, mean_q: 4.384593, mean_eps: 0.843915\n",
      "  434152/3750000: episode: 638, duration: 2.820s, episode steps: 378, steps per second: 134, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.018665, mae: 3.647204, mean_q: 4.399042, mean_eps: 0.843774\n",
      "  435277/3750000: episode: 639, duration: 8.448s, episode steps: 1125, steps per second: 133, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.020007, mae: 3.684551, mean_q: 4.441414, mean_eps: 0.843504\n",
      "  435706/3750000: episode: 640, duration: 3.278s, episode steps: 429, steps per second: 131, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.019873, mae: 3.649667, mean_q: 4.386371, mean_eps: 0.843224\n",
      "  436687/3750000: episode: 641, duration: 7.301s, episode steps: 981, steps per second: 134, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.012860, mae: 3.690547, mean_q: 4.444664, mean_eps: 0.842968\n",
      "  437617/3750000: episode: 642, duration: 6.962s, episode steps: 930, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.017473, mae: 3.660623, mean_q: 4.406428, mean_eps: 0.842626\n",
      "  438227/3750000: episode: 643, duration: 4.662s, episode steps: 610, steps per second: 131, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.020392, mae: 3.658074, mean_q: 4.404220, mean_eps: 0.842349\n",
      "  439037/3750000: episode: 644, duration: 5.998s, episode steps: 810, steps per second: 135, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.020096, mae: 3.644486, mean_q: 4.383330, mean_eps: 0.842093\n",
      "  439963/3750000: episode: 645, duration: 7.065s, episode steps: 926, steps per second: 131, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.020013, mae: 3.591842, mean_q: 4.320181, mean_eps: 0.841780\n",
      "  441420/3750000: episode: 646, duration: 10.889s, episode steps: 1457, steps per second: 134, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.017499, mae: 3.617874, mean_q: 4.354367, mean_eps: 0.841352\n",
      "  441965/3750000: episode: 647, duration: 4.127s, episode steps: 545, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.020775, mae: 3.599786, mean_q: 4.328567, mean_eps: 0.840992\n",
      "  442638/3750000: episode: 648, duration: 4.980s, episode steps: 673, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.020112, mae: 3.594650, mean_q: 4.335111, mean_eps: 0.840772\n",
      "  443303/3750000: episode: 649, duration: 5.011s, episode steps: 665, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.019287, mae: 3.612442, mean_q: 4.348618, mean_eps: 0.840531\n",
      "  443932/3750000: episode: 650, duration: 4.721s, episode steps: 629, steps per second: 133, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.015130, mae: 3.600872, mean_q: 4.331679, mean_eps: 0.840297\n",
      "  444965/3750000: episode: 651, duration: 7.715s, episode steps: 1033, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.026630, mae: 3.606929, mean_q: 4.335120, mean_eps: 0.839998\n",
      "  445503/3750000: episode: 652, duration: 4.105s, episode steps: 538, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.014752, mae: 3.615580, mean_q: 4.350842, mean_eps: 0.839714\n",
      "  446207/3750000: episode: 653, duration: 5.265s, episode steps: 704, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.018511, mae: 3.577845, mean_q: 4.302309, mean_eps: 0.839490\n",
      "  446864/3750000: episode: 654, duration: 4.925s, episode steps: 657, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.020731, mae: 3.643931, mean_q: 4.381892, mean_eps: 0.839246\n",
      "  447678/3750000: episode: 655, duration: 6.040s, episode steps: 814, steps per second: 135, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.025279, mae: 3.590387, mean_q: 4.325281, mean_eps: 0.838983\n",
      "  448206/3750000: episode: 656, duration: 4.002s, episode steps: 528, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.018144, mae: 3.581656, mean_q: 4.312771, mean_eps: 0.838742\n",
      "  448784/3750000: episode: 657, duration: 4.313s, episode steps: 578, steps per second: 134, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.019746, mae: 3.651240, mean_q: 4.397447, mean_eps: 0.838540\n",
      "  449336/3750000: episode: 658, duration: 4.077s, episode steps: 552, steps per second: 135, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.020627, mae: 3.658586, mean_q: 4.410964, mean_eps: 0.838338\n",
      "  450151/3750000: episode: 659, duration: 6.181s, episode steps: 815, steps per second: 132, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.015947, mae: 3.631015, mean_q: 4.370687, mean_eps: 0.838094\n",
      "  450693/3750000: episode: 660, duration: 4.044s, episode steps: 542, steps per second: 134, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.014735, mae: 3.652938, mean_q: 4.398776, mean_eps: 0.837849\n",
      "  451371/3750000: episode: 661, duration: 5.099s, episode steps: 678, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.016927, mae: 3.637826, mean_q: 4.374369, mean_eps: 0.837629\n",
      "  452223/3750000: episode: 662, duration: 6.501s, episode steps: 852, steps per second: 131, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.021256, mae: 3.642569, mean_q: 4.386033, mean_eps: 0.837352\n",
      "  452717/3750000: episode: 663, duration: 3.622s, episode steps: 494, steps per second: 136, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015203, mae: 3.625410, mean_q: 4.363448, mean_eps: 0.837111\n",
      "  453668/3750000: episode: 664, duration: 7.191s, episode steps: 951, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.019732, mae: 3.615999, mean_q: 4.357815, mean_eps: 0.836852\n",
      "  454187/3750000: episode: 665, duration: 4.003s, episode steps: 519, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.021436, mae: 3.622481, mean_q: 4.366163, mean_eps: 0.836585\n",
      "  454582/3750000: episode: 666, duration: 3.010s, episode steps: 395, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.017194, mae: 3.607717, mean_q: 4.340812, mean_eps: 0.836420\n",
      "  455212/3750000: episode: 667, duration: 4.687s, episode steps: 630, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.016770, mae: 3.597227, mean_q: 4.330661, mean_eps: 0.836236\n",
      "  456238/3750000: episode: 668, duration: 7.684s, episode steps: 1026, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.016504, mae: 3.670348, mean_q: 4.417093, mean_eps: 0.835941\n",
      "  456865/3750000: episode: 669, duration: 4.752s, episode steps: 627, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.020605, mae: 3.611793, mean_q: 4.349086, mean_eps: 0.835642\n",
      "  457375/3750000: episode: 670, duration: 3.756s, episode steps: 510, steps per second: 136, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.014625, mae: 3.561030, mean_q: 4.289066, mean_eps: 0.835437\n",
      "  458290/3750000: episode: 671, duration: 6.908s, episode steps: 915, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.017248, mae: 3.627235, mean_q: 4.362746, mean_eps: 0.835181\n",
      "  459046/3750000: episode: 672, duration: 5.727s, episode steps: 756, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017572, mae: 3.613797, mean_q: 4.350630, mean_eps: 0.834879\n",
      "  459728/3750000: episode: 673, duration: 5.062s, episode steps: 682, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.018476, mae: 3.526936, mean_q: 4.238951, mean_eps: 0.834620\n",
      "  460530/3750000: episode: 674, duration: 6.005s, episode steps: 802, steps per second: 134, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.016244, mae: 3.566238, mean_q: 4.290623, mean_eps: 0.834353\n",
      "  461008/3750000: episode: 675, duration: 3.593s, episode steps: 478, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.024896, mae: 3.585182, mean_q: 4.309090, mean_eps: 0.834123\n",
      "  461988/3750000: episode: 676, duration: 7.345s, episode steps: 980, steps per second: 133, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.017727, mae: 3.524980, mean_q: 4.242460, mean_eps: 0.833860\n",
      "  462700/3750000: episode: 677, duration: 5.405s, episode steps: 712, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.023232, mae: 3.556104, mean_q: 4.273999, mean_eps: 0.833558\n",
      "  463225/3750000: episode: 678, duration: 4.030s, episode steps: 525, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.017540, mae: 3.495220, mean_q: 4.210099, mean_eps: 0.833334\n",
      "  463588/3750000: episode: 679, duration: 2.701s, episode steps: 363, steps per second: 134, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.017109, mae: 3.516117, mean_q: 4.230825, mean_eps: 0.833172\n",
      "  464774/3750000: episode: 680, duration: 8.879s, episode steps: 1186, steps per second: 134, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.020236, mae: 3.522984, mean_q: 4.244933, mean_eps: 0.832895\n",
      "  465495/3750000: episode: 681, duration: 5.403s, episode steps: 721, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.017121, mae: 3.476932, mean_q: 4.190955, mean_eps: 0.832553\n",
      "  466153/3750000: episode: 682, duration: 4.916s, episode steps: 658, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.012775, mae: 3.544992, mean_q: 4.269852, mean_eps: 0.832305\n",
      "  467132/3750000: episode: 683, duration: 7.489s, episode steps: 979, steps per second: 131, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.016641, mae: 3.514143, mean_q: 4.233693, mean_eps: 0.832010\n",
      "  467969/3750000: episode: 684, duration: 6.322s, episode steps: 837, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.021470, mae: 3.535317, mean_q: 4.254516, mean_eps: 0.831682\n",
      "  468956/3750000: episode: 685, duration: 7.503s, episode steps: 987, steps per second: 132, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017062, mae: 3.509822, mean_q: 4.235527, mean_eps: 0.831354\n",
      "  469550/3750000: episode: 686, duration: 4.506s, episode steps: 594, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015340, mae: 3.542495, mean_q: 4.269214, mean_eps: 0.831070\n",
      "  470378/3750000: episode: 687, duration: 6.176s, episode steps: 828, steps per second: 134, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.015035, mae: 3.514793, mean_q: 4.230811, mean_eps: 0.830814\n",
      "  470807/3750000: episode: 688, duration: 3.272s, episode steps: 429, steps per second: 131, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015449, mae: 3.481422, mean_q: 4.192400, mean_eps: 0.830588\n",
      "  471496/3750000: episode: 689, duration: 5.226s, episode steps: 689, steps per second: 132, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.022992, mae: 3.458608, mean_q: 4.160262, mean_eps: 0.830386\n",
      "  472553/3750000: episode: 690, duration: 7.938s, episode steps: 1057, steps per second: 133, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.015092, mae: 3.447141, mean_q: 4.145157, mean_eps: 0.830073\n",
      "  473267/3750000: episode: 691, duration: 5.442s, episode steps: 714, steps per second: 131, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013832, mae: 3.454994, mean_q: 4.153757, mean_eps: 0.829752\n",
      "  474385/3750000: episode: 692, duration: 8.357s, episode steps: 1118, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.015674, mae: 3.428233, mean_q: 4.123959, mean_eps: 0.829421\n",
      "  475209/3750000: episode: 693, duration: 6.149s, episode steps: 824, steps per second: 134, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.019715, mae: 3.443243, mean_q: 4.137506, mean_eps: 0.829072\n",
      "  476498/3750000: episode: 694, duration: 9.670s, episode steps: 1289, steps per second: 133, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.015591, mae: 3.443634, mean_q: 4.145225, mean_eps: 0.828694\n",
      "  477437/3750000: episode: 695, duration: 7.027s, episode steps: 939, steps per second: 134, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.018134, mae: 3.412395, mean_q: 4.107205, mean_eps: 0.828294\n",
      "  478152/3750000: episode: 696, duration: 5.440s, episode steps: 715, steps per second: 131, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.011424, mae: 3.458344, mean_q: 4.162246, mean_eps: 0.827996\n",
      "  478730/3750000: episode: 697, duration: 4.354s, episode steps: 578, steps per second: 133, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.020231, mae: 3.432616, mean_q: 4.127449, mean_eps: 0.827762\n",
      "  479423/3750000: episode: 698, duration: 5.197s, episode steps: 693, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.015826, mae: 3.425253, mean_q: 4.121671, mean_eps: 0.827531\n",
      "  480465/3750000: episode: 699, duration: 7.898s, episode steps: 1042, steps per second: 132, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.013613, mae: 3.397942, mean_q: 4.095353, mean_eps: 0.827218\n",
      "  481512/3750000: episode: 700, duration: 7.801s, episode steps: 1047, steps per second: 134, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.023347, mae: 3.419368, mean_q: 4.119089, mean_eps: 0.826844\n",
      "  482089/3750000: episode: 701, duration: 4.388s, episode steps: 577, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.021133, mae: 3.429358, mean_q: 4.125607, mean_eps: 0.826552\n",
      "  482516/3750000: episode: 702, duration: 3.200s, episode steps: 427, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.010927, mae: 3.453936, mean_q: 4.168992, mean_eps: 0.826372\n",
      "  482874/3750000: episode: 703, duration: 2.699s, episode steps: 358, steps per second: 133, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.011387, mae: 3.425164, mean_q: 4.124308, mean_eps: 0.826232\n",
      "  483382/3750000: episode: 704, duration: 3.930s, episode steps: 508, steps per second: 129, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.021042, mae: 3.433510, mean_q: 4.130419, mean_eps: 0.826073\n",
      "  484346/3750000: episode: 705, duration: 7.268s, episode steps: 964, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.020539, mae: 3.402139, mean_q: 4.104838, mean_eps: 0.825807\n",
      "  485042/3750000: episode: 706, duration: 5.272s, episode steps: 696, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.015332, mae: 3.402540, mean_q: 4.104195, mean_eps: 0.825508\n",
      "  485912/3750000: episode: 707, duration: 6.476s, episode steps: 870, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.017532, mae: 3.382888, mean_q: 4.070190, mean_eps: 0.825227\n",
      "  486445/3750000: episode: 708, duration: 4.074s, episode steps: 533, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.018708, mae: 3.441887, mean_q: 4.134191, mean_eps: 0.824975\n",
      "  486918/3750000: episode: 709, duration: 3.497s, episode steps: 473, steps per second: 135, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.015501, mae: 3.368437, mean_q: 4.057463, mean_eps: 0.824795\n",
      "  487506/3750000: episode: 710, duration: 4.471s, episode steps: 588, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.020274, mae: 3.382051, mean_q: 4.079045, mean_eps: 0.824604\n",
      "  488243/3750000: episode: 711, duration: 5.559s, episode steps: 737, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013129, mae: 3.405226, mean_q: 4.097090, mean_eps: 0.824363\n",
      "  488858/3750000: episode: 712, duration: 4.529s, episode steps: 615, steps per second: 136, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.015911, mae: 3.381554, mean_q: 4.067802, mean_eps: 0.824122\n",
      "  489256/3750000: episode: 713, duration: 3.008s, episode steps: 398, steps per second: 132, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016272, mae: 3.370627, mean_q: 4.053045, mean_eps: 0.823942\n",
      "  490114/3750000: episode: 714, duration: 6.432s, episode steps: 858, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.013384, mae: 3.398323, mean_q: 4.087738, mean_eps: 0.823715\n",
      "  490784/3750000: episode: 715, duration: 5.127s, episode steps: 670, steps per second: 131, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.017264, mae: 3.461775, mean_q: 4.162000, mean_eps: 0.823438\n",
      "  491472/3750000: episode: 716, duration: 5.141s, episode steps: 688, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.016240, mae: 3.442947, mean_q: 4.143292, mean_eps: 0.823193\n",
      "  492326/3750000: episode: 717, duration: 6.437s, episode steps: 854, steps per second: 133, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.020593, mae: 3.440021, mean_q: 4.151193, mean_eps: 0.822916\n",
      "  493136/3750000: episode: 718, duration: 6.105s, episode steps: 810, steps per second: 133, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.024496, mae: 3.480650, mean_q: 4.189918, mean_eps: 0.822617\n",
      "  493609/3750000: episode: 719, duration: 3.578s, episode steps: 473, steps per second: 132, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.012900, mae: 3.482512, mean_q: 4.200159, mean_eps: 0.822387\n",
      "  493961/3750000: episode: 720, duration: 2.669s, episode steps: 352, steps per second: 132, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.024416, mae: 3.508005, mean_q: 4.225346, mean_eps: 0.822236\n",
      "  494768/3750000: episode: 721, duration: 6.086s, episode steps: 807, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.016703, mae: 3.492903, mean_q: 4.220648, mean_eps: 0.822027\n",
      "  495423/3750000: episode: 722, duration: 4.932s, episode steps: 655, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.013416, mae: 3.464878, mean_q: 4.171147, mean_eps: 0.821764\n",
      "  496088/3750000: episode: 723, duration: 4.956s, episode steps: 665, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.019410, mae: 3.487722, mean_q: 4.200328, mean_eps: 0.821526\n",
      "  496763/3750000: episode: 724, duration: 5.145s, episode steps: 675, steps per second: 131, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.013687, mae: 3.502603, mean_q: 4.212648, mean_eps: 0.821285\n",
      "  497458/3750000: episode: 725, duration: 5.112s, episode steps: 695, steps per second: 136, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.019998, mae: 3.461300, mean_q: 4.168812, mean_eps: 0.821040\n",
      "  498125/3750000: episode: 726, duration: 5.034s, episode steps: 667, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.014452, mae: 3.494319, mean_q: 4.202880, mean_eps: 0.820796\n",
      "  498721/3750000: episode: 727, duration: 4.506s, episode steps: 596, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.013938, mae: 3.499573, mean_q: 4.213577, mean_eps: 0.820565\n",
      "  499163/3750000: episode: 728, duration: 3.430s, episode steps: 442, steps per second: 129, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.019571, mae: 3.479148, mean_q: 4.185148, mean_eps: 0.820378\n",
      "  499544/3750000: episode: 729, duration: 2.873s, episode steps: 381, steps per second: 133, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.017329, mae: 3.394171, mean_q: 4.082689, mean_eps: 0.820230\n",
      "  500293/3750000: episode: 730, duration: 5.839s, episode steps: 749, steps per second: 128, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.020820, mae: 3.488056, mean_q: 4.202175, mean_eps: 0.820029\n",
      "  501256/3750000: episode: 731, duration: 7.269s, episode steps: 963, steps per second: 132, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.023897, mae: 3.483075, mean_q: 4.194331, mean_eps: 0.819723\n",
      "  502353/3750000: episode: 732, duration: 8.207s, episode steps: 1097, steps per second: 134, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.013890, mae: 3.534919, mean_q: 4.252902, mean_eps: 0.819352\n",
      "  502857/3750000: episode: 733, duration: 3.759s, episode steps: 504, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014283, mae: 3.457352, mean_q: 4.160276, mean_eps: 0.819064\n",
      "  503805/3750000: episode: 734, duration: 7.185s, episode steps: 948, steps per second: 132, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.014070, mae: 3.509535, mean_q: 4.223720, mean_eps: 0.818801\n",
      "  504282/3750000: episode: 735, duration: 3.584s, episode steps: 477, steps per second: 133, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014113, mae: 3.484477, mean_q: 4.193335, mean_eps: 0.818542\n",
      "  505093/3750000: episode: 736, duration: 6.020s, episode steps: 811, steps per second: 135, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.018798, mae: 3.546096, mean_q: 4.269081, mean_eps: 0.818312\n",
      "  505466/3750000: episode: 737, duration: 2.852s, episode steps: 373, steps per second: 131, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.021094, mae: 3.563067, mean_q: 4.302485, mean_eps: 0.818099\n",
      "  506119/3750000: episode: 738, duration: 4.828s, episode steps: 653, steps per second: 135, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.014485, mae: 3.526196, mean_q: 4.245366, mean_eps: 0.817916\n",
      "  506983/3750000: episode: 739, duration: 6.529s, episode steps: 864, steps per second: 132, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.013599, mae: 3.532294, mean_q: 4.255665, mean_eps: 0.817642\n",
      "  507356/3750000: episode: 740, duration: 2.809s, episode steps: 373, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.012339, mae: 3.563178, mean_q: 4.296436, mean_eps: 0.817419\n",
      "  507822/3750000: episode: 741, duration: 3.551s, episode steps: 466, steps per second: 131, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.029694, mae: 3.470352, mean_q: 4.176356, mean_eps: 0.817268\n",
      "  508212/3750000: episode: 742, duration: 2.885s, episode steps: 390, steps per second: 135, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.021966, mae: 3.584767, mean_q: 4.313351, mean_eps: 0.817113\n",
      "  508626/3750000: episode: 743, duration: 3.129s, episode steps: 414, steps per second: 132, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.022346, mae: 3.499794, mean_q: 4.210121, mean_eps: 0.816969\n",
      "  509095/3750000: episode: 744, duration: 3.463s, episode steps: 469, steps per second: 135, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.026057, mae: 3.519875, mean_q: 4.235954, mean_eps: 0.816810\n",
      "  510040/3750000: episode: 745, duration: 7.185s, episode steps: 945, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.016763, mae: 3.594277, mean_q: 4.322812, mean_eps: 0.816558\n",
      "  510933/3750000: episode: 746, duration: 6.728s, episode steps: 893, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.018969, mae: 3.585754, mean_q: 4.315896, mean_eps: 0.816227\n",
      "  511623/3750000: episode: 747, duration: 5.297s, episode steps: 690, steps per second: 130, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.016778, mae: 3.540059, mean_q: 4.257254, mean_eps: 0.815939\n",
      "  512317/3750000: episode: 748, duration: 5.183s, episode steps: 694, steps per second: 134, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.015082, mae: 3.552986, mean_q: 4.277323, mean_eps: 0.815691\n",
      "  512957/3750000: episode: 749, duration: 4.785s, episode steps: 640, steps per second: 134, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014609, mae: 3.559615, mean_q: 4.288708, mean_eps: 0.815453\n",
      "  513508/3750000: episode: 750, duration: 4.202s, episode steps: 551, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014546, mae: 3.586938, mean_q: 4.313820, mean_eps: 0.815237\n",
      "  513943/3750000: episode: 751, duration: 3.337s, episode steps: 435, steps per second: 130, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.014074, mae: 3.655055, mean_q: 4.395120, mean_eps: 0.815057\n",
      "  514491/3750000: episode: 752, duration: 4.063s, episode steps: 548, steps per second: 135, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.012867, mae: 3.599646, mean_q: 4.335502, mean_eps: 0.814881\n",
      "  515330/3750000: episode: 753, duration: 6.366s, episode steps: 839, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.022353, mae: 3.625100, mean_q: 4.364529, mean_eps: 0.814632\n",
      "  515756/3750000: episode: 754, duration: 3.209s, episode steps: 426, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.017873, mae: 3.602868, mean_q: 4.341298, mean_eps: 0.814406\n",
      "  516693/3750000: episode: 755, duration: 7.043s, episode steps: 937, steps per second: 133, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.017566, mae: 3.661869, mean_q: 4.404913, mean_eps: 0.814161\n",
      "  517065/3750000: episode: 756, duration: 2.836s, episode steps: 372, steps per second: 131, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015125, mae: 3.668803, mean_q: 4.410497, mean_eps: 0.813923\n",
      "  518078/3750000: episode: 757, duration: 7.608s, episode steps: 1013, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.018886, mae: 3.646103, mean_q: 4.387881, mean_eps: 0.813675\n",
      "  518712/3750000: episode: 758, duration: 4.777s, episode steps: 634, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.019148, mae: 3.611462, mean_q: 4.342684, mean_eps: 0.813380\n",
      "  519284/3750000: episode: 759, duration: 4.341s, episode steps: 572, steps per second: 132, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.019413, mae: 3.637944, mean_q: 4.391064, mean_eps: 0.813160\n",
      "  519815/3750000: episode: 760, duration: 3.940s, episode steps: 531, steps per second: 135, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.021235, mae: 3.686615, mean_q: 4.436779, mean_eps: 0.812962\n",
      "  520629/3750000: episode: 761, duration: 6.172s, episode steps: 814, steps per second: 132, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.019051, mae: 3.607692, mean_q: 4.340254, mean_eps: 0.812721\n",
      "  521452/3750000: episode: 762, duration: 6.143s, episode steps: 823, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.019507, mae: 3.580941, mean_q: 4.309477, mean_eps: 0.812426\n",
      "  521844/3750000: episode: 763, duration: 2.956s, episode steps: 392, steps per second: 133, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.022646, mae: 3.612538, mean_q: 4.342816, mean_eps: 0.812206\n",
      "  522388/3750000: episode: 764, duration: 4.116s, episode steps: 544, steps per second: 132, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.019212, mae: 3.589725, mean_q: 4.317064, mean_eps: 0.812037\n",
      "  522873/3750000: episode: 765, duration: 3.607s, episode steps: 485, steps per second: 134, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.018207, mae: 3.538811, mean_q: 4.262624, mean_eps: 0.811853\n",
      "  523509/3750000: episode: 766, duration: 4.799s, episode steps: 636, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.018466, mae: 3.593889, mean_q: 4.325397, mean_eps: 0.811652\n",
      "  524387/3750000: episode: 767, duration: 6.722s, episode steps: 878, steps per second: 131, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.016726, mae: 3.650890, mean_q: 4.399013, mean_eps: 0.811378\n",
      "  525499/3750000: episode: 768, duration: 8.281s, episode steps: 1112, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.018429, mae: 3.605451, mean_q: 4.343681, mean_eps: 0.811022\n",
      "  525958/3750000: episode: 769, duration: 3.439s, episode steps: 459, steps per second: 133, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.895 [0.000, 5.000],  loss: 0.024832, mae: 3.584240, mean_q: 4.312023, mean_eps: 0.810741\n",
      "  526512/3750000: episode: 770, duration: 4.246s, episode steps: 554, steps per second: 130, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.016772, mae: 3.583707, mean_q: 4.306791, mean_eps: 0.810557\n",
      "  527107/3750000: episode: 771, duration: 4.492s, episode steps: 595, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.021183, mae: 3.593560, mean_q: 4.325299, mean_eps: 0.810348\n",
      "  528094/3750000: episode: 772, duration: 7.358s, episode steps: 987, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.015487, mae: 3.583830, mean_q: 4.324078, mean_eps: 0.810064\n",
      "  528819/3750000: episode: 773, duration: 5.473s, episode steps: 725, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.018751, mae: 3.615857, mean_q: 4.352660, mean_eps: 0.809758\n",
      "  529687/3750000: episode: 774, duration: 6.535s, episode steps: 868, steps per second: 133, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.020204, mae: 3.608350, mean_q: 4.347485, mean_eps: 0.809470\n",
      "  530142/3750000: episode: 775, duration: 3.445s, episode steps: 455, steps per second: 132, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.018059, mae: 3.585866, mean_q: 4.315641, mean_eps: 0.809229\n",
      "  530526/3750000: episode: 776, duration: 2.890s, episode steps: 384, steps per second: 133, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.022147, mae: 3.612625, mean_q: 4.348346, mean_eps: 0.809078\n",
      "  531037/3750000: episode: 777, duration: 3.866s, episode steps: 511, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.019692, mae: 3.611989, mean_q: 4.349576, mean_eps: 0.808919\n",
      "  531645/3750000: episode: 778, duration: 4.709s, episode steps: 608, steps per second: 129, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.028753, mae: 3.596049, mean_q: 4.328501, mean_eps: 0.808718\n",
      "  532363/3750000: episode: 779, duration: 5.392s, episode steps: 718, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.019888, mae: 3.596553, mean_q: 4.325781, mean_eps: 0.808476\n",
      "  533102/3750000: episode: 780, duration: 5.605s, episode steps: 739, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.016172, mae: 3.574609, mean_q: 4.299418, mean_eps: 0.808214\n",
      "  533842/3750000: episode: 781, duration: 5.607s, episode steps: 740, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.019624, mae: 3.593285, mean_q: 4.319446, mean_eps: 0.807947\n",
      "  534219/3750000: episode: 782, duration: 2.763s, episode steps: 377, steps per second: 136, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.910 [0.000, 5.000],  loss: 0.027279, mae: 3.593297, mean_q: 4.318271, mean_eps: 0.807749\n",
      "  535244/3750000: episode: 783, duration: 7.825s, episode steps: 1025, steps per second: 131, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.016464, mae: 3.586173, mean_q: 4.318607, mean_eps: 0.807497\n",
      "  535972/3750000: episode: 784, duration: 5.425s, episode steps: 728, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.015446, mae: 3.624578, mean_q: 4.371679, mean_eps: 0.807180\n",
      "  536660/3750000: episode: 785, duration: 5.189s, episode steps: 688, steps per second: 133, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.020461, mae: 3.592511, mean_q: 4.318357, mean_eps: 0.806928\n",
      "  537791/3750000: episode: 786, duration: 8.584s, episode steps: 1131, steps per second: 132, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.017182, mae: 3.577481, mean_q: 4.304430, mean_eps: 0.806601\n",
      "  538466/3750000: episode: 787, duration: 5.095s, episode steps: 675, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.012961, mae: 3.620632, mean_q: 4.353067, mean_eps: 0.806273\n",
      "  538919/3750000: episode: 788, duration: 3.329s, episode steps: 453, steps per second: 136, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.013329, mae: 3.612070, mean_q: 4.342845, mean_eps: 0.806072\n",
      "  540164/3750000: episode: 789, duration: 9.413s, episode steps: 1245, steps per second: 132, episode reward: 18.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.022441, mae: 3.592963, mean_q: 4.326245, mean_eps: 0.805766\n",
      "  540532/3750000: episode: 790, duration: 2.793s, episode steps: 368, steps per second: 132, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013005, mae: 3.602872, mean_q: 4.336792, mean_eps: 0.805474\n",
      "  540974/3750000: episode: 791, duration: 3.294s, episode steps: 442, steps per second: 134, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.024482, mae: 3.573591, mean_q: 4.302148, mean_eps: 0.805330\n",
      "  541448/3750000: episode: 792, duration: 3.646s, episode steps: 474, steps per second: 130, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.015970, mae: 3.589530, mean_q: 4.319517, mean_eps: 0.805164\n",
      "  542031/3750000: episode: 793, duration: 4.356s, episode steps: 583, steps per second: 134, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.019418, mae: 3.678970, mean_q: 4.420169, mean_eps: 0.804974\n",
      "  542797/3750000: episode: 794, duration: 5.713s, episode steps: 766, steps per second: 134, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.020680, mae: 3.556181, mean_q: 4.274649, mean_eps: 0.804732\n",
      "  543516/3750000: episode: 795, duration: 5.460s, episode steps: 719, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013110, mae: 3.596313, mean_q: 4.332394, mean_eps: 0.804466\n",
      "  544057/3750000: episode: 796, duration: 4.058s, episode steps: 541, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.018488, mae: 3.581437, mean_q: 4.308805, mean_eps: 0.804239\n",
      "  544705/3750000: episode: 797, duration: 4.880s, episode steps: 648, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.014907, mae: 3.644405, mean_q: 4.387305, mean_eps: 0.804023\n",
      "  545491/3750000: episode: 798, duration: 5.880s, episode steps: 786, steps per second: 134, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.018906, mae: 3.572259, mean_q: 4.301833, mean_eps: 0.803764\n",
      "  546110/3750000: episode: 799, duration: 4.615s, episode steps: 619, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.016270, mae: 3.641320, mean_q: 4.386769, mean_eps: 0.803512\n",
      "  547069/3750000: episode: 800, duration: 7.223s, episode steps: 959, steps per second: 133, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.015946, mae: 3.608740, mean_q: 4.346675, mean_eps: 0.803228\n",
      "  547508/3750000: episode: 801, duration: 3.320s, episode steps: 439, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.019756, mae: 3.611544, mean_q: 4.340374, mean_eps: 0.802976\n",
      "  547943/3750000: episode: 802, duration: 3.344s, episode steps: 435, steps per second: 130, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.013802, mae: 3.615093, mean_q: 4.348582, mean_eps: 0.802817\n",
      "  548654/3750000: episode: 803, duration: 5.256s, episode steps: 711, steps per second: 135, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.021285, mae: 3.669933, mean_q: 4.413225, mean_eps: 0.802612\n",
      "  549017/3750000: episode: 804, duration: 2.710s, episode steps: 363, steps per second: 134, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.016780, mae: 3.634815, mean_q: 4.376019, mean_eps: 0.802421\n",
      "  550233/3750000: episode: 805, duration: 9.229s, episode steps: 1216, steps per second: 132, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.023804, mae: 3.678854, mean_q: 4.427058, mean_eps: 0.802137\n",
      "  551062/3750000: episode: 806, duration: 6.229s, episode steps: 829, steps per second: 133, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.019677, mae: 3.632130, mean_q: 4.371355, mean_eps: 0.801766\n",
      "  552027/3750000: episode: 807, duration: 7.191s, episode steps: 965, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.016388, mae: 3.632042, mean_q: 4.368700, mean_eps: 0.801442\n",
      "  552661/3750000: episode: 808, duration: 4.773s, episode steps: 634, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.014256, mae: 3.603995, mean_q: 4.336590, mean_eps: 0.801154\n",
      "  553163/3750000: episode: 809, duration: 3.741s, episode steps: 502, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.024267, mae: 3.580926, mean_q: 4.305646, mean_eps: 0.800949\n",
      "  553790/3750000: episode: 810, duration: 4.620s, episode steps: 627, steps per second: 136, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.027411, mae: 3.562198, mean_q: 4.282474, mean_eps: 0.800747\n",
      "  554722/3750000: episode: 811, duration: 7.061s, episode steps: 932, steps per second: 132, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.017788, mae: 3.602881, mean_q: 4.337343, mean_eps: 0.800466\n",
      "  555222/3750000: episode: 812, duration: 3.742s, episode steps: 500, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.019742, mae: 3.633351, mean_q: 4.373164, mean_eps: 0.800207\n",
      "  555910/3750000: episode: 813, duration: 5.098s, episode steps: 688, steps per second: 135, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.014832, mae: 3.668100, mean_q: 4.416187, mean_eps: 0.799995\n",
      "  556521/3750000: episode: 814, duration: 4.723s, episode steps: 611, steps per second: 129, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.025074, mae: 3.668808, mean_q: 4.427761, mean_eps: 0.799761\n",
      "  557116/3750000: episode: 815, duration: 4.401s, episode steps: 595, steps per second: 135, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.017253, mae: 3.677702, mean_q: 4.426984, mean_eps: 0.799545\n",
      "  558086/3750000: episode: 816, duration: 7.287s, episode steps: 970, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.015473, mae: 3.680249, mean_q: 4.428462, mean_eps: 0.799264\n",
      "  558741/3750000: episode: 817, duration: 4.970s, episode steps: 655, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.019978, mae: 3.639972, mean_q: 4.380516, mean_eps: 0.798969\n",
      "  559488/3750000: episode: 818, duration: 5.577s, episode steps: 747, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.024895, mae: 3.679470, mean_q: 4.430797, mean_eps: 0.798717\n",
      "  560175/3750000: episode: 819, duration: 5.101s, episode steps: 687, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.017990, mae: 3.739407, mean_q: 4.502159, mean_eps: 0.798461\n",
      "  560850/3750000: episode: 820, duration: 5.163s, episode steps: 675, steps per second: 131, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.014808, mae: 3.690151, mean_q: 4.438856, mean_eps: 0.798216\n",
      "  561867/3750000: episode: 821, duration: 7.575s, episode steps: 1017, steps per second: 134, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.020866, mae: 3.709999, mean_q: 4.471717, mean_eps: 0.797910\n",
      "  562884/3750000: episode: 822, duration: 7.745s, episode steps: 1017, steps per second: 131, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.021353, mae: 3.648583, mean_q: 4.390899, mean_eps: 0.797543\n",
      "  563872/3750000: episode: 823, duration: 7.347s, episode steps: 988, steps per second: 134, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.017538, mae: 3.656077, mean_q: 4.406881, mean_eps: 0.797183\n",
      "  564517/3750000: episode: 824, duration: 4.782s, episode steps: 645, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.020352, mae: 3.607808, mean_q: 4.337232, mean_eps: 0.796892\n",
      "  565185/3750000: episode: 825, duration: 5.111s, episode steps: 668, steps per second: 131, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016417, mae: 3.641138, mean_q: 4.378363, mean_eps: 0.796654\n",
      "  565916/3750000: episode: 826, duration: 5.420s, episode steps: 731, steps per second: 135, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.015068, mae: 3.658646, mean_q: 4.400821, mean_eps: 0.796402\n",
      "  566906/3750000: episode: 827, duration: 7.499s, episode steps: 990, steps per second: 132, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.016252, mae: 3.630243, mean_q: 4.369478, mean_eps: 0.796092\n",
      "  567382/3750000: episode: 828, duration: 3.581s, episode steps: 476, steps per second: 133, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.019809, mae: 3.620272, mean_q: 4.360622, mean_eps: 0.795826\n",
      "  567734/3750000: episode: 829, duration: 2.589s, episode steps: 352, steps per second: 136, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.018441, mae: 3.604570, mean_q: 4.341008, mean_eps: 0.795678\n",
      "  568311/3750000: episode: 830, duration: 4.314s, episode steps: 577, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.013776, mae: 3.646788, mean_q: 4.383031, mean_eps: 0.795513\n",
      "  568921/3750000: episode: 831, duration: 4.639s, episode steps: 610, steps per second: 131, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.021559, mae: 3.702658, mean_q: 4.454008, mean_eps: 0.795297\n",
      "  569768/3750000: episode: 832, duration: 6.328s, episode steps: 847, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017003, mae: 3.685840, mean_q: 4.430474, mean_eps: 0.795034\n",
      "  570296/3750000: episode: 833, duration: 3.890s, episode steps: 528, steps per second: 136, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.992 [0.000, 5.000],  loss: 0.012771, mae: 3.717030, mean_q: 4.477088, mean_eps: 0.794789\n",
      "  570804/3750000: episode: 834, duration: 3.856s, episode steps: 508, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.016934, mae: 3.643758, mean_q: 4.377269, mean_eps: 0.794602\n",
      "  571641/3750000: episode: 835, duration: 6.369s, episode steps: 837, steps per second: 131, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.025033, mae: 3.644118, mean_q: 4.381755, mean_eps: 0.794357\n",
      "  572292/3750000: episode: 836, duration: 4.832s, episode steps: 651, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.017541, mae: 3.683368, mean_q: 4.434318, mean_eps: 0.794091\n",
      "  573002/3750000: episode: 837, duration: 5.336s, episode steps: 710, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.016391, mae: 3.609399, mean_q: 4.342335, mean_eps: 0.793846\n",
      "  573984/3750000: episode: 838, duration: 7.341s, episode steps: 982, steps per second: 134, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.021590, mae: 3.654024, mean_q: 4.403326, mean_eps: 0.793540\n",
      "  574612/3750000: episode: 839, duration: 4.657s, episode steps: 628, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.019575, mae: 3.676468, mean_q: 4.423796, mean_eps: 0.793252\n",
      "  575568/3750000: episode: 840, duration: 7.194s, episode steps: 956, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.016246, mae: 3.670579, mean_q: 4.411689, mean_eps: 0.792968\n",
      "  576126/3750000: episode: 841, duration: 4.166s, episode steps: 558, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013071, mae: 3.650552, mean_q: 4.393036, mean_eps: 0.792694\n",
      "  576693/3750000: episode: 842, duration: 4.385s, episode steps: 567, steps per second: 129, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.025885, mae: 3.675460, mean_q: 4.417710, mean_eps: 0.792492\n",
      "  577562/3750000: episode: 843, duration: 6.616s, episode steps: 869, steps per second: 131, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.017757, mae: 3.660542, mean_q: 4.405022, mean_eps: 0.792233\n",
      "  578481/3750000: episode: 844, duration: 6.879s, episode steps: 919, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.023792, mae: 3.629164, mean_q: 4.364071, mean_eps: 0.791909\n",
      "  579135/3750000: episode: 845, duration: 4.906s, episode steps: 654, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.017256, mae: 3.618851, mean_q: 4.350786, mean_eps: 0.791628\n",
      "  580078/3750000: episode: 846, duration: 7.054s, episode steps: 943, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.024106, mae: 3.650685, mean_q: 4.391372, mean_eps: 0.791344\n",
      "  580710/3750000: episode: 847, duration: 4.725s, episode steps: 632, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.014134, mae: 3.639013, mean_q: 4.379515, mean_eps: 0.791060\n",
      "  581502/3750000: episode: 848, duration: 5.935s, episode steps: 792, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.017868, mae: 3.600803, mean_q: 4.332759, mean_eps: 0.790800\n",
      "  582300/3750000: episode: 849, duration: 6.054s, episode steps: 798, steps per second: 132, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014469, mae: 3.589553, mean_q: 4.323610, mean_eps: 0.790516\n",
      "  583235/3750000: episode: 850, duration: 7.011s, episode steps: 935, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.015784, mae: 3.613362, mean_q: 4.351398, mean_eps: 0.790206\n",
      "  583854/3750000: episode: 851, duration: 4.720s, episode steps: 619, steps per second: 131, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.021469, mae: 3.663218, mean_q: 4.416185, mean_eps: 0.789926\n",
      "  584355/3750000: episode: 852, duration: 3.742s, episode steps: 501, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.019315, mae: 3.549526, mean_q: 4.265790, mean_eps: 0.789724\n",
      "  584899/3750000: episode: 853, duration: 4.032s, episode steps: 544, steps per second: 135, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.017701, mae: 3.569273, mean_q: 4.295413, mean_eps: 0.789537\n",
      "  585637/3750000: episode: 854, duration: 5.637s, episode steps: 738, steps per second: 131, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.014753, mae: 3.610548, mean_q: 4.343539, mean_eps: 0.789306\n",
      "  587331/3750000: episode: 855, duration: 12.717s, episode steps: 1694, steps per second: 133, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.017697, mae: 3.576010, mean_q: 4.297629, mean_eps: 0.788867\n",
      "  588012/3750000: episode: 856, duration: 5.105s, episode steps: 681, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013719, mae: 3.606159, mean_q: 4.344439, mean_eps: 0.788439\n",
      "  588892/3750000: episode: 857, duration: 6.610s, episode steps: 880, steps per second: 133, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.019655, mae: 3.605159, mean_q: 4.336976, mean_eps: 0.788158\n",
      "  589620/3750000: episode: 858, duration: 5.513s, episode steps: 728, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.016241, mae: 3.581470, mean_q: 4.315041, mean_eps: 0.787870\n",
      "  590307/3750000: episode: 859, duration: 5.277s, episode steps: 687, steps per second: 130, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.021494, mae: 3.592734, mean_q: 4.325703, mean_eps: 0.787614\n",
      "  590934/3750000: episode: 860, duration: 4.656s, episode steps: 627, steps per second: 135, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.037077, mae: 3.531479, mean_q: 4.253151, mean_eps: 0.787377\n",
      "  591937/3750000: episode: 861, duration: 7.481s, episode steps: 1003, steps per second: 134, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.014548, mae: 3.587335, mean_q: 4.316356, mean_eps: 0.787085\n",
      "  592729/3750000: episode: 862, duration: 6.022s, episode steps: 792, steps per second: 132, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.019037, mae: 3.624780, mean_q: 4.363888, mean_eps: 0.786761\n",
      "  593906/3750000: episode: 863, duration: 8.790s, episode steps: 1177, steps per second: 134, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.019421, mae: 3.621017, mean_q: 4.353698, mean_eps: 0.786405\n",
      "  595196/3750000: episode: 864, duration: 9.747s, episode steps: 1290, steps per second: 132, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.018704, mae: 3.543827, mean_q: 4.264093, mean_eps: 0.785962\n",
      "  595938/3750000: episode: 865, duration: 5.588s, episode steps: 742, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.019137, mae: 3.553753, mean_q: 4.276790, mean_eps: 0.785598\n",
      "  596641/3750000: episode: 866, duration: 5.404s, episode steps: 703, steps per second: 130, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.013012, mae: 3.539054, mean_q: 4.260649, mean_eps: 0.785336\n",
      "  597441/3750000: episode: 867, duration: 6.005s, episode steps: 800, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014445, mae: 3.547359, mean_q: 4.266301, mean_eps: 0.785062\n",
      "  598669/3750000: episode: 868, duration: 9.190s, episode steps: 1228, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.017917, mae: 3.529180, mean_q: 4.241730, mean_eps: 0.784698\n",
      "  599454/3750000: episode: 869, duration: 5.880s, episode steps: 785, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.027484, mae: 3.548436, mean_q: 4.264918, mean_eps: 0.784338\n",
      "  599939/3750000: episode: 870, duration: 3.602s, episode steps: 485, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.023698, mae: 3.507276, mean_q: 4.223456, mean_eps: 0.784112\n",
      "  600318/3750000: episode: 871, duration: 2.850s, episode steps: 379, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.014980, mae: 3.543449, mean_q: 4.270296, mean_eps: 0.783957\n",
      "  600832/3750000: episode: 872, duration: 3.911s, episode steps: 514, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.017589, mae: 3.514097, mean_q: 4.236542, mean_eps: 0.783795\n",
      "  601661/3750000: episode: 873, duration: 6.259s, episode steps: 829, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.021762, mae: 3.494227, mean_q: 4.207160, mean_eps: 0.783550\n",
      "  602230/3750000: episode: 874, duration: 4.228s, episode steps: 569, steps per second: 135, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.013803, mae: 3.556878, mean_q: 4.276396, mean_eps: 0.783298\n",
      "  602628/3750000: episode: 875, duration: 2.997s, episode steps: 398, steps per second: 133, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.013322, mae: 3.586653, mean_q: 4.314847, mean_eps: 0.783125\n",
      "  603142/3750000: episode: 876, duration: 3.946s, episode steps: 514, steps per second: 130, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.010691, mae: 3.535185, mean_q: 4.272289, mean_eps: 0.782960\n",
      "  603681/3750000: episode: 877, duration: 4.022s, episode steps: 539, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.023551, mae: 3.547366, mean_q: 4.261845, mean_eps: 0.782769\n",
      "  604187/3750000: episode: 878, duration: 3.766s, episode steps: 506, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.015464, mae: 3.551273, mean_q: 4.269920, mean_eps: 0.782582\n",
      "  605079/3750000: episode: 879, duration: 6.655s, episode steps: 892, steps per second: 134, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.018778, mae: 3.553948, mean_q: 4.270407, mean_eps: 0.782333\n",
      "  605986/3750000: episode: 880, duration: 6.853s, episode steps: 907, steps per second: 132, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.016039, mae: 3.542918, mean_q: 4.260250, mean_eps: 0.782009\n",
      "  606664/3750000: episode: 881, duration: 5.598s, episode steps: 678, steps per second: 121, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.018859, mae: 3.482028, mean_q: 4.194510, mean_eps: 0.781721\n",
      "  607505/3750000: episode: 882, duration: 6.449s, episode steps: 841, steps per second: 130, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.016610, mae: 3.509443, mean_q: 4.220887, mean_eps: 0.781448\n",
      "  608410/3750000: episode: 883, duration: 6.745s, episode steps: 905, steps per second: 134, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.017525, mae: 3.534549, mean_q: 4.252882, mean_eps: 0.781134\n",
      "  608933/3750000: episode: 884, duration: 3.905s, episode steps: 523, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012224, mae: 3.504066, mean_q: 4.212132, mean_eps: 0.780879\n",
      "  609797/3750000: episode: 885, duration: 6.549s, episode steps: 864, steps per second: 132, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.017772, mae: 3.528330, mean_q: 4.247516, mean_eps: 0.780630\n",
      "  610801/3750000: episode: 886, duration: 7.637s, episode steps: 1004, steps per second: 131, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.014947, mae: 3.533702, mean_q: 4.248193, mean_eps: 0.780292\n",
      "  611598/3750000: episode: 887, duration: 5.980s, episode steps: 797, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.014538, mae: 3.497198, mean_q: 4.207442, mean_eps: 0.779968\n",
      "  612486/3750000: episode: 888, duration: 6.718s, episode steps: 888, steps per second: 132, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.014156, mae: 3.487412, mean_q: 4.201322, mean_eps: 0.779666\n",
      "  613219/3750000: episode: 889, duration: 5.460s, episode steps: 733, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.016013, mae: 3.507622, mean_q: 4.217424, mean_eps: 0.779374\n",
      "  614161/3750000: episode: 890, duration: 7.171s, episode steps: 942, steps per second: 131, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.019583, mae: 3.481643, mean_q: 4.190223, mean_eps: 0.779072\n",
      "  614524/3750000: episode: 891, duration: 2.699s, episode steps: 363, steps per second: 135, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.018879, mae: 3.503074, mean_q: 4.218947, mean_eps: 0.778834\n",
      "  615180/3750000: episode: 892, duration: 4.963s, episode steps: 656, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.014395, mae: 3.505762, mean_q: 4.222566, mean_eps: 0.778654\n",
      "  616008/3750000: episode: 893, duration: 6.312s, episode steps: 828, steps per second: 131, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.018980, mae: 3.491664, mean_q: 4.200163, mean_eps: 0.778388\n",
      "  616656/3750000: episode: 894, duration: 4.810s, episode steps: 648, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.017938, mae: 3.463596, mean_q: 4.166721, mean_eps: 0.778121\n",
      "  617105/3750000: episode: 895, duration: 3.395s, episode steps: 449, steps per second: 132, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.016387, mae: 3.410806, mean_q: 4.106893, mean_eps: 0.777923\n",
      "  617602/3750000: episode: 896, duration: 3.743s, episode steps: 497, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.012141, mae: 3.460864, mean_q: 4.163878, mean_eps: 0.777750\n",
      "  618455/3750000: episode: 897, duration: 6.379s, episode steps: 853, steps per second: 134, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.017799, mae: 3.456229, mean_q: 4.154418, mean_eps: 0.777509\n",
      "  619359/3750000: episode: 898, duration: 6.727s, episode steps: 904, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.022741, mae: 3.455539, mean_q: 4.153335, mean_eps: 0.777196\n",
      "  619969/3750000: episode: 899, duration: 4.653s, episode steps: 610, steps per second: 131, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.022205, mae: 3.433309, mean_q: 4.146606, mean_eps: 0.776922\n",
      "  620601/3750000: episode: 900, duration: 4.747s, episode steps: 632, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.015022, mae: 3.466122, mean_q: 4.172811, mean_eps: 0.776696\n",
      "  621474/3750000: episode: 901, duration: 6.464s, episode steps: 873, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.014781, mae: 3.467993, mean_q: 4.173668, mean_eps: 0.776426\n",
      "  622253/3750000: episode: 902, duration: 6.006s, episode steps: 779, steps per second: 130, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.025578, mae: 3.402074, mean_q: 4.096978, mean_eps: 0.776130\n",
      "  622915/3750000: episode: 903, duration: 5.013s, episode steps: 662, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.014610, mae: 3.470150, mean_q: 4.173762, mean_eps: 0.775871\n",
      "  623679/3750000: episode: 904, duration: 5.722s, episode steps: 764, steps per second: 134, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.016456, mae: 3.471712, mean_q: 4.173868, mean_eps: 0.775616\n",
      "  624593/3750000: episode: 905, duration: 6.914s, episode steps: 914, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.015827, mae: 3.481312, mean_q: 4.187423, mean_eps: 0.775313\n",
      "  625250/3750000: episode: 906, duration: 4.932s, episode steps: 657, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.014576, mae: 3.501837, mean_q: 4.218306, mean_eps: 0.775029\n",
      "  626088/3750000: episode: 907, duration: 6.267s, episode steps: 838, steps per second: 134, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.017062, mae: 3.449721, mean_q: 4.154024, mean_eps: 0.774759\n",
      "  626750/3750000: episode: 908, duration: 5.027s, episode steps: 662, steps per second: 132, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.023022, mae: 3.480668, mean_q: 4.189711, mean_eps: 0.774489\n",
      "  627247/3750000: episode: 909, duration: 3.730s, episode steps: 497, steps per second: 133, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.015974, mae: 3.494113, mean_q: 4.201954, mean_eps: 0.774280\n",
      "  627916/3750000: episode: 910, duration: 4.955s, episode steps: 669, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.018413, mae: 3.464079, mean_q: 4.160340, mean_eps: 0.774071\n",
      "  629330/3750000: episode: 911, duration: 10.654s, episode steps: 1414, steps per second: 133, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.017288, mae: 3.448077, mean_q: 4.146295, mean_eps: 0.773697\n",
      "  629948/3750000: episode: 912, duration: 4.629s, episode steps: 618, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.010736, mae: 3.515029, mean_q: 4.230387, mean_eps: 0.773330\n",
      "  630338/3750000: episode: 913, duration: 2.871s, episode steps: 390, steps per second: 136, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.023386, mae: 3.422010, mean_q: 4.113745, mean_eps: 0.773150\n",
      "  631021/3750000: episode: 914, duration: 5.259s, episode steps: 683, steps per second: 130, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.013773, mae: 3.440624, mean_q: 4.140625, mean_eps: 0.772955\n",
      "  631726/3750000: episode: 915, duration: 5.237s, episode steps: 705, steps per second: 135, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.018678, mae: 3.499842, mean_q: 4.209414, mean_eps: 0.772703\n",
      "  632395/3750000: episode: 916, duration: 4.951s, episode steps: 669, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.018138, mae: 3.414672, mean_q: 4.104988, mean_eps: 0.772458\n",
      "  633040/3750000: episode: 917, duration: 4.954s, episode steps: 645, steps per second: 130, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.021577, mae: 3.431646, mean_q: 4.122903, mean_eps: 0.772224\n",
      "  633899/3750000: episode: 918, duration: 6.422s, episode steps: 859, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.017535, mae: 3.378685, mean_q: 4.066126, mean_eps: 0.771954\n",
      "  634547/3750000: episode: 919, duration: 4.894s, episode steps: 648, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014334, mae: 3.363231, mean_q: 4.046987, mean_eps: 0.771681\n",
      "  634898/3750000: episode: 920, duration: 2.645s, episode steps: 351, steps per second: 133, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009296, mae: 3.393086, mean_q: 4.088580, mean_eps: 0.771501\n",
      "  635538/3750000: episode: 921, duration: 4.795s, episode steps: 640, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.014502, mae: 3.388468, mean_q: 4.075913, mean_eps: 0.771324\n",
      "  636030/3750000: episode: 922, duration: 3.721s, episode steps: 492, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.016087, mae: 3.384833, mean_q: 4.077862, mean_eps: 0.771119\n",
      "  636710/3750000: episode: 923, duration: 5.094s, episode steps: 680, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.013928, mae: 3.313005, mean_q: 3.989359, mean_eps: 0.770907\n",
      "  637403/3750000: episode: 924, duration: 5.315s, episode steps: 693, steps per second: 130, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.019875, mae: 3.337380, mean_q: 4.014783, mean_eps: 0.770658\n",
      "  638027/3750000: episode: 925, duration: 4.759s, episode steps: 624, steps per second: 131, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.020282, mae: 3.332971, mean_q: 4.008554, mean_eps: 0.770421\n",
      "  638640/3750000: episode: 926, duration: 4.703s, episode steps: 613, steps per second: 130, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.014749, mae: 3.300892, mean_q: 3.973991, mean_eps: 0.770201\n",
      "  639499/3750000: episode: 927, duration: 6.489s, episode steps: 859, steps per second: 132, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.019150, mae: 3.346373, mean_q: 4.029070, mean_eps: 0.769938\n",
      "  640729/3750000: episode: 928, duration: 9.213s, episode steps: 1230, steps per second: 134, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.013647, mae: 3.363937, mean_q: 4.047182, mean_eps: 0.769560\n",
      "  641572/3750000: episode: 929, duration: 6.342s, episode steps: 843, steps per second: 133, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.016582, mae: 3.375899, mean_q: 4.061071, mean_eps: 0.769186\n",
      "  642533/3750000: episode: 930, duration: 7.250s, episode steps: 961, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.017321, mae: 3.339754, mean_q: 4.014105, mean_eps: 0.768862\n",
      "  643438/3750000: episode: 931, duration: 6.942s, episode steps: 905, steps per second: 130, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.020536, mae: 3.369775, mean_q: 4.054228, mean_eps: 0.768527\n",
      "  644410/3750000: episode: 932, duration: 7.327s, episode steps: 972, steps per second: 133, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.018380, mae: 3.354858, mean_q: 4.036905, mean_eps: 0.768189\n",
      "  644949/3750000: episode: 933, duration: 4.067s, episode steps: 539, steps per second: 133, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014428, mae: 3.390970, mean_q: 4.092334, mean_eps: 0.767915\n",
      "  645551/3750000: episode: 934, duration: 4.607s, episode steps: 602, steps per second: 131, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.012601, mae: 3.398175, mean_q: 4.089156, mean_eps: 0.767710\n",
      "  646176/3750000: episode: 935, duration: 4.709s, episode steps: 625, steps per second: 133, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.018293, mae: 3.387301, mean_q: 4.072568, mean_eps: 0.767490\n",
      "  646736/3750000: episode: 936, duration: 4.257s, episode steps: 560, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015915, mae: 3.388626, mean_q: 4.077795, mean_eps: 0.767278\n",
      "  647512/3750000: episode: 937, duration: 5.890s, episode steps: 776, steps per second: 132, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.016307, mae: 3.367060, mean_q: 4.051880, mean_eps: 0.767037\n",
      "  648453/3750000: episode: 938, duration: 7.053s, episode steps: 941, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.017514, mae: 3.374476, mean_q: 4.061221, mean_eps: 0.766727\n",
      "  649440/3750000: episode: 939, duration: 7.485s, episode steps: 987, steps per second: 132, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.016705, mae: 3.375286, mean_q: 4.065520, mean_eps: 0.766382\n",
      "  649845/3750000: episode: 940, duration: 3.159s, episode steps: 405, steps per second: 128, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.016101, mae: 3.468424, mean_q: 4.173060, mean_eps: 0.766130\n",
      "  650369/3750000: episode: 941, duration: 3.902s, episode steps: 524, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.015079, mae: 3.432295, mean_q: 4.130081, mean_eps: 0.765960\n",
      "  651415/3750000: episode: 942, duration: 7.888s, episode steps: 1046, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.016471, mae: 3.391628, mean_q: 4.078755, mean_eps: 0.765680\n",
      "  651853/3750000: episode: 943, duration: 3.360s, episode steps: 438, steps per second: 130, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.019746, mae: 3.371083, mean_q: 4.054740, mean_eps: 0.765413\n",
      "  653178/3750000: episode: 944, duration: 9.926s, episode steps: 1325, steps per second: 133, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.020969, mae: 3.372701, mean_q: 4.061986, mean_eps: 0.765096\n",
      "  653774/3750000: episode: 945, duration: 4.632s, episode steps: 596, steps per second: 129, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.020896, mae: 3.336050, mean_q: 4.015561, mean_eps: 0.764751\n",
      "  654152/3750000: episode: 946, duration: 2.886s, episode steps: 378, steps per second: 131, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.012182, mae: 3.374436, mean_q: 4.060754, mean_eps: 0.764574\n",
      "  654730/3750000: episode: 947, duration: 4.346s, episode steps: 578, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.016760, mae: 3.372306, mean_q: 4.059415, mean_eps: 0.764402\n",
      "  655742/3750000: episode: 948, duration: 7.603s, episode steps: 1012, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.019432, mae: 3.381170, mean_q: 4.073902, mean_eps: 0.764114\n",
      "  656594/3750000: episode: 949, duration: 6.463s, episode steps: 852, steps per second: 132, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016890, mae: 3.324015, mean_q: 4.005288, mean_eps: 0.763779\n",
      "  657507/3750000: episode: 950, duration: 6.906s, episode steps: 913, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.015536, mae: 3.318551, mean_q: 3.996013, mean_eps: 0.763462\n",
      "  658165/3750000: episode: 951, duration: 5.062s, episode steps: 658, steps per second: 130, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.015551, mae: 3.333267, mean_q: 4.021645, mean_eps: 0.763178\n",
      "  658542/3750000: episode: 952, duration: 2.863s, episode steps: 377, steps per second: 132, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.017552, mae: 3.335069, mean_q: 4.018912, mean_eps: 0.762990\n",
      "  659822/3750000: episode: 953, duration: 9.547s, episode steps: 1280, steps per second: 134, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.016441, mae: 3.322407, mean_q: 3.999428, mean_eps: 0.762692\n",
      "  660479/3750000: episode: 954, duration: 4.988s, episode steps: 657, steps per second: 132, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.019653, mae: 3.333445, mean_q: 4.013514, mean_eps: 0.762346\n",
      "  661321/3750000: episode: 955, duration: 6.353s, episode steps: 842, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.016138, mae: 3.288319, mean_q: 3.959266, mean_eps: 0.762076\n",
      "  662016/3750000: episode: 956, duration: 5.168s, episode steps: 695, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.011886, mae: 3.329689, mean_q: 4.009297, mean_eps: 0.761799\n",
      "  662880/3750000: episode: 957, duration: 6.591s, episode steps: 864, steps per second: 131, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.016401, mae: 3.285072, mean_q: 3.954412, mean_eps: 0.761522\n",
      "  664065/3750000: episode: 958, duration: 8.872s, episode steps: 1185, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.018122, mae: 3.283236, mean_q: 3.951384, mean_eps: 0.761151\n",
      "  664473/3750000: episode: 959, duration: 3.073s, episode steps: 408, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.009719, mae: 3.309051, mean_q: 3.987625, mean_eps: 0.760863\n",
      "  665333/3750000: episode: 960, duration: 6.444s, episode steps: 860, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.015895, mae: 3.283402, mean_q: 3.952833, mean_eps: 0.760636\n",
      "  666705/3750000: episode: 961, duration: 10.294s, episode steps: 1372, steps per second: 133, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.015930, mae: 3.291363, mean_q: 3.961819, mean_eps: 0.760233\n",
      "  667633/3750000: episode: 962, duration: 6.908s, episode steps: 928, steps per second: 134, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.020653, mae: 3.339936, mean_q: 4.023294, mean_eps: 0.759819\n",
      "  668571/3750000: episode: 963, duration: 7.011s, episode steps: 938, steps per second: 134, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.021586, mae: 3.330000, mean_q: 4.010316, mean_eps: 0.759484\n",
      "  669277/3750000: episode: 964, duration: 5.364s, episode steps: 706, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.013353, mae: 3.337327, mean_q: 4.020008, mean_eps: 0.759189\n",
      "  670073/3750000: episode: 965, duration: 5.953s, episode steps: 796, steps per second: 134, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016332, mae: 3.318789, mean_q: 3.995490, mean_eps: 0.758919\n",
      "  670469/3750000: episode: 966, duration: 2.991s, episode steps: 396, steps per second: 132, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.015603, mae: 3.311948, mean_q: 3.991057, mean_eps: 0.758703\n",
      "  671035/3750000: episode: 967, duration: 4.249s, episode steps: 566, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013623, mae: 3.241528, mean_q: 3.906506, mean_eps: 0.758530\n",
      "  671491/3750000: episode: 968, duration: 3.494s, episode steps: 456, steps per second: 131, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.015435, mae: 3.250671, mean_q: 3.910343, mean_eps: 0.758346\n",
      "  672057/3750000: episode: 969, duration: 4.292s, episode steps: 566, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.017010, mae: 3.275496, mean_q: 3.948950, mean_eps: 0.758163\n",
      "  672949/3750000: episode: 970, duration: 6.722s, episode steps: 892, steps per second: 133, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.020884, mae: 3.325898, mean_q: 4.005867, mean_eps: 0.757900\n",
      "  673663/3750000: episode: 971, duration: 5.459s, episode steps: 714, steps per second: 131, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.016488, mae: 3.256799, mean_q: 3.924880, mean_eps: 0.757608\n",
      "  674377/3750000: episode: 972, duration: 5.375s, episode steps: 714, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.014550, mae: 3.297829, mean_q: 3.971719, mean_eps: 0.757353\n",
      "  674982/3750000: episode: 973, duration: 4.672s, episode steps: 605, steps per second: 130, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014973, mae: 3.305897, mean_q: 3.984776, mean_eps: 0.757115\n",
      "  675875/3750000: episode: 974, duration: 6.688s, episode steps: 893, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.020575, mae: 3.315029, mean_q: 3.992424, mean_eps: 0.756845\n",
      "  676405/3750000: episode: 975, duration: 4.001s, episode steps: 530, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.014997, mae: 3.277449, mean_q: 3.951549, mean_eps: 0.756590\n",
      "  677314/3750000: episode: 976, duration: 6.806s, episode steps: 909, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.019032, mae: 3.237399, mean_q: 3.901516, mean_eps: 0.756330\n",
      "  677935/3750000: episode: 977, duration: 4.623s, episode steps: 621, steps per second: 134, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.015047, mae: 3.239904, mean_q: 3.910476, mean_eps: 0.756057\n",
      "  678351/3750000: episode: 978, duration: 3.153s, episode steps: 416, steps per second: 132, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.013736, mae: 3.266713, mean_q: 3.943371, mean_eps: 0.755870\n",
      "  679233/3750000: episode: 979, duration: 6.634s, episode steps: 882, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.014217, mae: 3.313801, mean_q: 3.989672, mean_eps: 0.755636\n",
      "  679823/3750000: episode: 980, duration: 4.522s, episode steps: 590, steps per second: 130, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.014847, mae: 3.274742, mean_q: 3.938688, mean_eps: 0.755369\n",
      "  680686/3750000: episode: 981, duration: 6.405s, episode steps: 863, steps per second: 135, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.011008, mae: 3.300194, mean_q: 3.974302, mean_eps: 0.755106\n",
      "  681364/3750000: episode: 982, duration: 5.116s, episode steps: 678, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.016058, mae: 3.342517, mean_q: 4.024654, mean_eps: 0.754829\n",
      "  681794/3750000: episode: 983, duration: 3.233s, episode steps: 430, steps per second: 133, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.011604, mae: 3.340821, mean_q: 4.019072, mean_eps: 0.754631\n",
      "  682835/3750000: episode: 984, duration: 7.777s, episode steps: 1041, steps per second: 134, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.020773, mae: 3.287773, mean_q: 3.957040, mean_eps: 0.754368\n",
      "  683415/3750000: episode: 985, duration: 4.344s, episode steps: 580, steps per second: 134, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.020516, mae: 3.261327, mean_q: 3.923904, mean_eps: 0.754077\n",
      "  683909/3750000: episode: 986, duration: 3.778s, episode steps: 494, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015444, mae: 3.260150, mean_q: 3.922098, mean_eps: 0.753882\n",
      "  684357/3750000: episode: 987, duration: 3.327s, episode steps: 448, steps per second: 135, episode reward: 12.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.018307, mae: 3.312903, mean_q: 3.986656, mean_eps: 0.753713\n",
      "  685260/3750000: episode: 988, duration: 6.882s, episode steps: 903, steps per second: 131, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.014657, mae: 3.268485, mean_q: 3.937450, mean_eps: 0.753472\n",
      "  685920/3750000: episode: 989, duration: 5.109s, episode steps: 660, steps per second: 129, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.021673, mae: 3.262761, mean_q: 3.927690, mean_eps: 0.753191\n",
      "  686867/3750000: episode: 990, duration: 7.103s, episode steps: 947, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.014401, mae: 3.282819, mean_q: 3.952980, mean_eps: 0.752900\n",
      "  687537/3750000: episode: 991, duration: 4.955s, episode steps: 670, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.011505, mae: 3.312695, mean_q: 3.989773, mean_eps: 0.752608\n",
      "  688100/3750000: episode: 992, duration: 4.380s, episode steps: 563, steps per second: 129, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.016642, mae: 3.309809, mean_q: 3.981307, mean_eps: 0.752388\n",
      "  688803/3750000: episode: 993, duration: 5.319s, episode steps: 703, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.018385, mae: 3.274091, mean_q: 3.949165, mean_eps: 0.752158\n",
      "  689624/3750000: episode: 994, duration: 6.150s, episode steps: 821, steps per second: 133, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.013810, mae: 3.306752, mean_q: 3.987991, mean_eps: 0.751881\n",
      "  690583/3750000: episode: 995, duration: 7.360s, episode steps: 959, steps per second: 130, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013107, mae: 3.346569, mean_q: 4.027200, mean_eps: 0.751560\n",
      "  691543/3750000: episode: 996, duration: 7.186s, episode steps: 960, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.018013, mae: 3.325124, mean_q: 4.003816, mean_eps: 0.751215\n",
      "  692181/3750000: episode: 997, duration: 4.883s, episode steps: 638, steps per second: 131, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.017848, mae: 3.361281, mean_q: 4.043705, mean_eps: 0.750927\n",
      "  692969/3750000: episode: 998, duration: 5.852s, episode steps: 788, steps per second: 135, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.012221, mae: 3.347872, mean_q: 4.031744, mean_eps: 0.750671\n",
      "  693511/3750000: episode: 999, duration: 4.046s, episode steps: 542, steps per second: 134, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.013786, mae: 3.304139, mean_q: 3.985347, mean_eps: 0.750434\n",
      "  694067/3750000: episode: 1000, duration: 4.175s, episode steps: 556, steps per second: 133, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.031326, mae: 3.375113, mean_q: 4.068808, mean_eps: 0.750236\n",
      "  694693/3750000: episode: 1001, duration: 4.747s, episode steps: 626, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.013745, mae: 3.361526, mean_q: 4.048314, mean_eps: 0.750023\n",
      "  695250/3750000: episode: 1002, duration: 4.238s, episode steps: 557, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.013809, mae: 3.314693, mean_q: 3.996129, mean_eps: 0.749811\n",
      "  695929/3750000: episode: 1003, duration: 5.082s, episode steps: 679, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.016996, mae: 3.351515, mean_q: 4.035292, mean_eps: 0.749588\n",
      "  697000/3750000: episode: 1004, duration: 8.092s, episode steps: 1071, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.014357, mae: 3.375336, mean_q: 4.060980, mean_eps: 0.749274\n",
      "  697681/3750000: episode: 1005, duration: 5.153s, episode steps: 681, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.017023, mae: 3.344355, mean_q: 4.027945, mean_eps: 0.748958\n",
      "  698065/3750000: episode: 1006, duration: 2.875s, episode steps: 384, steps per second: 134, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.017999, mae: 3.291388, mean_q: 3.975026, mean_eps: 0.748763\n",
      "  698410/3750000: episode: 1007, duration: 2.619s, episode steps: 345, steps per second: 132, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.012916, mae: 3.318960, mean_q: 4.014365, mean_eps: 0.748634\n",
      "  699008/3750000: episode: 1008, duration: 4.502s, episode steps: 598, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.025342, mae: 3.383575, mean_q: 4.082632, mean_eps: 0.748464\n",
      "  699547/3750000: episode: 1009, duration: 4.024s, episode steps: 539, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.013635, mae: 3.396826, mean_q: 4.090857, mean_eps: 0.748259\n",
      "  700069/3750000: episode: 1010, duration: 3.928s, episode steps: 522, steps per second: 133, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.022776, mae: 3.391268, mean_q: 4.077809, mean_eps: 0.748068\n",
      "  700579/3750000: episode: 1011, duration: 3.850s, episode steps: 510, steps per second: 132, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.021845, mae: 3.418373, mean_q: 4.112464, mean_eps: 0.747885\n",
      "  701645/3750000: episode: 1012, duration: 8.136s, episode steps: 1066, steps per second: 131, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011918, mae: 3.431259, mean_q: 4.131445, mean_eps: 0.747600\n",
      "  702043/3750000: episode: 1013, duration: 2.993s, episode steps: 398, steps per second: 133, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.007593, mae: 3.422987, mean_q: 4.118612, mean_eps: 0.747334\n",
      "  702700/3750000: episode: 1014, duration: 4.982s, episode steps: 657, steps per second: 132, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.018297, mae: 3.415604, mean_q: 4.110745, mean_eps: 0.747147\n",
      "  703632/3750000: episode: 1015, duration: 7.031s, episode steps: 932, steps per second: 133, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.021467, mae: 3.384858, mean_q: 4.078208, mean_eps: 0.746862\n",
      "  704679/3750000: episode: 1016, duration: 7.830s, episode steps: 1047, steps per second: 134, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.013865, mae: 3.409329, mean_q: 4.104095, mean_eps: 0.746506\n",
      "  705170/3750000: episode: 1017, duration: 3.782s, episode steps: 491, steps per second: 130, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.017398, mae: 3.349946, mean_q: 4.031798, mean_eps: 0.746229\n",
      "  705887/3750000: episode: 1018, duration: 5.401s, episode steps: 717, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.014211, mae: 3.480898, mean_q: 4.192678, mean_eps: 0.746009\n",
      "  706432/3750000: episode: 1019, duration: 4.117s, episode steps: 545, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.018086, mae: 3.360985, mean_q: 4.041695, mean_eps: 0.745782\n",
      "  707168/3750000: episode: 1020, duration: 5.595s, episode steps: 736, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.015366, mae: 3.414140, mean_q: 4.110379, mean_eps: 0.745552\n",
      "  708462/3750000: episode: 1021, duration: 9.661s, episode steps: 1294, steps per second: 134, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.016262, mae: 3.414791, mean_q: 4.112126, mean_eps: 0.745185\n",
      "  709426/3750000: episode: 1022, duration: 7.338s, episode steps: 964, steps per second: 131, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.018802, mae: 3.396394, mean_q: 4.081231, mean_eps: 0.744778\n",
      "  710192/3750000: episode: 1023, duration: 5.763s, episode steps: 766, steps per second: 133, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.014778, mae: 3.399346, mean_q: 4.086274, mean_eps: 0.744468\n",
      "  710846/3750000: episode: 1024, duration: 4.953s, episode steps: 654, steps per second: 132, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014783, mae: 3.414874, mean_q: 4.106614, mean_eps: 0.744213\n",
      "  711371/3750000: episode: 1025, duration: 3.966s, episode steps: 525, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.012578, mae: 3.342359, mean_q: 4.022553, mean_eps: 0.744000\n",
      "  712341/3750000: episode: 1026, duration: 7.291s, episode steps: 970, steps per second: 133, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.016298, mae: 3.376847, mean_q: 4.060382, mean_eps: 0.743730\n",
      "  712955/3750000: episode: 1027, duration: 4.545s, episode steps: 614, steps per second: 135, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014795, mae: 3.331803, mean_q: 4.011589, mean_eps: 0.743446\n",
      "  713704/3750000: episode: 1028, duration: 5.706s, episode steps: 749, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.013454, mae: 3.353080, mean_q: 4.035249, mean_eps: 0.743201\n",
      "  714426/3750000: episode: 1029, duration: 5.407s, episode steps: 722, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.015745, mae: 3.386673, mean_q: 4.076029, mean_eps: 0.742935\n",
      "  715032/3750000: episode: 1030, duration: 4.507s, episode steps: 606, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.012711, mae: 3.425348, mean_q: 4.122035, mean_eps: 0.742697\n",
      "  715697/3750000: episode: 1031, duration: 4.990s, episode steps: 665, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.015812, mae: 3.432851, mean_q: 4.140095, mean_eps: 0.742470\n",
      "  716528/3750000: episode: 1032, duration: 6.256s, episode steps: 831, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.013669, mae: 3.388576, mean_q: 4.079681, mean_eps: 0.742200\n",
      "  717071/3750000: episode: 1033, duration: 4.072s, episode steps: 543, steps per second: 133, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.013901, mae: 3.368266, mean_q: 4.055184, mean_eps: 0.741952\n",
      "  717604/3750000: episode: 1034, duration: 4.064s, episode steps: 533, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.017209, mae: 3.361824, mean_q: 4.042978, mean_eps: 0.741758\n",
      "  718721/3750000: episode: 1035, duration: 8.387s, episode steps: 1117, steps per second: 133, episode reward: 33.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.016549, mae: 3.407058, mean_q: 4.107605, mean_eps: 0.741459\n",
      "  719285/3750000: episode: 1036, duration: 4.223s, episode steps: 564, steps per second: 134, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.015249, mae: 3.393446, mean_q: 4.090507, mean_eps: 0.741156\n",
      "  720065/3750000: episode: 1037, duration: 5.902s, episode steps: 780, steps per second: 132, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.019071, mae: 3.435784, mean_q: 4.133670, mean_eps: 0.740915\n",
      "  720887/3750000: episode: 1038, duration: 6.134s, episode steps: 822, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.019808, mae: 3.423232, mean_q: 4.120135, mean_eps: 0.740627\n",
      "  721256/3750000: episode: 1039, duration: 2.736s, episode steps: 369, steps per second: 135, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.020653, mae: 3.535635, mean_q: 4.270648, mean_eps: 0.740415\n",
      "  722155/3750000: episode: 1040, duration: 6.787s, episode steps: 899, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.018870, mae: 3.486359, mean_q: 4.198045, mean_eps: 0.740188\n",
      "  723499/3750000: episode: 1041, duration: 10.000s, episode steps: 1344, steps per second: 134, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.018261, mae: 3.417804, mean_q: 4.116622, mean_eps: 0.739785\n",
      "  724040/3750000: episode: 1042, duration: 4.168s, episode steps: 541, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.012302, mae: 3.362156, mean_q: 4.046872, mean_eps: 0.739446\n",
      "  724686/3750000: episode: 1043, duration: 4.899s, episode steps: 646, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.013893, mae: 3.439089, mean_q: 4.141440, mean_eps: 0.739230\n",
      "  725305/3750000: episode: 1044, duration: 4.629s, episode steps: 619, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.015631, mae: 3.378429, mean_q: 4.062923, mean_eps: 0.739000\n",
      "  726383/3750000: episode: 1045, duration: 8.183s, episode steps: 1078, steps per second: 132, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.022640, mae: 3.370937, mean_q: 4.065382, mean_eps: 0.738694\n",
      "  727179/3750000: episode: 1046, duration: 5.908s, episode steps: 796, steps per second: 135, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.013569, mae: 3.374779, mean_q: 4.064892, mean_eps: 0.738359\n",
      "  727970/3750000: episode: 1047, duration: 5.931s, episode steps: 791, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.020974, mae: 3.356438, mean_q: 4.041764, mean_eps: 0.738075\n",
      "  729119/3750000: episode: 1048, duration: 8.616s, episode steps: 1149, steps per second: 133, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.016790, mae: 3.408747, mean_q: 4.100706, mean_eps: 0.737726\n",
      "  730035/3750000: episode: 1049, duration: 6.879s, episode steps: 916, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.014092, mae: 3.398223, mean_q: 4.093761, mean_eps: 0.737355\n",
      "  730590/3750000: episode: 1050, duration: 4.296s, episode steps: 555, steps per second: 129, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.017356, mae: 3.394407, mean_q: 4.079450, mean_eps: 0.737088\n",
      "  731391/3750000: episode: 1051, duration: 5.993s, episode steps: 801, steps per second: 134, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.017629, mae: 3.369369, mean_q: 4.051216, mean_eps: 0.736844\n",
      "  732062/3750000: episode: 1052, duration: 5.076s, episode steps: 671, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.018871, mae: 3.377133, mean_q: 4.065215, mean_eps: 0.736577\n",
      "  732624/3750000: episode: 1053, duration: 4.275s, episode steps: 562, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.015730, mae: 3.448869, mean_q: 4.149501, mean_eps: 0.736354\n",
      "  733251/3750000: episode: 1054, duration: 4.689s, episode steps: 627, steps per second: 134, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.863 [0.000, 5.000],  loss: 0.016330, mae: 3.402034, mean_q: 4.091405, mean_eps: 0.736142\n",
      "  734202/3750000: episode: 1055, duration: 7.156s, episode steps: 951, steps per second: 133, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015587, mae: 3.410486, mean_q: 4.101531, mean_eps: 0.735857\n",
      "  734636/3750000: episode: 1056, duration: 3.242s, episode steps: 434, steps per second: 134, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.025072, mae: 3.338913, mean_q: 4.010390, mean_eps: 0.735609\n",
      "  735169/3750000: episode: 1057, duration: 4.019s, episode steps: 533, steps per second: 133, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.024293, mae: 3.413881, mean_q: 4.102600, mean_eps: 0.735436\n",
      "  735779/3750000: episode: 1058, duration: 4.690s, episode steps: 610, steps per second: 130, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.015662, mae: 3.379722, mean_q: 4.077202, mean_eps: 0.735231\n",
      "  736770/3750000: episode: 1059, duration: 7.559s, episode steps: 991, steps per second: 131, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017650, mae: 3.418710, mean_q: 4.110302, mean_eps: 0.734943\n",
      "  737455/3750000: episode: 1060, duration: 5.132s, episode steps: 685, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.018059, mae: 3.446835, mean_q: 4.153098, mean_eps: 0.734640\n",
      "  737866/3750000: episode: 1061, duration: 3.215s, episode steps: 411, steps per second: 128, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.015356, mae: 3.406743, mean_q: 4.109802, mean_eps: 0.734442\n",
      "  738463/3750000: episode: 1062, duration: 4.512s, episode steps: 597, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.015141, mae: 3.439705, mean_q: 4.143175, mean_eps: 0.734259\n",
      "  739041/3750000: episode: 1063, duration: 4.406s, episode steps: 578, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.020315, mae: 3.432903, mean_q: 4.128232, mean_eps: 0.734046\n",
      "  739513/3750000: episode: 1064, duration: 3.495s, episode steps: 472, steps per second: 135, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.018663, mae: 3.388719, mean_q: 4.073028, mean_eps: 0.733859\n",
      "  740062/3750000: episode: 1065, duration: 4.186s, episode steps: 549, steps per second: 131, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.023612, mae: 3.413493, mean_q: 4.112805, mean_eps: 0.733676\n",
      "  740616/3750000: episode: 1066, duration: 4.099s, episode steps: 554, steps per second: 135, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.017169, mae: 3.435918, mean_q: 4.145080, mean_eps: 0.733478\n",
      "  741261/3750000: episode: 1067, duration: 4.932s, episode steps: 645, steps per second: 131, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.018948, mae: 3.443118, mean_q: 4.143223, mean_eps: 0.733262\n",
      "  742031/3750000: episode: 1068, duration: 5.701s, episode steps: 770, steps per second: 135, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.017814, mae: 3.456756, mean_q: 4.155490, mean_eps: 0.733006\n",
      "  743803/3750000: episode: 1069, duration: 13.228s, episode steps: 1772, steps per second: 134, episode reward: 35.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.016259, mae: 3.457391, mean_q: 4.161683, mean_eps: 0.732549\n",
      "  744775/3750000: episode: 1070, duration: 7.189s, episode steps: 972, steps per second: 135, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.020159, mae: 3.472015, mean_q: 4.185247, mean_eps: 0.732056\n",
      "  745556/3750000: episode: 1071, duration: 5.903s, episode steps: 781, steps per second: 132, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.015647, mae: 3.431901, mean_q: 4.122295, mean_eps: 0.731742\n",
      "  746251/3750000: episode: 1072, duration: 5.220s, episode steps: 695, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.021893, mae: 3.442593, mean_q: 4.141854, mean_eps: 0.731476\n",
      "  747224/3750000: episode: 1073, duration: 7.316s, episode steps: 973, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.018296, mae: 3.448707, mean_q: 4.152813, mean_eps: 0.731174\n",
      "  747721/3750000: episode: 1074, duration: 3.800s, episode steps: 497, steps per second: 131, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.018772, mae: 3.523774, mean_q: 4.239943, mean_eps: 0.730907\n",
      "  748322/3750000: episode: 1075, duration: 4.496s, episode steps: 601, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.017973, mae: 3.418556, mean_q: 4.120356, mean_eps: 0.730709\n",
      "  748717/3750000: episode: 1076, duration: 2.916s, episode steps: 395, steps per second: 135, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.022439, mae: 3.526339, mean_q: 4.245146, mean_eps: 0.730533\n",
      "  749122/3750000: episode: 1077, duration: 3.096s, episode steps: 405, steps per second: 131, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.013346, mae: 3.416796, mean_q: 4.115319, mean_eps: 0.730389\n",
      "  750041/3750000: episode: 1078, duration: 6.957s, episode steps: 919, steps per second: 132, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.025361, mae: 3.437986, mean_q: 4.141688, mean_eps: 0.730148\n",
      "  750498/3750000: episode: 1079, duration: 3.446s, episode steps: 457, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.015795, mae: 3.479346, mean_q: 4.203705, mean_eps: 0.729903\n",
      "  750970/3750000: episode: 1080, duration: 3.588s, episode steps: 472, steps per second: 132, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.011337, mae: 3.379600, mean_q: 4.078902, mean_eps: 0.729737\n",
      "  751484/3750000: episode: 1081, duration: 3.904s, episode steps: 514, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.023075, mae: 3.422927, mean_q: 4.122806, mean_eps: 0.729557\n",
      "  752113/3750000: episode: 1082, duration: 4.708s, episode steps: 629, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.020678, mae: 3.504274, mean_q: 4.215621, mean_eps: 0.729352\n",
      "  753319/3750000: episode: 1083, duration: 8.966s, episode steps: 1206, steps per second: 135, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.016265, mae: 3.431928, mean_q: 4.133936, mean_eps: 0.729024\n",
      "  754212/3750000: episode: 1084, duration: 6.862s, episode steps: 893, steps per second: 130, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.018792, mae: 3.436430, mean_q: 4.144358, mean_eps: 0.728646\n",
      "  754611/3750000: episode: 1085, duration: 3.028s, episode steps: 399, steps per second: 132, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.010325, mae: 3.378735, mean_q: 4.071534, mean_eps: 0.728412\n",
      "  754999/3750000: episode: 1086, duration: 2.865s, episode steps: 388, steps per second: 135, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.031955, mae: 3.393280, mean_q: 4.084048, mean_eps: 0.728272\n",
      "  755823/3750000: episode: 1087, duration: 6.317s, episode steps: 824, steps per second: 130, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.016609, mae: 3.421800, mean_q: 4.121091, mean_eps: 0.728052\n",
      "  756740/3750000: episode: 1088, duration: 6.864s, episode steps: 917, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.019300, mae: 3.415317, mean_q: 4.113968, mean_eps: 0.727739\n",
      "  757548/3750000: episode: 1089, duration: 6.110s, episode steps: 808, steps per second: 132, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.018736, mae: 3.366948, mean_q: 4.053203, mean_eps: 0.727430\n",
      "  758524/3750000: episode: 1090, duration: 7.390s, episode steps: 976, steps per second: 132, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.015963, mae: 3.398366, mean_q: 4.095779, mean_eps: 0.727106\n",
      "  759437/3750000: episode: 1091, duration: 6.773s, episode steps: 913, steps per second: 135, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.013661, mae: 3.356979, mean_q: 4.041993, mean_eps: 0.726767\n",
      "  760175/3750000: episode: 1092, duration: 5.581s, episode steps: 738, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.018153, mae: 3.379732, mean_q: 4.066096, mean_eps: 0.726472\n",
      "  761258/3750000: episode: 1093, duration: 8.062s, episode steps: 1083, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.015005, mae: 3.289723, mean_q: 3.960488, mean_eps: 0.726144\n",
      "  762148/3750000: episode: 1094, duration: 6.759s, episode steps: 890, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.021296, mae: 3.363784, mean_q: 4.046220, mean_eps: 0.725788\n",
      "  762772/3750000: episode: 1095, duration: 4.718s, episode steps: 624, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.020634, mae: 3.290882, mean_q: 3.962984, mean_eps: 0.725514\n",
      "  763310/3750000: episode: 1096, duration: 4.054s, episode steps: 538, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.012385, mae: 3.331515, mean_q: 4.013388, mean_eps: 0.725306\n",
      "  764469/3750000: episode: 1097, duration: 8.755s, episode steps: 1159, steps per second: 132, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.014077, mae: 3.304155, mean_q: 3.976213, mean_eps: 0.725000\n",
      "  765307/3750000: episode: 1098, duration: 6.281s, episode steps: 838, steps per second: 133, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.018417, mae: 3.331522, mean_q: 4.007387, mean_eps: 0.724640\n",
      "  766309/3750000: episode: 1099, duration: 7.456s, episode steps: 1002, steps per second: 134, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.016480, mae: 3.330452, mean_q: 4.015241, mean_eps: 0.724308\n",
      "  767037/3750000: episode: 1100, duration: 5.467s, episode steps: 728, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.019202, mae: 3.382646, mean_q: 4.071599, mean_eps: 0.723999\n",
      "  768075/3750000: episode: 1101, duration: 7.779s, episode steps: 1038, steps per second: 133, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.018285, mae: 3.369910, mean_q: 4.057851, mean_eps: 0.723682\n",
      "  768595/3750000: episode: 1102, duration: 3.969s, episode steps: 520, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.013280, mae: 3.360514, mean_q: 4.039860, mean_eps: 0.723401\n",
      "  769154/3750000: episode: 1103, duration: 4.232s, episode steps: 559, steps per second: 132, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.017633, mae: 3.323629, mean_q: 3.995919, mean_eps: 0.723207\n",
      "  769833/3750000: episode: 1104, duration: 5.133s, episode steps: 679, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.014166, mae: 3.375762, mean_q: 4.063852, mean_eps: 0.722984\n",
      "  770566/3750000: episode: 1105, duration: 5.651s, episode steps: 733, steps per second: 130, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.020192, mae: 3.382075, mean_q: 4.071720, mean_eps: 0.722728\n",
      "  771716/3750000: episode: 1106, duration: 8.650s, episode steps: 1150, steps per second: 133, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.019138, mae: 3.393481, mean_q: 4.085451, mean_eps: 0.722390\n",
      "  772359/3750000: episode: 1107, duration: 4.843s, episode steps: 643, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.017179, mae: 3.431494, mean_q: 4.126122, mean_eps: 0.722069\n",
      "  772791/3750000: episode: 1108, duration: 3.282s, episode steps: 432, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.018157, mae: 3.459407, mean_q: 4.161155, mean_eps: 0.721875\n",
      "  773553/3750000: episode: 1109, duration: 5.754s, episode steps: 762, steps per second: 132, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.019034, mae: 3.413348, mean_q: 4.112505, mean_eps: 0.721659\n",
      "  774696/3750000: episode: 1110, duration: 8.523s, episode steps: 1143, steps per second: 134, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.022098, mae: 3.404006, mean_q: 4.093886, mean_eps: 0.721317\n",
      "  775640/3750000: episode: 1111, duration: 7.199s, episode steps: 944, steps per second: 131, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.022465, mae: 3.414821, mean_q: 4.106986, mean_eps: 0.720942\n",
      "  776252/3750000: episode: 1112, duration: 4.616s, episode steps: 612, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.015348, mae: 3.457587, mean_q: 4.171536, mean_eps: 0.720662\n",
      "  777311/3750000: episode: 1113, duration: 7.976s, episode steps: 1059, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.022931, mae: 3.466715, mean_q: 4.173561, mean_eps: 0.720359\n",
      "  778023/3750000: episode: 1114, duration: 5.386s, episode steps: 712, steps per second: 132, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.013095, mae: 3.449266, mean_q: 4.151509, mean_eps: 0.720039\n",
      "  778547/3750000: episode: 1115, duration: 3.917s, episode steps: 524, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.015548, mae: 3.481641, mean_q: 4.192960, mean_eps: 0.719816\n",
      "  779448/3750000: episode: 1116, duration: 6.755s, episode steps: 901, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.019197, mae: 3.469451, mean_q: 4.181095, mean_eps: 0.719560\n",
      "  780250/3750000: episode: 1117, duration: 6.069s, episode steps: 802, steps per second: 132, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.012451, mae: 3.411737, mean_q: 4.104642, mean_eps: 0.719254\n",
      "  780776/3750000: episode: 1118, duration: 3.917s, episode steps: 526, steps per second: 134, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.015496, mae: 3.471379, mean_q: 4.172248, mean_eps: 0.719016\n",
      "  781351/3750000: episode: 1119, duration: 4.367s, episode steps: 575, steps per second: 132, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.023802, mae: 3.460461, mean_q: 4.158343, mean_eps: 0.718818\n",
      "  782406/3750000: episode: 1120, duration: 8.035s, episode steps: 1055, steps per second: 131, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.022319, mae: 3.446375, mean_q: 4.143355, mean_eps: 0.718523\n",
      "  782989/3750000: episode: 1121, duration: 4.330s, episode steps: 583, steps per second: 135, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.014948, mae: 3.463239, mean_q: 4.172276, mean_eps: 0.718228\n",
      "  784075/3750000: episode: 1122, duration: 8.119s, episode steps: 1086, steps per second: 134, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.017526, mae: 3.436642, mean_q: 4.137934, mean_eps: 0.717929\n",
      "  784778/3750000: episode: 1123, duration: 5.233s, episode steps: 703, steps per second: 134, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.020883, mae: 3.442213, mean_q: 4.135651, mean_eps: 0.717609\n",
      "  785327/3750000: episode: 1124, duration: 4.180s, episode steps: 549, steps per second: 131, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.017265, mae: 3.497945, mean_q: 4.209392, mean_eps: 0.717382\n",
      "  786442/3750000: episode: 1125, duration: 8.511s, episode steps: 1115, steps per second: 131, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.016186, mae: 3.447882, mean_q: 4.146257, mean_eps: 0.717080\n",
      "  787428/3750000: episode: 1126, duration: 7.292s, episode steps: 986, steps per second: 135, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.013617, mae: 3.422927, mean_q: 4.122957, mean_eps: 0.716702\n",
      "  788710/3750000: episode: 1127, duration: 9.587s, episode steps: 1282, steps per second: 134, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.017269, mae: 3.479035, mean_q: 4.197270, mean_eps: 0.716295\n",
      "  789202/3750000: episode: 1128, duration: 3.718s, episode steps: 492, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.016270, mae: 3.496205, mean_q: 4.205579, mean_eps: 0.715974\n",
      "  790066/3750000: episode: 1129, duration: 6.482s, episode steps: 864, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.019021, mae: 3.484107, mean_q: 4.198822, mean_eps: 0.715730\n",
      "  790918/3750000: episode: 1130, duration: 6.309s, episode steps: 852, steps per second: 135, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.025069, mae: 3.511905, mean_q: 4.236534, mean_eps: 0.715424\n",
      "  791574/3750000: episode: 1131, duration: 4.920s, episode steps: 656, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.014704, mae: 3.554069, mean_q: 4.280178, mean_eps: 0.715154\n",
      "  792079/3750000: episode: 1132, duration: 3.791s, episode steps: 505, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.022458, mae: 3.515745, mean_q: 4.229377, mean_eps: 0.714945\n",
      "  792697/3750000: episode: 1133, duration: 4.641s, episode steps: 618, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.016350, mae: 3.492436, mean_q: 4.206632, mean_eps: 0.714743\n",
      "  793195/3750000: episode: 1134, duration: 3.718s, episode steps: 498, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.017755, mae: 3.514940, mean_q: 4.237451, mean_eps: 0.714542\n",
      "  794128/3750000: episode: 1135, duration: 7.025s, episode steps: 933, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.023734, mae: 3.575601, mean_q: 4.310189, mean_eps: 0.714282\n",
      "  794834/3750000: episode: 1136, duration: 5.242s, episode steps: 706, steps per second: 135, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.019011, mae: 3.592854, mean_q: 4.331065, mean_eps: 0.713987\n",
      "  795463/3750000: episode: 1137, duration: 4.755s, episode steps: 629, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.019246, mae: 3.551419, mean_q: 4.272364, mean_eps: 0.713746\n",
      "  796535/3750000: episode: 1138, duration: 8.000s, episode steps: 1072, steps per second: 134, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.025612, mae: 3.543002, mean_q: 4.260282, mean_eps: 0.713440\n",
      "  797552/3750000: episode: 1139, duration: 7.626s, episode steps: 1017, steps per second: 133, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.022808, mae: 3.596501, mean_q: 4.331673, mean_eps: 0.713066\n",
      "  798433/3750000: episode: 1140, duration: 6.610s, episode steps: 881, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.028545, mae: 3.570440, mean_q: 4.298679, mean_eps: 0.712724\n",
      "  799524/3750000: episode: 1141, duration: 8.181s, episode steps: 1091, steps per second: 133, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.022763, mae: 3.605126, mean_q: 4.338222, mean_eps: 0.712367\n",
      "  799940/3750000: episode: 1142, duration: 3.129s, episode steps: 416, steps per second: 133, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.021755, mae: 3.570401, mean_q: 4.313527, mean_eps: 0.712097\n",
      "  800473/3750000: episode: 1143, duration: 4.056s, episode steps: 533, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.017812, mae: 3.589677, mean_q: 4.330392, mean_eps: 0.711928\n",
      "  801257/3750000: episode: 1144, duration: 5.850s, episode steps: 784, steps per second: 134, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.022803, mae: 3.632109, mean_q: 4.371170, mean_eps: 0.711690\n",
      "  801930/3750000: episode: 1145, duration: 5.137s, episode steps: 673, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.020296, mae: 3.618559, mean_q: 4.351718, mean_eps: 0.711428\n",
      "  802804/3750000: episode: 1146, duration: 6.628s, episode steps: 874, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.021142, mae: 3.661092, mean_q: 4.405655, mean_eps: 0.711147\n",
      "  803443/3750000: episode: 1147, duration: 4.772s, episode steps: 639, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.022042, mae: 3.640088, mean_q: 4.374727, mean_eps: 0.710873\n",
      "  803990/3750000: episode: 1148, duration: 4.059s, episode steps: 547, steps per second: 135, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.021188, mae: 3.672356, mean_q: 4.417863, mean_eps: 0.710661\n",
      "  804560/3750000: episode: 1149, duration: 4.333s, episode steps: 570, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.017304, mae: 3.623734, mean_q: 4.356901, mean_eps: 0.710463\n",
      "  805117/3750000: episode: 1150, duration: 4.234s, episode steps: 557, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.024947, mae: 3.587966, mean_q: 4.313707, mean_eps: 0.710261\n",
      "  806010/3750000: episode: 1151, duration: 6.702s, episode steps: 893, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.023923, mae: 3.563495, mean_q: 4.288330, mean_eps: 0.709998\n",
      "  806689/3750000: episode: 1152, duration: 5.057s, episode steps: 679, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.016491, mae: 3.632152, mean_q: 4.373846, mean_eps: 0.709714\n",
      "  807261/3750000: episode: 1153, duration: 4.367s, episode steps: 572, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.020339, mae: 3.597850, mean_q: 4.329531, mean_eps: 0.709487\n",
      "  808315/3750000: episode: 1154, duration: 7.845s, episode steps: 1054, steps per second: 134, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.018590, mae: 3.622559, mean_q: 4.361586, mean_eps: 0.709196\n",
      "  809252/3750000: episode: 1155, duration: 7.094s, episode steps: 937, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.020345, mae: 3.643254, mean_q: 4.381365, mean_eps: 0.708839\n",
      "  809887/3750000: episode: 1156, duration: 4.769s, episode steps: 635, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.017390, mae: 3.653473, mean_q: 4.394229, mean_eps: 0.708555\n",
      "  810512/3750000: episode: 1157, duration: 4.707s, episode steps: 625, steps per second: 133, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.028987, mae: 3.634075, mean_q: 4.372041, mean_eps: 0.708328\n",
      "  811135/3750000: episode: 1158, duration: 4.694s, episode steps: 623, steps per second: 133, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.017842, mae: 3.642622, mean_q: 4.376875, mean_eps: 0.708105\n",
      "  811853/3750000: episode: 1159, duration: 5.415s, episode steps: 718, steps per second: 133, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.019587, mae: 3.567507, mean_q: 4.291174, mean_eps: 0.707864\n",
      "  812731/3750000: episode: 1160, duration: 6.579s, episode steps: 878, steps per second: 133, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.017189, mae: 3.585321, mean_q: 4.318569, mean_eps: 0.707576\n",
      "  813780/3750000: episode: 1161, duration: 7.920s, episode steps: 1049, steps per second: 132, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.024322, mae: 3.553291, mean_q: 4.275778, mean_eps: 0.707230\n",
      "  814184/3750000: episode: 1162, duration: 3.100s, episode steps: 404, steps per second: 130, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.022357, mae: 3.646654, mean_q: 4.385500, mean_eps: 0.706967\n",
      "  815180/3750000: episode: 1163, duration: 7.420s, episode steps: 996, steps per second: 134, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.017402, mae: 3.552092, mean_q: 4.280423, mean_eps: 0.706715\n",
      "  815865/3750000: episode: 1164, duration: 5.267s, episode steps: 685, steps per second: 130, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.014647, mae: 3.486218, mean_q: 4.205082, mean_eps: 0.706413\n",
      "  816284/3750000: episode: 1165, duration: 3.152s, episode steps: 419, steps per second: 133, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.024329, mae: 3.463483, mean_q: 4.170150, mean_eps: 0.706211\n",
      "  816976/3750000: episode: 1166, duration: 5.111s, episode steps: 692, steps per second: 135, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.017664, mae: 3.518723, mean_q: 4.235984, mean_eps: 0.706013\n",
      "  817546/3750000: episode: 1167, duration: 4.424s, episode steps: 570, steps per second: 129, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.017939, mae: 3.542808, mean_q: 4.270485, mean_eps: 0.705786\n",
      "  818340/3750000: episode: 1168, duration: 6.054s, episode steps: 794, steps per second: 131, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.022703, mae: 3.566153, mean_q: 4.288830, mean_eps: 0.705542\n",
      "  818721/3750000: episode: 1169, duration: 2.969s, episode steps: 381, steps per second: 128, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.023672, mae: 3.545439, mean_q: 4.263697, mean_eps: 0.705329\n",
      "  819538/3750000: episode: 1170, duration: 6.120s, episode steps: 817, steps per second: 134, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016469, mae: 3.600261, mean_q: 4.330396, mean_eps: 0.705113\n",
      "  820032/3750000: episode: 1171, duration: 3.775s, episode steps: 494, steps per second: 131, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.015294, mae: 3.525024, mean_q: 4.236652, mean_eps: 0.704879\n",
      "  820932/3750000: episode: 1172, duration: 6.715s, episode steps: 900, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.017848, mae: 3.548205, mean_q: 4.265747, mean_eps: 0.704627\n",
      "  821576/3750000: episode: 1173, duration: 4.802s, episode steps: 644, steps per second: 134, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.019946, mae: 3.567337, mean_q: 4.289153, mean_eps: 0.704350\n",
      "  822325/3750000: episode: 1174, duration: 5.713s, episode steps: 749, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.148 [0.000, 5.000],  loss: 0.015861, mae: 3.542176, mean_q: 4.262730, mean_eps: 0.704098\n",
      "  823473/3750000: episode: 1175, duration: 8.523s, episode steps: 1148, steps per second: 135, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.025809, mae: 3.569037, mean_q: 4.291706, mean_eps: 0.703756\n",
      "  824102/3750000: episode: 1176, duration: 4.811s, episode steps: 629, steps per second: 131, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.023484, mae: 3.596368, mean_q: 4.330186, mean_eps: 0.703436\n",
      "  824799/3750000: episode: 1177, duration: 5.150s, episode steps: 697, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.018066, mae: 3.562038, mean_q: 4.286921, mean_eps: 0.703198\n",
      "  826100/3750000: episode: 1178, duration: 9.905s, episode steps: 1301, steps per second: 131, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.023203, mae: 3.575606, mean_q: 4.303476, mean_eps: 0.702842\n",
      "  826628/3750000: episode: 1179, duration: 3.996s, episode steps: 528, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.016916, mae: 3.577235, mean_q: 4.303942, mean_eps: 0.702510\n",
      "  827161/3750000: episode: 1180, duration: 4.017s, episode steps: 533, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.025747, mae: 3.602908, mean_q: 4.341386, mean_eps: 0.702316\n",
      "  827937/3750000: episode: 1181, duration: 5.725s, episode steps: 776, steps per second: 136, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.029709, mae: 3.567236, mean_q: 4.298735, mean_eps: 0.702082\n",
      "  828673/3750000: episode: 1182, duration: 5.584s, episode steps: 736, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.023492, mae: 3.551926, mean_q: 4.273637, mean_eps: 0.701812\n",
      "  829587/3750000: episode: 1183, duration: 6.884s, episode steps: 914, steps per second: 133, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.021850, mae: 3.527364, mean_q: 4.247430, mean_eps: 0.701513\n",
      "  830215/3750000: episode: 1184, duration: 4.694s, episode steps: 628, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.021802, mae: 3.528873, mean_q: 4.243546, mean_eps: 0.701236\n",
      "  830858/3750000: episode: 1185, duration: 4.833s, episode steps: 643, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.021208, mae: 3.495405, mean_q: 4.204607, mean_eps: 0.701009\n",
      "  831518/3750000: episode: 1186, duration: 4.963s, episode steps: 660, steps per second: 133, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.024228, mae: 3.569252, mean_q: 4.291165, mean_eps: 0.700775\n",
      "  832457/3750000: episode: 1187, duration: 7.053s, episode steps: 939, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.021979, mae: 3.525202, mean_q: 4.245052, mean_eps: 0.700487\n",
      "  832917/3750000: episode: 1188, duration: 3.453s, episode steps: 460, steps per second: 133, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.022404, mae: 3.531081, mean_q: 4.247105, mean_eps: 0.700235\n",
      "  833711/3750000: episode: 1189, duration: 6.049s, episode steps: 794, steps per second: 131, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.019758, mae: 3.503351, mean_q: 4.216144, mean_eps: 0.700008\n",
      "  834421/3750000: episode: 1190, duration: 5.428s, episode steps: 710, steps per second: 131, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.016020, mae: 3.572118, mean_q: 4.298647, mean_eps: 0.699735\n",
      "  834813/3750000: episode: 1191, duration: 2.910s, episode steps: 392, steps per second: 135, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.016376, mae: 3.480059, mean_q: 4.195002, mean_eps: 0.699537\n",
      "  836174/3750000: episode: 1192, duration: 10.234s, episode steps: 1361, steps per second: 133, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.016728, mae: 3.558764, mean_q: 4.282541, mean_eps: 0.699224\n",
      "  836554/3750000: episode: 1193, duration: 2.858s, episode steps: 380, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.025253, mae: 3.540286, mean_q: 4.259437, mean_eps: 0.698910\n",
      "  837115/3750000: episode: 1194, duration: 4.259s, episode steps: 561, steps per second: 132, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.020213, mae: 3.509728, mean_q: 4.221446, mean_eps: 0.698741\n",
      "  837805/3750000: episode: 1195, duration: 5.231s, episode steps: 690, steps per second: 132, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.021341, mae: 3.554105, mean_q: 4.279598, mean_eps: 0.698514\n",
      "  838646/3750000: episode: 1196, duration: 6.283s, episode steps: 841, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.019052, mae: 3.565001, mean_q: 4.290737, mean_eps: 0.698237\n",
      "  839286/3750000: episode: 1197, duration: 4.862s, episode steps: 640, steps per second: 132, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017084, mae: 3.609551, mean_q: 4.338627, mean_eps: 0.697971\n",
      "  839936/3750000: episode: 1198, duration: 4.824s, episode steps: 650, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.017480, mae: 3.563717, mean_q: 4.291757, mean_eps: 0.697740\n",
      "  840404/3750000: episode: 1199, duration: 3.589s, episode steps: 468, steps per second: 130, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.024087, mae: 3.601381, mean_q: 4.336168, mean_eps: 0.697539\n",
      "  841232/3750000: episode: 1200, duration: 6.213s, episode steps: 828, steps per second: 133, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.017969, mae: 3.598877, mean_q: 4.346504, mean_eps: 0.697305\n",
      "  841652/3750000: episode: 1201, duration: 3.175s, episode steps: 420, steps per second: 132, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.011651, mae: 3.654536, mean_q: 4.404538, mean_eps: 0.697082\n",
      "  842513/3750000: episode: 1202, duration: 6.460s, episode steps: 861, steps per second: 133, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.017284, mae: 3.567979, mean_q: 4.297573, mean_eps: 0.696851\n",
      "  843435/3750000: episode: 1203, duration: 6.971s, episode steps: 922, steps per second: 132, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.019988, mae: 3.609702, mean_q: 4.339818, mean_eps: 0.696531\n",
      "  844140/3750000: episode: 1204, duration: 5.364s, episode steps: 705, steps per second: 131, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.021969, mae: 3.621912, mean_q: 4.359215, mean_eps: 0.696239\n",
      "  844917/3750000: episode: 1205, duration: 5.834s, episode steps: 777, steps per second: 133, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.022264, mae: 3.606033, mean_q: 4.339352, mean_eps: 0.695973\n",
      "  845678/3750000: episode: 1206, duration: 5.752s, episode steps: 761, steps per second: 132, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.020990, mae: 3.633266, mean_q: 4.375524, mean_eps: 0.695696\n",
      "  846184/3750000: episode: 1207, duration: 3.862s, episode steps: 506, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.020224, mae: 3.667697, mean_q: 4.412742, mean_eps: 0.695465\n",
      "  847339/3750000: episode: 1208, duration: 8.615s, episode steps: 1155, steps per second: 134, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.020597, mae: 3.586381, mean_q: 4.317701, mean_eps: 0.695166\n",
      "  847691/3750000: episode: 1209, duration: 2.670s, episode steps: 352, steps per second: 132, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.018814, mae: 3.568282, mean_q: 4.299194, mean_eps: 0.694896\n",
      "  848519/3750000: episode: 1210, duration: 6.208s, episode steps: 828, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014270, mae: 3.573194, mean_q: 4.304841, mean_eps: 0.694684\n",
      "  848998/3750000: episode: 1211, duration: 3.588s, episode steps: 479, steps per second: 134, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.016774, mae: 3.603936, mean_q: 4.334485, mean_eps: 0.694450\n",
      "  849655/3750000: episode: 1212, duration: 5.044s, episode steps: 657, steps per second: 130, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.020737, mae: 3.549818, mean_q: 4.270364, mean_eps: 0.694245\n",
      "  850195/3750000: episode: 1213, duration: 4.206s, episode steps: 540, steps per second: 128, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.021798, mae: 3.550704, mean_q: 4.271107, mean_eps: 0.694029\n",
      "  851198/3750000: episode: 1214, duration: 7.576s, episode steps: 1003, steps per second: 132, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.019554, mae: 3.527346, mean_q: 4.241496, mean_eps: 0.693752\n",
      "  852253/3750000: episode: 1215, duration: 8.032s, episode steps: 1055, steps per second: 131, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.019405, mae: 3.615961, mean_q: 4.349353, mean_eps: 0.693381\n",
      "  852794/3750000: episode: 1216, duration: 4.046s, episode steps: 541, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.014680, mae: 3.594175, mean_q: 4.323708, mean_eps: 0.693093\n",
      "  853744/3750000: episode: 1217, duration: 7.212s, episode steps: 950, steps per second: 132, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.021026, mae: 3.609208, mean_q: 4.339316, mean_eps: 0.692823\n",
      "  854365/3750000: episode: 1218, duration: 4.657s, episode steps: 621, steps per second: 133, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.020089, mae: 3.652928, mean_q: 4.394548, mean_eps: 0.692538\n",
      "  855229/3750000: episode: 1219, duration: 6.431s, episode steps: 864, steps per second: 134, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.025720, mae: 3.636718, mean_q: 4.373651, mean_eps: 0.692272\n",
      "  855664/3750000: episode: 1220, duration: 3.298s, episode steps: 435, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.019566, mae: 3.632624, mean_q: 4.367973, mean_eps: 0.692038\n",
      "  856125/3750000: episode: 1221, duration: 3.468s, episode steps: 461, steps per second: 133, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.137 [0.000, 5.000],  loss: 0.015933, mae: 3.587536, mean_q: 4.326534, mean_eps: 0.691876\n",
      "  856939/3750000: episode: 1222, duration: 6.045s, episode steps: 814, steps per second: 135, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.026869, mae: 3.618207, mean_q: 4.351849, mean_eps: 0.691649\n",
      "  857723/3750000: episode: 1223, duration: 5.926s, episode steps: 784, steps per second: 132, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.017852, mae: 3.684583, mean_q: 4.436614, mean_eps: 0.691361\n",
      "  858334/3750000: episode: 1224, duration: 4.560s, episode steps: 611, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.014877, mae: 3.618342, mean_q: 4.354481, mean_eps: 0.691109\n",
      "  858710/3750000: episode: 1225, duration: 2.852s, episode steps: 376, steps per second: 132, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.012542, mae: 3.699801, mean_q: 4.458999, mean_eps: 0.690933\n",
      "  859329/3750000: episode: 1226, duration: 4.642s, episode steps: 619, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.022662, mae: 3.662254, mean_q: 4.408724, mean_eps: 0.690753\n",
      "  859831/3750000: episode: 1227, duration: 3.737s, episode steps: 502, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.031411, mae: 3.637603, mean_q: 4.379367, mean_eps: 0.690551\n",
      "  860656/3750000: episode: 1228, duration: 6.230s, episode steps: 825, steps per second: 132, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.015865, mae: 3.681084, mean_q: 4.433107, mean_eps: 0.690314\n",
      "  861188/3750000: episode: 1229, duration: 4.090s, episode steps: 532, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.015030, mae: 3.696573, mean_q: 4.443414, mean_eps: 0.690069\n",
      "  862289/3750000: episode: 1230, duration: 8.266s, episode steps: 1101, steps per second: 133, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.023603, mae: 3.734616, mean_q: 4.492258, mean_eps: 0.689774\n",
      "  862774/3750000: episode: 1231, duration: 3.627s, episode steps: 485, steps per second: 134, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.016265, mae: 3.747633, mean_q: 4.517649, mean_eps: 0.689489\n",
      "  863455/3750000: episode: 1232, duration: 5.106s, episode steps: 681, steps per second: 133, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.019665, mae: 3.754213, mean_q: 4.515790, mean_eps: 0.689280\n",
      "  864203/3750000: episode: 1233, duration: 5.682s, episode steps: 748, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.019006, mae: 3.718088, mean_q: 4.473421, mean_eps: 0.689021\n",
      "  865246/3750000: episode: 1234, duration: 7.783s, episode steps: 1043, steps per second: 134, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.020809, mae: 3.688142, mean_q: 4.442472, mean_eps: 0.688697\n",
      "  866010/3750000: episode: 1235, duration: 5.706s, episode steps: 764, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.021302, mae: 3.679807, mean_q: 4.434257, mean_eps: 0.688373\n",
      "  866886/3750000: episode: 1236, duration: 6.624s, episode steps: 876, steps per second: 132, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.019937, mae: 3.758723, mean_q: 4.518473, mean_eps: 0.688078\n",
      "  867418/3750000: episode: 1237, duration: 3.927s, episode steps: 532, steps per second: 135, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.018129, mae: 3.636663, mean_q: 4.372378, mean_eps: 0.687826\n",
      "  868285/3750000: episode: 1238, duration: 6.550s, episode steps: 867, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.015281, mae: 3.679242, mean_q: 4.425767, mean_eps: 0.687574\n",
      "  868668/3750000: episode: 1239, duration: 2.903s, episode steps: 383, steps per second: 132, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.014877, mae: 3.703957, mean_q: 4.455577, mean_eps: 0.687347\n",
      "  869250/3750000: episode: 1240, duration: 4.360s, episode steps: 582, steps per second: 133, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.019065, mae: 3.669711, mean_q: 4.418737, mean_eps: 0.687174\n",
      "  869735/3750000: episode: 1241, duration: 3.652s, episode steps: 485, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.022375, mae: 3.694856, mean_q: 4.445483, mean_eps: 0.686984\n",
      "  870231/3750000: episode: 1242, duration: 3.756s, episode steps: 496, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.025878, mae: 3.661035, mean_q: 4.402215, mean_eps: 0.686807\n",
      "  870802/3750000: episode: 1243, duration: 4.371s, episode steps: 571, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.032416, mae: 3.736513, mean_q: 4.495983, mean_eps: 0.686613\n",
      "  871464/3750000: episode: 1244, duration: 4.961s, episode steps: 662, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.027301, mae: 3.757316, mean_q: 4.526912, mean_eps: 0.686390\n",
      "  871945/3750000: episode: 1245, duration: 3.586s, episode steps: 481, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.030935, mae: 3.773823, mean_q: 4.543640, mean_eps: 0.686184\n",
      "  872679/3750000: episode: 1246, duration: 5.439s, episode steps: 734, steps per second: 135, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.018936, mae: 3.685324, mean_q: 4.435648, mean_eps: 0.685968\n",
      "  873250/3750000: episode: 1247, duration: 4.346s, episode steps: 571, steps per second: 131, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.019106, mae: 3.714238, mean_q: 4.472525, mean_eps: 0.685734\n",
      "  873915/3750000: episode: 1248, duration: 4.952s, episode steps: 665, steps per second: 134, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.027975, mae: 3.703683, mean_q: 4.455598, mean_eps: 0.685511\n",
      "  874525/3750000: episode: 1249, duration: 4.594s, episode steps: 610, steps per second: 133, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.015238, mae: 3.785221, mean_q: 4.557674, mean_eps: 0.685281\n",
      "  875022/3750000: episode: 1250, duration: 3.809s, episode steps: 497, steps per second: 130, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.019032, mae: 3.736263, mean_q: 4.499323, mean_eps: 0.685079\n",
      "  875602/3750000: episode: 1251, duration: 4.353s, episode steps: 580, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.020468, mae: 3.714292, mean_q: 4.473855, mean_eps: 0.684885\n",
      "  875986/3750000: episode: 1252, duration: 2.851s, episode steps: 384, steps per second: 135, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.019681, mae: 3.708717, mean_q: 4.462557, mean_eps: 0.684712\n",
      "  876631/3750000: episode: 1253, duration: 4.827s, episode steps: 645, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.026959, mae: 3.748751, mean_q: 4.508096, mean_eps: 0.684528\n",
      "  877613/3750000: episode: 1254, duration: 7.372s, episode steps: 982, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013768, mae: 3.686444, mean_q: 4.439539, mean_eps: 0.684237\n",
      "  878143/3750000: episode: 1255, duration: 4.026s, episode steps: 530, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.027373, mae: 3.710261, mean_q: 4.469695, mean_eps: 0.683963\n",
      "  878726/3750000: episode: 1256, duration: 4.345s, episode steps: 583, steps per second: 134, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.016300, mae: 3.683674, mean_q: 4.438712, mean_eps: 0.683762\n",
      "  879219/3750000: episode: 1257, duration: 3.679s, episode steps: 493, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.023279, mae: 3.757862, mean_q: 4.520673, mean_eps: 0.683571\n",
      "  880255/3750000: episode: 1258, duration: 7.729s, episode steps: 1036, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.022189, mae: 3.718327, mean_q: 4.471378, mean_eps: 0.683297\n",
      "  881118/3750000: episode: 1259, duration: 6.435s, episode steps: 863, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.017862, mae: 3.666764, mean_q: 4.409391, mean_eps: 0.682955\n",
      "  882281/3750000: episode: 1260, duration: 8.849s, episode steps: 1163, steps per second: 131, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.024460, mae: 3.699304, mean_q: 4.446088, mean_eps: 0.682588\n",
      "  883257/3750000: episode: 1261, duration: 7.238s, episode steps: 976, steps per second: 135, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.020430, mae: 3.708429, mean_q: 4.459526, mean_eps: 0.682203\n",
      "  883756/3750000: episode: 1262, duration: 3.802s, episode steps: 499, steps per second: 131, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.015952, mae: 3.707625, mean_q: 4.462313, mean_eps: 0.681940\n",
      "  885018/3750000: episode: 1263, duration: 9.450s, episode steps: 1262, steps per second: 134, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.018250, mae: 3.746199, mean_q: 4.511367, mean_eps: 0.681623\n",
      "  886135/3750000: episode: 1264, duration: 8.415s, episode steps: 1117, steps per second: 133, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.018117, mae: 3.718637, mean_q: 4.477912, mean_eps: 0.681195\n",
      "  886804/3750000: episode: 1265, duration: 5.049s, episode steps: 669, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.026088, mae: 3.759121, mean_q: 4.526811, mean_eps: 0.680871\n",
      "  887849/3750000: episode: 1266, duration: 7.836s, episode steps: 1045, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.028559, mae: 3.715590, mean_q: 4.472016, mean_eps: 0.680561\n",
      "  888429/3750000: episode: 1267, duration: 4.375s, episode steps: 580, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.015168, mae: 3.776112, mean_q: 4.546569, mean_eps: 0.680270\n",
      "  889162/3750000: episode: 1268, duration: 5.515s, episode steps: 733, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.019158, mae: 3.733010, mean_q: 4.489617, mean_eps: 0.680032\n",
      "  890074/3750000: episode: 1269, duration: 6.833s, episode steps: 912, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.021792, mae: 3.786246, mean_q: 4.553604, mean_eps: 0.679737\n",
      "  891150/3750000: episode: 1270, duration: 8.124s, episode steps: 1076, steps per second: 132, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.019739, mae: 3.762546, mean_q: 4.530516, mean_eps: 0.679380\n",
      "  892037/3750000: episode: 1271, duration: 6.655s, episode steps: 887, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.024568, mae: 3.728013, mean_q: 4.490562, mean_eps: 0.679028\n",
      "  892788/3750000: episode: 1272, duration: 5.643s, episode steps: 751, steps per second: 133, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.014274, mae: 3.674288, mean_q: 4.419936, mean_eps: 0.678732\n",
      "  893656/3750000: episode: 1273, duration: 6.438s, episode steps: 868, steps per second: 135, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.021149, mae: 3.652682, mean_q: 4.392297, mean_eps: 0.678441\n",
      "  894578/3750000: episode: 1274, duration: 6.948s, episode steps: 922, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.026742, mae: 3.679805, mean_q: 4.425650, mean_eps: 0.678120\n",
      "  895490/3750000: episode: 1275, duration: 7.037s, episode steps: 912, steps per second: 130, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.016453, mae: 3.665449, mean_q: 4.412743, mean_eps: 0.677789\n",
      "  896084/3750000: episode: 1276, duration: 4.514s, episode steps: 594, steps per second: 132, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.014856, mae: 3.614584, mean_q: 4.354669, mean_eps: 0.677516\n",
      "  897029/3750000: episode: 1277, duration: 7.081s, episode steps: 945, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.024435, mae: 3.644432, mean_q: 4.381119, mean_eps: 0.677238\n",
      "  897631/3750000: episode: 1278, duration: 4.503s, episode steps: 602, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.020889, mae: 3.699347, mean_q: 4.448731, mean_eps: 0.676961\n",
      "  898707/3750000: episode: 1279, duration: 8.084s, episode steps: 1076, steps per second: 133, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.020584, mae: 3.681389, mean_q: 4.427519, mean_eps: 0.676659\n",
      "  899734/3750000: episode: 1280, duration: 7.796s, episode steps: 1027, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.022693, mae: 3.694616, mean_q: 4.444956, mean_eps: 0.676281\n",
      "  900489/3750000: episode: 1281, duration: 5.730s, episode steps: 755, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.021605, mae: 3.715467, mean_q: 4.474228, mean_eps: 0.675960\n",
      "  901467/3750000: episode: 1282, duration: 7.389s, episode steps: 978, steps per second: 132, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.016850, mae: 3.660285, mean_q: 4.400724, mean_eps: 0.675647\n",
      "  902201/3750000: episode: 1283, duration: 5.496s, episode steps: 734, steps per second: 134, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.019916, mae: 3.657849, mean_q: 4.402691, mean_eps: 0.675338\n",
      "  902996/3750000: episode: 1284, duration: 5.948s, episode steps: 795, steps per second: 134, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.021684, mae: 3.721202, mean_q: 4.482425, mean_eps: 0.675064\n",
      "  903695/3750000: episode: 1285, duration: 5.232s, episode steps: 699, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 0.016778, mae: 3.708312, mean_q: 4.462822, mean_eps: 0.674798\n",
      "  904327/3750000: episode: 1286, duration: 4.794s, episode steps: 632, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.015801, mae: 3.727179, mean_q: 4.485455, mean_eps: 0.674556\n",
      "  905435/3750000: episode: 1287, duration: 8.267s, episode steps: 1108, steps per second: 134, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.023936, mae: 3.710278, mean_q: 4.467250, mean_eps: 0.674243\n",
      "  906374/3750000: episode: 1288, duration: 7.000s, episode steps: 939, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.021586, mae: 3.707932, mean_q: 4.458110, mean_eps: 0.673876\n",
      "  907700/3750000: episode: 1289, duration: 10.013s, episode steps: 1326, steps per second: 132, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.016335, mae: 3.679807, mean_q: 4.430109, mean_eps: 0.673469\n",
      "  908173/3750000: episode: 1290, duration: 3.584s, episode steps: 473, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.216 [0.000, 5.000],  loss: 0.018796, mae: 3.700947, mean_q: 4.462541, mean_eps: 0.673145\n",
      "  909180/3750000: episode: 1291, duration: 7.622s, episode steps: 1007, steps per second: 132, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.016783, mae: 3.637660, mean_q: 4.379781, mean_eps: 0.672879\n",
      "  909567/3750000: episode: 1292, duration: 2.936s, episode steps: 387, steps per second: 132, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.016159, mae: 3.620633, mean_q: 4.357917, mean_eps: 0.672627\n",
      "  910516/3750000: episode: 1293, duration: 7.047s, episode steps: 949, steps per second: 135, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.017814, mae: 3.630140, mean_q: 4.368474, mean_eps: 0.672386\n",
      "  911212/3750000: episode: 1294, duration: 5.284s, episode steps: 696, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.016089, mae: 3.594028, mean_q: 4.324999, mean_eps: 0.672090\n",
      "  911809/3750000: episode: 1295, duration: 4.483s, episode steps: 597, steps per second: 133, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.025155, mae: 3.588247, mean_q: 4.319029, mean_eps: 0.671856\n",
      "  912657/3750000: episode: 1296, duration: 6.276s, episode steps: 848, steps per second: 135, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.021343, mae: 3.587399, mean_q: 4.319206, mean_eps: 0.671597\n",
      "  913485/3750000: episode: 1297, duration: 6.358s, episode steps: 828, steps per second: 130, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.021365, mae: 3.618243, mean_q: 4.358048, mean_eps: 0.671295\n",
      "  914002/3750000: episode: 1298, duration: 3.903s, episode steps: 517, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.023442, mae: 3.558372, mean_q: 4.282165, mean_eps: 0.671050\n",
      "  914656/3750000: episode: 1299, duration: 4.946s, episode steps: 654, steps per second: 132, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.013777, mae: 3.663760, mean_q: 4.408265, mean_eps: 0.670841\n",
      "  915360/3750000: episode: 1300, duration: 5.387s, episode steps: 704, steps per second: 131, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.018170, mae: 3.663553, mean_q: 4.407228, mean_eps: 0.670600\n",
      "  916020/3750000: episode: 1301, duration: 5.053s, episode steps: 660, steps per second: 131, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.842 [0.000, 5.000],  loss: 0.020102, mae: 3.673549, mean_q: 4.427509, mean_eps: 0.670355\n",
      "  917070/3750000: episode: 1302, duration: 7.931s, episode steps: 1050, steps per second: 132, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014849, mae: 3.639259, mean_q: 4.378918, mean_eps: 0.670046\n",
      "  917602/3750000: episode: 1303, duration: 4.085s, episode steps: 532, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.020229, mae: 3.681007, mean_q: 4.422264, mean_eps: 0.669758\n",
      "  918271/3750000: episode: 1304, duration: 4.971s, episode steps: 669, steps per second: 135, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.017831, mae: 3.634357, mean_q: 4.378878, mean_eps: 0.669542\n",
      "  918687/3750000: episode: 1305, duration: 3.129s, episode steps: 416, steps per second: 133, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.033338, mae: 3.608680, mean_q: 4.338930, mean_eps: 0.669347\n",
      "  919432/3750000: episode: 1306, duration: 5.518s, episode steps: 745, steps per second: 135, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.017157, mae: 3.599923, mean_q: 4.336660, mean_eps: 0.669138\n",
      "  920009/3750000: episode: 1307, duration: 4.384s, episode steps: 577, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.018691, mae: 3.704238, mean_q: 4.456677, mean_eps: 0.668901\n",
      "  920506/3750000: episode: 1308, duration: 3.734s, episode steps: 497, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.012672, mae: 3.670450, mean_q: 4.418431, mean_eps: 0.668706\n",
      "  920932/3750000: episode: 1309, duration: 3.164s, episode steps: 426, steps per second: 135, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.019472, mae: 3.690919, mean_q: 4.435631, mean_eps: 0.668541\n",
      "  921711/3750000: episode: 1310, duration: 5.883s, episode steps: 779, steps per second: 132, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.016183, mae: 3.712267, mean_q: 4.467281, mean_eps: 0.668325\n",
      "  922344/3750000: episode: 1311, duration: 4.782s, episode steps: 633, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.020855, mae: 3.608192, mean_q: 4.343638, mean_eps: 0.668069\n",
      "  923126/3750000: episode: 1312, duration: 5.863s, episode steps: 782, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.014595, mae: 3.640068, mean_q: 4.381567, mean_eps: 0.667814\n",
      "  923795/3750000: episode: 1313, duration: 4.997s, episode steps: 669, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015217, mae: 3.659206, mean_q: 4.408137, mean_eps: 0.667554\n",
      "  924448/3750000: episode: 1314, duration: 4.951s, episode steps: 653, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.022721, mae: 3.727725, mean_q: 4.489441, mean_eps: 0.667317\n",
      "  925436/3750000: episode: 1315, duration: 7.336s, episode steps: 988, steps per second: 135, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.018150, mae: 3.694865, mean_q: 4.449204, mean_eps: 0.667022\n",
      "  925965/3750000: episode: 1316, duration: 4.056s, episode steps: 529, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.022059, mae: 3.679243, mean_q: 4.430780, mean_eps: 0.666748\n",
      "  926467/3750000: episode: 1317, duration: 3.757s, episode steps: 502, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.016534, mae: 3.729736, mean_q: 4.493519, mean_eps: 0.666561\n",
      "  926997/3750000: episode: 1318, duration: 3.919s, episode steps: 530, steps per second: 135, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.022213, mae: 3.762477, mean_q: 4.520224, mean_eps: 0.666377\n",
      "  927514/3750000: episode: 1319, duration: 3.898s, episode steps: 517, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.020922, mae: 3.676526, mean_q: 4.423465, mean_eps: 0.666190\n",
      "  928151/3750000: episode: 1320, duration: 4.860s, episode steps: 637, steps per second: 131, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.024364, mae: 3.679423, mean_q: 4.433530, mean_eps: 0.665981\n",
      "  929193/3750000: episode: 1321, duration: 7.823s, episode steps: 1042, steps per second: 133, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.017934, mae: 3.667880, mean_q: 4.420167, mean_eps: 0.665679\n",
      "  929768/3750000: episode: 1322, duration: 4.376s, episode steps: 575, steps per second: 131, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.025144, mae: 3.746742, mean_q: 4.511007, mean_eps: 0.665387\n",
      "  930670/3750000: episode: 1323, duration: 6.815s, episode steps: 902, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.028111, mae: 3.699554, mean_q: 4.450048, mean_eps: 0.665121\n",
      "  931507/3750000: episode: 1324, duration: 6.279s, episode steps: 837, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.024506, mae: 3.729106, mean_q: 4.481652, mean_eps: 0.664808\n",
      "  932923/3750000: episode: 1325, duration: 10.593s, episode steps: 1416, steps per second: 134, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.022953, mae: 3.739965, mean_q: 4.504186, mean_eps: 0.664401\n",
      "  933883/3750000: episode: 1326, duration: 7.163s, episode steps: 960, steps per second: 134, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.024228, mae: 3.756927, mean_q: 4.522462, mean_eps: 0.663972\n",
      "  934727/3750000: episode: 1327, duration: 6.392s, episode steps: 844, steps per second: 132, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.025401, mae: 3.739005, mean_q: 4.510735, mean_eps: 0.663648\n",
      "  935130/3750000: episode: 1328, duration: 3.008s, episode steps: 403, steps per second: 134, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.016755, mae: 3.846307, mean_q: 4.635900, mean_eps: 0.663425\n",
      "  935898/3750000: episode: 1329, duration: 5.815s, episode steps: 768, steps per second: 132, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.021731, mae: 3.822933, mean_q: 4.604028, mean_eps: 0.663216\n",
      "  936531/3750000: episode: 1330, duration: 4.894s, episode steps: 633, steps per second: 129, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.019927, mae: 3.804559, mean_q: 4.583981, mean_eps: 0.662964\n",
      "  937568/3750000: episode: 1331, duration: 7.809s, episode steps: 1037, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.018633, mae: 3.775056, mean_q: 4.545742, mean_eps: 0.662662\n",
      "  937960/3750000: episode: 1332, duration: 2.974s, episode steps: 392, steps per second: 132, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.021146, mae: 3.823585, mean_q: 4.605458, mean_eps: 0.662406\n",
      "  938797/3750000: episode: 1333, duration: 6.332s, episode steps: 837, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.022375, mae: 3.787601, mean_q: 4.560618, mean_eps: 0.662187\n",
      "  939410/3750000: episode: 1334, duration: 4.660s, episode steps: 613, steps per second: 132, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.014054, mae: 3.749099, mean_q: 4.514619, mean_eps: 0.661924\n",
      "  939791/3750000: episode: 1335, duration: 2.855s, episode steps: 381, steps per second: 133, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.016750, mae: 3.681476, mean_q: 4.431011, mean_eps: 0.661744\n",
      "  940820/3750000: episode: 1336, duration: 7.791s, episode steps: 1029, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.018638, mae: 3.747429, mean_q: 4.512287, mean_eps: 0.661492\n",
      "  941957/3750000: episode: 1337, duration: 8.536s, episode steps: 1137, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.015876, mae: 3.720928, mean_q: 4.477816, mean_eps: 0.661103\n",
      "  942581/3750000: episode: 1338, duration: 4.766s, episode steps: 624, steps per second: 131, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.021521, mae: 3.759021, mean_q: 4.533841, mean_eps: 0.660783\n",
      "  943974/3750000: episode: 1339, duration: 10.404s, episode steps: 1393, steps per second: 134, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015785, mae: 3.723799, mean_q: 4.481028, mean_eps: 0.660419\n",
      "  944455/3750000: episode: 1340, duration: 3.601s, episode steps: 481, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.016834, mae: 3.699378, mean_q: 4.456996, mean_eps: 0.660084\n",
      "  945067/3750000: episode: 1341, duration: 4.752s, episode steps: 612, steps per second: 129, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.021777, mae: 3.709069, mean_q: 4.465435, mean_eps: 0.659886\n",
      "  945851/3750000: episode: 1342, duration: 5.873s, episode steps: 784, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.016266, mae: 3.679759, mean_q: 4.431854, mean_eps: 0.659634\n",
      "  946384/3750000: episode: 1343, duration: 4.022s, episode steps: 533, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.024540, mae: 3.710684, mean_q: 4.478960, mean_eps: 0.659397\n",
      "  946899/3750000: episode: 1344, duration: 3.810s, episode steps: 515, steps per second: 135, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.023293, mae: 3.663734, mean_q: 4.415475, mean_eps: 0.659210\n",
      "  947690/3750000: episode: 1345, duration: 6.054s, episode steps: 791, steps per second: 131, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.024909, mae: 3.681294, mean_q: 4.428614, mean_eps: 0.658976\n",
      "  948903/3750000: episode: 1346, duration: 9.099s, episode steps: 1213, steps per second: 133, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.017546, mae: 3.662076, mean_q: 4.406042, mean_eps: 0.658612\n",
      "  949990/3750000: episode: 1347, duration: 8.131s, episode steps: 1087, steps per second: 134, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.019581, mae: 3.674264, mean_q: 4.423654, mean_eps: 0.658198\n",
      "  951095/3750000: episode: 1348, duration: 8.226s, episode steps: 1105, steps per second: 134, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.017748, mae: 3.637654, mean_q: 4.377618, mean_eps: 0.657806\n",
      "  951648/3750000: episode: 1349, duration: 4.261s, episode steps: 553, steps per second: 130, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.029336, mae: 3.609322, mean_q: 4.341023, mean_eps: 0.657507\n",
      "  952429/3750000: episode: 1350, duration: 5.848s, episode steps: 781, steps per second: 134, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.020165, mae: 3.615990, mean_q: 4.347043, mean_eps: 0.657266\n",
      "  953348/3750000: episode: 1351, duration: 6.897s, episode steps: 919, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.020058, mae: 3.626382, mean_q: 4.361112, mean_eps: 0.656960\n",
      "  954059/3750000: episode: 1352, duration: 5.345s, episode steps: 711, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.017269, mae: 3.619752, mean_q: 4.357805, mean_eps: 0.656668\n",
      "  954774/3750000: episode: 1353, duration: 5.357s, episode steps: 715, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.016826, mae: 3.613841, mean_q: 4.360495, mean_eps: 0.656412\n",
      "  955328/3750000: episode: 1354, duration: 4.167s, episode steps: 554, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.017192, mae: 3.661182, mean_q: 4.414280, mean_eps: 0.656182\n",
      "  955886/3750000: episode: 1355, duration: 4.255s, episode steps: 558, steps per second: 131, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.014647, mae: 3.662891, mean_q: 4.408771, mean_eps: 0.655980\n",
      "  957142/3750000: episode: 1356, duration: 9.406s, episode steps: 1256, steps per second: 134, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.018811, mae: 3.604499, mean_q: 4.338406, mean_eps: 0.655653\n",
      "  957797/3750000: episode: 1357, duration: 4.847s, episode steps: 655, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.019910, mae: 3.628317, mean_q: 4.364642, mean_eps: 0.655311\n",
      "  958900/3750000: episode: 1358, duration: 8.370s, episode steps: 1103, steps per second: 132, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.021657, mae: 3.652302, mean_q: 4.392331, mean_eps: 0.654998\n",
      "  959588/3750000: episode: 1359, duration: 5.203s, episode steps: 688, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.014937, mae: 3.694707, mean_q: 4.441815, mean_eps: 0.654674\n",
      "  960224/3750000: episode: 1360, duration: 4.870s, episode steps: 636, steps per second: 131, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.024627, mae: 3.630399, mean_q: 4.376422, mean_eps: 0.654432\n",
      "  961160/3750000: episode: 1361, duration: 7.072s, episode steps: 936, steps per second: 132, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.018553, mae: 3.640541, mean_q: 4.381544, mean_eps: 0.654152\n",
      "  961584/3750000: episode: 1362, duration: 3.267s, episode steps: 424, steps per second: 130, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.021920, mae: 3.623167, mean_q: 4.367556, mean_eps: 0.653907\n",
      "  962148/3750000: episode: 1363, duration: 4.288s, episode steps: 564, steps per second: 132, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.015270, mae: 3.623002, mean_q: 4.357877, mean_eps: 0.653727\n",
      "  962802/3750000: episode: 1364, duration: 4.956s, episode steps: 654, steps per second: 132, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.018315, mae: 3.585007, mean_q: 4.317428, mean_eps: 0.653507\n",
      "  963158/3750000: episode: 1365, duration: 2.600s, episode steps: 356, steps per second: 137, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.026788, mae: 3.653647, mean_q: 4.403249, mean_eps: 0.653327\n",
      "  963943/3750000: episode: 1366, duration: 5.918s, episode steps: 785, steps per second: 133, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.016130, mae: 3.591050, mean_q: 4.322957, mean_eps: 0.653122\n",
      "  964980/3750000: episode: 1367, duration: 7.826s, episode steps: 1037, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.023043, mae: 3.618421, mean_q: 4.350141, mean_eps: 0.652794\n",
      "  965674/3750000: episode: 1368, duration: 5.265s, episode steps: 694, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.016612, mae: 3.578686, mean_q: 4.305737, mean_eps: 0.652485\n",
      "  966134/3750000: episode: 1369, duration: 3.478s, episode steps: 460, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.012640, mae: 3.542557, mean_q: 4.274253, mean_eps: 0.652276\n",
      "  966571/3750000: episode: 1370, duration: 3.346s, episode steps: 437, steps per second: 131, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013816, mae: 3.578891, mean_q: 4.303495, mean_eps: 0.652114\n",
      "  967244/3750000: episode: 1371, duration: 5.058s, episode steps: 673, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.017623, mae: 3.606741, mean_q: 4.342175, mean_eps: 0.651912\n",
      "  968950/3750000: episode: 1372, duration: 12.720s, episode steps: 1706, steps per second: 134, episode reward: 45.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.017019, mae: 3.596878, mean_q: 4.327422, mean_eps: 0.651484\n",
      "  969420/3750000: episode: 1373, duration: 3.566s, episode steps: 470, steps per second: 132, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.014286, mae: 3.522857, mean_q: 4.238148, mean_eps: 0.651095\n",
      "  969930/3750000: episode: 1374, duration: 3.870s, episode steps: 510, steps per second: 132, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.018255, mae: 3.510701, mean_q: 4.221009, mean_eps: 0.650919\n",
      "  970803/3750000: episode: 1375, duration: 6.606s, episode steps: 873, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.019013, mae: 3.599577, mean_q: 4.334216, mean_eps: 0.650667\n",
      "  971490/3750000: episode: 1376, duration: 5.107s, episode steps: 687, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.020683, mae: 3.595529, mean_q: 4.330122, mean_eps: 0.650386\n",
      "  971880/3750000: episode: 1377, duration: 2.971s, episode steps: 390, steps per second: 131, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.015709, mae: 3.601838, mean_q: 4.341678, mean_eps: 0.650195\n",
      "  972630/3750000: episode: 1378, duration: 5.671s, episode steps: 750, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.024691, mae: 3.547280, mean_q: 4.268481, mean_eps: 0.649990\n",
      "  973607/3750000: episode: 1379, duration: 7.345s, episode steps: 977, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.019356, mae: 3.614620, mean_q: 4.348017, mean_eps: 0.649677\n",
      "  974124/3750000: episode: 1380, duration: 3.890s, episode steps: 517, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.020661, mae: 3.535389, mean_q: 4.253951, mean_eps: 0.649407\n",
      "  975231/3750000: episode: 1381, duration: 8.301s, episode steps: 1107, steps per second: 133, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.015625, mae: 3.561539, mean_q: 4.285562, mean_eps: 0.649115\n",
      "  975731/3750000: episode: 1382, duration: 3.740s, episode steps: 500, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.021088, mae: 3.527017, mean_q: 4.245271, mean_eps: 0.648827\n",
      "  976109/3750000: episode: 1383, duration: 2.870s, episode steps: 378, steps per second: 132, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.023241, mae: 3.604743, mean_q: 4.336179, mean_eps: 0.648669\n",
      "  977187/3750000: episode: 1384, duration: 8.225s, episode steps: 1078, steps per second: 131, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.018892, mae: 3.551963, mean_q: 4.272005, mean_eps: 0.648406\n",
      "  978256/3750000: episode: 1385, duration: 7.986s, episode steps: 1069, steps per second: 134, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.023345, mae: 3.553949, mean_q: 4.274549, mean_eps: 0.648021\n",
      "  978912/3750000: episode: 1386, duration: 4.936s, episode steps: 656, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.016736, mae: 3.540787, mean_q: 4.258433, mean_eps: 0.647711\n",
      "  979421/3750000: episode: 1387, duration: 3.950s, episode steps: 509, steps per second: 129, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.019376, mae: 3.569040, mean_q: 4.297697, mean_eps: 0.647499\n",
      "  980471/3750000: episode: 1388, duration: 7.825s, episode steps: 1050, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.026476, mae: 3.559770, mean_q: 4.286522, mean_eps: 0.647218\n",
      "  981155/3750000: episode: 1389, duration: 5.118s, episode steps: 684, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.020015, mae: 3.603369, mean_q: 4.336261, mean_eps: 0.646908\n",
      "  981722/3750000: episode: 1390, duration: 4.309s, episode steps: 567, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.023485, mae: 3.625550, mean_q: 4.362658, mean_eps: 0.646682\n",
      "  982658/3750000: episode: 1391, duration: 6.906s, episode steps: 936, steps per second: 136, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.017939, mae: 3.601278, mean_q: 4.330862, mean_eps: 0.646412\n",
      "  983760/3750000: episode: 1392, duration: 8.347s, episode steps: 1102, steps per second: 132, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.019162, mae: 3.605720, mean_q: 4.340477, mean_eps: 0.646048\n",
      "  984601/3750000: episode: 1393, duration: 6.412s, episode steps: 841, steps per second: 131, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.022445, mae: 3.572867, mean_q: 4.299641, mean_eps: 0.645695\n",
      "  985224/3750000: episode: 1394, duration: 4.654s, episode steps: 623, steps per second: 134, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.020223, mae: 3.584845, mean_q: 4.310050, mean_eps: 0.645429\n",
      "  985928/3750000: episode: 1395, duration: 5.324s, episode steps: 704, steps per second: 132, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.020695, mae: 3.600945, mean_q: 4.327455, mean_eps: 0.645191\n",
      "  986931/3750000: episode: 1396, duration: 7.487s, episode steps: 1003, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.020162, mae: 3.627009, mean_q: 4.360612, mean_eps: 0.644885\n",
      "  987730/3750000: episode: 1397, duration: 6.023s, episode steps: 799, steps per second: 133, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.021955, mae: 3.641783, mean_q: 4.384209, mean_eps: 0.644561\n",
      "  988889/3750000: episode: 1398, duration: 8.682s, episode steps: 1159, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.015606, mae: 3.677390, mean_q: 4.428044, mean_eps: 0.644208\n",
      "  989566/3750000: episode: 1399, duration: 5.077s, episode steps: 677, steps per second: 133, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.027757, mae: 3.632341, mean_q: 4.366508, mean_eps: 0.643877\n",
      "  990112/3750000: episode: 1400, duration: 4.111s, episode steps: 546, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.018051, mae: 3.670798, mean_q: 4.412092, mean_eps: 0.643658\n",
      "  990777/3750000: episode: 1401, duration: 4.945s, episode steps: 665, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.022682, mae: 3.670980, mean_q: 4.410932, mean_eps: 0.643442\n",
      "  991402/3750000: episode: 1402, duration: 4.779s, episode steps: 625, steps per second: 131, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.016774, mae: 3.619431, mean_q: 4.353292, mean_eps: 0.643208\n",
      "  992382/3750000: episode: 1403, duration: 7.370s, episode steps: 980, steps per second: 133, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.020595, mae: 3.623053, mean_q: 4.361379, mean_eps: 0.642916\n",
      "  993510/3750000: episode: 1404, duration: 8.464s, episode steps: 1128, steps per second: 133, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.019882, mae: 3.627942, mean_q: 4.368380, mean_eps: 0.642538\n",
      "  994414/3750000: episode: 1405, duration: 6.842s, episode steps: 904, steps per second: 132, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.021919, mae: 3.676200, mean_q: 4.418845, mean_eps: 0.642174\n",
      "  995431/3750000: episode: 1406, duration: 7.582s, episode steps: 1017, steps per second: 134, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.022168, mae: 3.623000, mean_q: 4.354188, mean_eps: 0.641829\n",
      "  996864/3750000: episode: 1407, duration: 10.779s, episode steps: 1433, steps per second: 133, episode reward: 43.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.025338, mae: 3.588594, mean_q: 4.314959, mean_eps: 0.641386\n",
      "  997349/3750000: episode: 1408, duration: 3.615s, episode steps: 485, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.014 [0.000, 5.000],  loss: 0.014326, mae: 3.579236, mean_q: 4.312283, mean_eps: 0.641040\n",
      "  997838/3750000: episode: 1409, duration: 3.640s, episode steps: 489, steps per second: 134, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.019640, mae: 3.591882, mean_q: 4.320110, mean_eps: 0.640868\n",
      "  999067/3750000: episode: 1410, duration: 9.287s, episode steps: 1229, steps per second: 132, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.018249, mae: 3.574163, mean_q: 4.301759, mean_eps: 0.640558\n",
      " 1000253/3750000: episode: 1411, duration: 8.907s, episode steps: 1186, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.019910, mae: 3.606628, mean_q: 4.347426, mean_eps: 0.640122\n",
      " 1001264/3750000: episode: 1412, duration: 7.689s, episode steps: 1011, steps per second: 131, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.136 [0.000, 5.000],  loss: 0.019645, mae: 3.565570, mean_q: 4.296648, mean_eps: 0.639726\n",
      " 1001696/3750000: episode: 1413, duration: 3.191s, episode steps: 432, steps per second: 135, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.012683, mae: 3.553621, mean_q: 4.279200, mean_eps: 0.639467\n",
      " 1002482/3750000: episode: 1414, duration: 6.006s, episode steps: 786, steps per second: 131, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.018092, mae: 3.556696, mean_q: 4.276159, mean_eps: 0.639248\n",
      " 1003018/3750000: episode: 1415, duration: 3.998s, episode steps: 536, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.014905, mae: 3.561386, mean_q: 4.279408, mean_eps: 0.639010\n",
      " 1003668/3750000: episode: 1416, duration: 4.924s, episode steps: 650, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.024479, mae: 3.626384, mean_q: 4.366355, mean_eps: 0.638798\n",
      " 1004296/3750000: episode: 1417, duration: 4.706s, episode steps: 628, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.019296, mae: 3.649308, mean_q: 4.391318, mean_eps: 0.638567\n",
      " 1005146/3750000: episode: 1418, duration: 6.472s, episode steps: 850, steps per second: 131, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.019405, mae: 3.630423, mean_q: 4.364411, mean_eps: 0.638301\n",
      " 1005826/3750000: episode: 1419, duration: 5.096s, episode steps: 680, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.018801, mae: 3.645486, mean_q: 4.379646, mean_eps: 0.638024\n",
      " 1006149/3750000: episode: 1420, duration: 2.417s, episode steps: 323, steps per second: 134, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.021877, mae: 3.686096, mean_q: 4.429321, mean_eps: 0.637844\n",
      " 1006782/3750000: episode: 1421, duration: 4.821s, episode steps: 633, steps per second: 131, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.017191, mae: 3.661427, mean_q: 4.407812, mean_eps: 0.637671\n",
      " 1007225/3750000: episode: 1422, duration: 3.297s, episode steps: 443, steps per second: 134, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.989 [0.000, 5.000],  loss: 0.016081, mae: 3.618798, mean_q: 4.359865, mean_eps: 0.637476\n",
      " 1007979/3750000: episode: 1423, duration: 5.603s, episode steps: 754, steps per second: 135, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.016593, mae: 3.618603, mean_q: 4.352560, mean_eps: 0.637264\n",
      " 1008530/3750000: episode: 1424, duration: 4.151s, episode steps: 551, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.019961, mae: 3.657763, mean_q: 4.393480, mean_eps: 0.637030\n",
      " 1009130/3750000: episode: 1425, duration: 4.642s, episode steps: 600, steps per second: 129, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.015610, mae: 3.679410, mean_q: 4.430890, mean_eps: 0.636821\n",
      " 1009650/3750000: episode: 1426, duration: 3.967s, episode steps: 520, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.020795, mae: 3.655041, mean_q: 4.404045, mean_eps: 0.636620\n",
      " 1010344/3750000: episode: 1427, duration: 5.259s, episode steps: 694, steps per second: 132, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.019587, mae: 3.684108, mean_q: 4.424346, mean_eps: 0.636400\n",
      " 1010954/3750000: episode: 1428, duration: 4.566s, episode steps: 610, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013591, mae: 3.621801, mean_q: 4.359247, mean_eps: 0.636166\n",
      " 1011484/3750000: episode: 1429, duration: 4.042s, episode steps: 530, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.016540, mae: 3.597720, mean_q: 4.329224, mean_eps: 0.635961\n",
      " 1012179/3750000: episode: 1430, duration: 5.148s, episode steps: 695, steps per second: 135, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.037269, mae: 3.639212, mean_q: 4.377135, mean_eps: 0.635741\n",
      " 1013165/3750000: episode: 1431, duration: 7.499s, episode steps: 986, steps per second: 131, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.027792, mae: 3.613335, mean_q: 4.343031, mean_eps: 0.635439\n",
      " 1013597/3750000: episode: 1432, duration: 3.175s, episode steps: 432, steps per second: 136, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.011514, mae: 3.636616, mean_q: 4.383634, mean_eps: 0.635183\n",
      " 1014101/3750000: episode: 1433, duration: 3.837s, episode steps: 504, steps per second: 131, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.017081, mae: 3.636288, mean_q: 4.377714, mean_eps: 0.635014\n",
      " 1014719/3750000: episode: 1434, duration: 4.557s, episode steps: 618, steps per second: 136, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.013 [0.000, 5.000],  loss: 0.014684, mae: 3.740554, mean_q: 4.498082, mean_eps: 0.634812\n",
      " 1015365/3750000: episode: 1435, duration: 4.944s, episode steps: 646, steps per second: 131, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.019299, mae: 3.704940, mean_q: 4.457293, mean_eps: 0.634586\n",
      " 1016178/3750000: episode: 1436, duration: 6.055s, episode steps: 813, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.015507, mae: 3.650933, mean_q: 4.394254, mean_eps: 0.634323\n",
      " 1017132/3750000: episode: 1437, duration: 7.207s, episode steps: 954, steps per second: 132, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.026654, mae: 3.647127, mean_q: 4.383426, mean_eps: 0.634006\n",
      " 1017621/3750000: episode: 1438, duration: 3.780s, episode steps: 489, steps per second: 129, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.010552, mae: 3.625826, mean_q: 4.360909, mean_eps: 0.633743\n",
      " 1018515/3750000: episode: 1439, duration: 6.649s, episode steps: 894, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.025728, mae: 3.663838, mean_q: 4.401980, mean_eps: 0.633495\n",
      " 1019102/3750000: episode: 1440, duration: 4.456s, episode steps: 587, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.019820, mae: 3.585254, mean_q: 4.310660, mean_eps: 0.633228\n",
      " 1019535/3750000: episode: 1441, duration: 3.306s, episode steps: 433, steps per second: 131, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.016852, mae: 3.679058, mean_q: 4.424446, mean_eps: 0.633045\n",
      " 1020075/3750000: episode: 1442, duration: 4.074s, episode steps: 540, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.015816, mae: 3.660089, mean_q: 4.400799, mean_eps: 0.632872\n",
      " 1020543/3750000: episode: 1443, duration: 3.575s, episode steps: 468, steps per second: 131, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.014485, mae: 3.719782, mean_q: 4.470698, mean_eps: 0.632688\n",
      " 1021438/3750000: episode: 1444, duration: 6.629s, episode steps: 895, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.020482, mae: 3.677987, mean_q: 4.426107, mean_eps: 0.632444\n",
      " 1022585/3750000: episode: 1445, duration: 8.742s, episode steps: 1147, steps per second: 131, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.015210, mae: 3.631799, mean_q: 4.366455, mean_eps: 0.632076\n",
      " 1022977/3750000: episode: 1446, duration: 2.878s, episode steps: 392, steps per second: 136, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.012891, mae: 3.707158, mean_q: 4.458430, mean_eps: 0.631799\n",
      " 1023450/3750000: episode: 1447, duration: 3.592s, episode steps: 473, steps per second: 132, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.024254, mae: 3.643938, mean_q: 4.383247, mean_eps: 0.631644\n",
      " 1024110/3750000: episode: 1448, duration: 4.992s, episode steps: 660, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.892 [0.000, 5.000],  loss: 0.018888, mae: 3.643116, mean_q: 4.391929, mean_eps: 0.631439\n",
      " 1025257/3750000: episode: 1449, duration: 8.575s, episode steps: 1147, steps per second: 134, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.020987, mae: 3.667455, mean_q: 4.415071, mean_eps: 0.631115\n",
      " 1025705/3750000: episode: 1450, duration: 3.429s, episode steps: 448, steps per second: 131, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.026052, mae: 3.712054, mean_q: 4.470089, mean_eps: 0.630827\n",
      " 1026229/3750000: episode: 1451, duration: 3.980s, episode steps: 524, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.016326, mae: 3.697830, mean_q: 4.441282, mean_eps: 0.630651\n",
      " 1027309/3750000: episode: 1452, duration: 8.066s, episode steps: 1080, steps per second: 134, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.019647, mae: 3.674332, mean_q: 4.417604, mean_eps: 0.630363\n",
      " 1028376/3750000: episode: 1453, duration: 8.016s, episode steps: 1067, steps per second: 133, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.018470, mae: 3.702818, mean_q: 4.449413, mean_eps: 0.629978\n",
      " 1028920/3750000: episode: 1454, duration: 4.167s, episode steps: 544, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.014758, mae: 3.689095, mean_q: 4.441891, mean_eps: 0.629690\n",
      " 1029601/3750000: episode: 1455, duration: 5.173s, episode steps: 681, steps per second: 132, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.023718, mae: 3.677241, mean_q: 4.417093, mean_eps: 0.629466\n",
      " 1030268/3750000: episode: 1456, duration: 4.994s, episode steps: 667, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.025974, mae: 3.650586, mean_q: 4.388091, mean_eps: 0.629222\n",
      " 1030882/3750000: episode: 1457, duration: 4.624s, episode steps: 614, steps per second: 133, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.022074, mae: 3.608641, mean_q: 4.349982, mean_eps: 0.628991\n",
      " 1032012/3750000: episode: 1458, duration: 8.422s, episode steps: 1130, steps per second: 134, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.016519, mae: 3.643910, mean_q: 4.383617, mean_eps: 0.628678\n",
      " 1032640/3750000: episode: 1459, duration: 4.825s, episode steps: 628, steps per second: 130, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.023180, mae: 3.616527, mean_q: 4.354825, mean_eps: 0.628365\n",
      " 1033312/3750000: episode: 1460, duration: 5.060s, episode steps: 672, steps per second: 133, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.020795, mae: 3.590235, mean_q: 4.317049, mean_eps: 0.628131\n",
      " 1033841/3750000: episode: 1461, duration: 3.999s, episode steps: 529, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.028383, mae: 3.612576, mean_q: 4.344362, mean_eps: 0.627911\n",
      " 1034893/3750000: episode: 1462, duration: 7.879s, episode steps: 1052, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.016127, mae: 3.655366, mean_q: 4.399731, mean_eps: 0.627627\n",
      " 1035378/3750000: episode: 1463, duration: 3.630s, episode steps: 485, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.015857, mae: 3.682644, mean_q: 4.433337, mean_eps: 0.627353\n",
      " 1036111/3750000: episode: 1464, duration: 5.509s, episode steps: 733, steps per second: 133, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.015272, mae: 3.583589, mean_q: 4.310575, mean_eps: 0.627134\n",
      " 1036789/3750000: episode: 1465, duration: 5.130s, episode steps: 678, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.018171, mae: 3.583412, mean_q: 4.311743, mean_eps: 0.626878\n",
      " 1037517/3750000: episode: 1466, duration: 5.391s, episode steps: 728, steps per second: 135, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.028890, mae: 3.619943, mean_q: 4.351868, mean_eps: 0.626626\n",
      " 1038028/3750000: episode: 1467, duration: 3.895s, episode steps: 511, steps per second: 131, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.017416, mae: 3.600457, mean_q: 4.340166, mean_eps: 0.626403\n",
      " 1038979/3750000: episode: 1468, duration: 7.100s, episode steps: 951, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.018477, mae: 3.665724, mean_q: 4.411586, mean_eps: 0.626140\n",
      " 1039531/3750000: episode: 1469, duration: 4.201s, episode steps: 552, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.016751, mae: 3.658361, mean_q: 4.400011, mean_eps: 0.625870\n",
      " 1040028/3750000: episode: 1470, duration: 3.746s, episode steps: 497, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.020834, mae: 3.638537, mean_q: 4.374312, mean_eps: 0.625679\n",
      " 1040518/3750000: episode: 1471, duration: 3.632s, episode steps: 490, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: 0.017703, mae: 3.693067, mean_q: 4.438631, mean_eps: 0.625503\n",
      " 1041017/3750000: episode: 1472, duration: 3.921s, episode steps: 499, steps per second: 127, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.023671, mae: 3.668890, mean_q: 4.412717, mean_eps: 0.625326\n",
      " 1041793/3750000: episode: 1473, duration: 5.812s, episode steps: 776, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.018743, mae: 3.688002, mean_q: 4.436954, mean_eps: 0.625096\n",
      " 1042898/3750000: episode: 1474, duration: 8.257s, episode steps: 1105, steps per second: 134, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.018409, mae: 3.633567, mean_q: 4.375611, mean_eps: 0.624758\n",
      " 1043700/3750000: episode: 1475, duration: 6.112s, episode steps: 802, steps per second: 131, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.017470, mae: 3.631552, mean_q: 4.370209, mean_eps: 0.624416\n",
      " 1044342/3750000: episode: 1476, duration: 4.892s, episode steps: 642, steps per second: 131, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.019378, mae: 3.645185, mean_q: 4.386422, mean_eps: 0.624153\n",
      " 1044888/3750000: episode: 1477, duration: 4.065s, episode steps: 546, steps per second: 134, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014395, mae: 3.613294, mean_q: 4.348514, mean_eps: 0.623937\n",
      " 1045757/3750000: episode: 1478, duration: 6.509s, episode steps: 869, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.017624, mae: 3.620272, mean_q: 4.349934, mean_eps: 0.623685\n",
      " 1046635/3750000: episode: 1479, duration: 6.568s, episode steps: 878, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.016876, mae: 3.624712, mean_q: 4.354791, mean_eps: 0.623372\n",
      " 1047328/3750000: episode: 1480, duration: 5.256s, episode steps: 693, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.023721, mae: 3.635206, mean_q: 4.375511, mean_eps: 0.623087\n",
      " 1047794/3750000: episode: 1481, duration: 3.520s, episode steps: 466, steps per second: 132, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.022899, mae: 3.660208, mean_q: 4.408536, mean_eps: 0.622878\n",
      " 1048461/3750000: episode: 1482, duration: 5.044s, episode steps: 667, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.021367, mae: 3.632415, mean_q: 4.389391, mean_eps: 0.622673\n",
      " 1049002/3750000: episode: 1483, duration: 4.065s, episode steps: 541, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.018026, mae: 3.629653, mean_q: 4.361766, mean_eps: 0.622454\n",
      " 1049532/3750000: episode: 1484, duration: 3.999s, episode steps: 530, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.025291, mae: 3.636920, mean_q: 4.381980, mean_eps: 0.622263\n",
      " 1050116/3750000: episode: 1485, duration: 4.381s, episode steps: 584, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.022392, mae: 3.627171, mean_q: 4.361364, mean_eps: 0.622065\n",
      " 1050912/3750000: episode: 1486, duration: 6.025s, episode steps: 796, steps per second: 132, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.020970, mae: 3.640228, mean_q: 4.379962, mean_eps: 0.621816\n",
      " 1051664/3750000: episode: 1487, duration: 5.899s, episode steps: 752, steps per second: 127, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.019992, mae: 3.618818, mean_q: 4.354102, mean_eps: 0.621536\n",
      " 1052367/3750000: episode: 1488, duration: 5.296s, episode steps: 703, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.026750, mae: 3.601889, mean_q: 4.330292, mean_eps: 0.621273\n",
      " 1053360/3750000: episode: 1489, duration: 7.471s, episode steps: 993, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.013618, mae: 3.592359, mean_q: 4.325954, mean_eps: 0.620970\n",
      " 1053846/3750000: episode: 1490, duration: 3.761s, episode steps: 486, steps per second: 129, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.024840, mae: 3.616744, mean_q: 4.358806, mean_eps: 0.620704\n",
      " 1054669/3750000: episode: 1491, duration: 6.302s, episode steps: 823, steps per second: 131, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.021015, mae: 3.612919, mean_q: 4.343642, mean_eps: 0.620466\n",
      " 1055738/3750000: episode: 1492, duration: 8.080s, episode steps: 1069, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.023338, mae: 3.574464, mean_q: 4.300262, mean_eps: 0.620128\n",
      " 1056375/3750000: episode: 1493, duration: 4.839s, episode steps: 637, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.018402, mae: 3.590720, mean_q: 4.322517, mean_eps: 0.619822\n",
      " 1056888/3750000: episode: 1494, duration: 3.921s, episode steps: 513, steps per second: 131, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.017626, mae: 3.629157, mean_q: 4.372173, mean_eps: 0.619613\n",
      " 1057485/3750000: episode: 1495, duration: 4.488s, episode steps: 597, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.021871, mae: 3.635803, mean_q: 4.371418, mean_eps: 0.619412\n",
      " 1057862/3750000: episode: 1496, duration: 2.862s, episode steps: 377, steps per second: 132, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.020695, mae: 3.598685, mean_q: 4.323754, mean_eps: 0.619235\n",
      " 1058356/3750000: episode: 1497, duration: 3.666s, episode steps: 494, steps per second: 135, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.013318, mae: 3.627708, mean_q: 4.358437, mean_eps: 0.619080\n",
      " 1059244/3750000: episode: 1498, duration: 6.724s, episode steps: 888, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.019418, mae: 3.602401, mean_q: 4.333627, mean_eps: 0.618832\n",
      " 1059898/3750000: episode: 1499, duration: 4.930s, episode steps: 654, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.016997, mae: 3.672763, mean_q: 4.417236, mean_eps: 0.618555\n",
      " 1060643/3750000: episode: 1500, duration: 5.673s, episode steps: 745, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.016087, mae: 3.659917, mean_q: 4.403158, mean_eps: 0.618303\n",
      " 1061083/3750000: episode: 1501, duration: 3.305s, episode steps: 440, steps per second: 133, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.021394, mae: 3.628252, mean_q: 4.371403, mean_eps: 0.618087\n",
      " 1061754/3750000: episode: 1502, duration: 5.012s, episode steps: 671, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.019951, mae: 3.633055, mean_q: 4.364168, mean_eps: 0.617889\n",
      " 1062536/3750000: episode: 1503, duration: 5.906s, episode steps: 782, steps per second: 132, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.029 [0.000, 5.000],  loss: 0.015494, mae: 3.610146, mean_q: 4.341945, mean_eps: 0.617630\n",
      " 1063213/3750000: episode: 1504, duration: 5.081s, episode steps: 677, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.016866, mae: 3.641396, mean_q: 4.373931, mean_eps: 0.617367\n",
      " 1063816/3750000: episode: 1505, duration: 4.496s, episode steps: 603, steps per second: 134, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.015674, mae: 3.593577, mean_q: 4.332037, mean_eps: 0.617136\n",
      " 1064327/3750000: episode: 1506, duration: 3.931s, episode steps: 511, steps per second: 130, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.017567, mae: 3.582672, mean_q: 4.311678, mean_eps: 0.616935\n",
      " 1064793/3750000: episode: 1507, duration: 3.462s, episode steps: 466, steps per second: 135, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.021159, mae: 3.589866, mean_q: 4.313549, mean_eps: 0.616758\n",
      " 1065720/3750000: episode: 1508, duration: 6.973s, episode steps: 927, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.018330, mae: 3.533971, mean_q: 4.247172, mean_eps: 0.616510\n",
      " 1066374/3750000: episode: 1509, duration: 4.986s, episode steps: 654, steps per second: 131, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.017715, mae: 3.578143, mean_q: 4.305774, mean_eps: 0.616226\n",
      " 1066870/3750000: episode: 1510, duration: 3.758s, episode steps: 496, steps per second: 132, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.014239, mae: 3.545012, mean_q: 4.262799, mean_eps: 0.616017\n",
      " 1067550/3750000: episode: 1511, duration: 5.088s, episode steps: 680, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.023822, mae: 3.574236, mean_q: 4.302848, mean_eps: 0.615804\n",
      " 1068476/3750000: episode: 1512, duration: 6.976s, episode steps: 926, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.021925, mae: 3.553655, mean_q: 4.282951, mean_eps: 0.615516\n",
      " 1068938/3750000: episode: 1513, duration: 3.471s, episode steps: 462, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.023779, mae: 3.544874, mean_q: 4.263186, mean_eps: 0.615268\n",
      " 1069450/3750000: episode: 1514, duration: 3.879s, episode steps: 512, steps per second: 132, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.017805, mae: 3.470669, mean_q: 4.167468, mean_eps: 0.615092\n",
      " 1070172/3750000: episode: 1515, duration: 5.424s, episode steps: 722, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.017068, mae: 3.527951, mean_q: 4.248682, mean_eps: 0.614868\n",
      " 1071229/3750000: episode: 1516, duration: 7.994s, episode steps: 1057, steps per second: 132, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.016834, mae: 3.499265, mean_q: 4.207931, mean_eps: 0.614548\n",
      " 1071819/3750000: episode: 1517, duration: 4.369s, episode steps: 590, steps per second: 135, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.851 [0.000, 5.000],  loss: 0.021155, mae: 3.516833, mean_q: 4.225774, mean_eps: 0.614253\n",
      " 1072436/3750000: episode: 1518, duration: 4.714s, episode steps: 617, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.022694, mae: 3.460688, mean_q: 4.162095, mean_eps: 0.614037\n",
      " 1072767/3750000: episode: 1519, duration: 2.595s, episode steps: 331, steps per second: 128, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.014672, mae: 3.488773, mean_q: 4.200000, mean_eps: 0.613864\n",
      " 1073156/3750000: episode: 1520, duration: 2.878s, episode steps: 389, steps per second: 135, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.021614, mae: 3.508053, mean_q: 4.221189, mean_eps: 0.613734\n",
      " 1073642/3750000: episode: 1521, duration: 3.716s, episode steps: 486, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.016248, mae: 3.400356, mean_q: 4.089152, mean_eps: 0.613576\n",
      " 1074976/3750000: episode: 1522, duration: 9.959s, episode steps: 1334, steps per second: 134, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.018580, mae: 3.463692, mean_q: 4.167670, mean_eps: 0.613248\n",
      " 1075935/3750000: episode: 1523, duration: 7.187s, episode steps: 959, steps per second: 133, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.019817, mae: 3.404922, mean_q: 4.094621, mean_eps: 0.612838\n",
      " 1076751/3750000: episode: 1524, duration: 6.142s, episode steps: 816, steps per second: 133, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.018809, mae: 3.477540, mean_q: 4.174555, mean_eps: 0.612518\n",
      " 1077467/3750000: episode: 1525, duration: 5.430s, episode steps: 716, steps per second: 132, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.015273, mae: 3.402928, mean_q: 4.094747, mean_eps: 0.612240\n",
      " 1078097/3750000: episode: 1526, duration: 4.674s, episode steps: 630, steps per second: 135, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.028032, mae: 3.455636, mean_q: 4.161248, mean_eps: 0.611999\n",
      " 1078864/3750000: episode: 1527, duration: 5.809s, episode steps: 767, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.016537, mae: 3.485652, mean_q: 4.191960, mean_eps: 0.611747\n",
      " 1079451/3750000: episode: 1528, duration: 4.446s, episode steps: 587, steps per second: 132, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.016556, mae: 3.442113, mean_q: 4.140126, mean_eps: 0.611502\n",
      " 1080100/3750000: episode: 1529, duration: 4.904s, episode steps: 649, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.018924, mae: 3.435062, mean_q: 4.129072, mean_eps: 0.611283\n",
      " 1080648/3750000: episode: 1530, duration: 4.178s, episode steps: 548, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.013999, mae: 3.503174, mean_q: 4.213842, mean_eps: 0.611067\n",
      " 1081264/3750000: episode: 1531, duration: 4.697s, episode steps: 616, steps per second: 131, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.019500, mae: 3.419689, mean_q: 4.116051, mean_eps: 0.610854\n",
      " 1081635/3750000: episode: 1532, duration: 2.736s, episode steps: 371, steps per second: 136, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.817 [0.000, 5.000],  loss: 0.023168, mae: 3.396046, mean_q: 4.080699, mean_eps: 0.610678\n",
      " 1082585/3750000: episode: 1533, duration: 7.142s, episode steps: 950, steps per second: 133, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.023043, mae: 3.423965, mean_q: 4.114523, mean_eps: 0.610440\n",
      " 1083486/3750000: episode: 1534, duration: 6.822s, episode steps: 901, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.941 [0.000, 5.000],  loss: 0.018596, mae: 3.446400, mean_q: 4.150502, mean_eps: 0.610106\n",
      " 1084315/3750000: episode: 1535, duration: 6.168s, episode steps: 829, steps per second: 134, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.022213, mae: 3.428487, mean_q: 4.124039, mean_eps: 0.609796\n",
      " 1085302/3750000: episode: 1536, duration: 7.461s, episode steps: 987, steps per second: 132, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.020236, mae: 3.390390, mean_q: 4.075672, mean_eps: 0.609468\n",
      " 1085977/3750000: episode: 1537, duration: 4.973s, episode steps: 675, steps per second: 136, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.012143, mae: 3.415386, mean_q: 4.106935, mean_eps: 0.609170\n",
      " 1086427/3750000: episode: 1538, duration: 3.456s, episode steps: 450, steps per second: 130, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.014361, mae: 3.399421, mean_q: 4.085105, mean_eps: 0.608968\n",
      " 1086967/3750000: episode: 1539, duration: 4.033s, episode steps: 540, steps per second: 134, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.012479, mae: 3.476724, mean_q: 4.179559, mean_eps: 0.608788\n",
      " 1087502/3750000: episode: 1540, duration: 4.097s, episode steps: 535, steps per second: 131, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.021784, mae: 3.445755, mean_q: 4.145020, mean_eps: 0.608594\n",
      " 1088379/3750000: episode: 1541, duration: 6.553s, episode steps: 877, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.014503, mae: 3.452907, mean_q: 4.161342, mean_eps: 0.608342\n",
      " 1088966/3750000: episode: 1542, duration: 4.516s, episode steps: 587, steps per second: 130, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.020539, mae: 3.434125, mean_q: 4.134182, mean_eps: 0.608079\n",
      " 1089631/3750000: episode: 1543, duration: 5.018s, episode steps: 665, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.023447, mae: 3.394094, mean_q: 4.090750, mean_eps: 0.607852\n",
      " 1090655/3750000: episode: 1544, duration: 7.677s, episode steps: 1024, steps per second: 133, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.019018, mae: 3.424076, mean_q: 4.120324, mean_eps: 0.607550\n",
      " 1091243/3750000: episode: 1545, duration: 4.465s, episode steps: 588, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.017966, mae: 3.387805, mean_q: 4.073824, mean_eps: 0.607258\n",
      " 1091869/3750000: episode: 1546, duration: 4.714s, episode steps: 626, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.019518, mae: 3.381613, mean_q: 4.070508, mean_eps: 0.607038\n",
      " 1092689/3750000: episode: 1547, duration: 6.125s, episode steps: 820, steps per second: 134, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.012429, mae: 3.407615, mean_q: 4.102472, mean_eps: 0.606779\n",
      " 1093530/3750000: episode: 1548, duration: 6.332s, episode steps: 841, steps per second: 133, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.019020, mae: 3.422407, mean_q: 4.116890, mean_eps: 0.606480\n",
      " 1094380/3750000: episode: 1549, duration: 6.460s, episode steps: 850, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.017580, mae: 3.446776, mean_q: 4.154740, mean_eps: 0.606178\n",
      " 1095074/3750000: episode: 1550, duration: 5.237s, episode steps: 694, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.017996, mae: 3.461054, mean_q: 4.163687, mean_eps: 0.605901\n",
      " 1095707/3750000: episode: 1551, duration: 4.791s, episode steps: 633, steps per second: 132, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.012412, mae: 3.391813, mean_q: 4.079289, mean_eps: 0.605660\n",
      " 1096663/3750000: episode: 1552, duration: 7.194s, episode steps: 956, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.025015, mae: 3.409727, mean_q: 4.102755, mean_eps: 0.605372\n",
      " 1097972/3750000: episode: 1553, duration: 9.698s, episode steps: 1309, steps per second: 135, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.023202, mae: 3.439670, mean_q: 4.141502, mean_eps: 0.604965\n",
      " 1098515/3750000: episode: 1554, duration: 4.128s, episode steps: 543, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.018384, mae: 3.417554, mean_q: 4.116041, mean_eps: 0.604634\n",
      " 1099195/3750000: episode: 1555, duration: 5.083s, episode steps: 680, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.016941, mae: 3.448567, mean_q: 4.146743, mean_eps: 0.604414\n",
      " 1099785/3750000: episode: 1556, duration: 4.450s, episode steps: 590, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.013367, mae: 3.434528, mean_q: 4.136113, mean_eps: 0.604184\n",
      " 1100670/3750000: episode: 1557, duration: 6.662s, episode steps: 885, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.019972, mae: 3.397715, mean_q: 4.091839, mean_eps: 0.603917\n",
      " 1101161/3750000: episode: 1558, duration: 3.712s, episode steps: 491, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.018138, mae: 3.410345, mean_q: 4.103546, mean_eps: 0.603669\n",
      " 1102288/3750000: episode: 1559, duration: 8.397s, episode steps: 1127, steps per second: 134, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015847, mae: 3.446316, mean_q: 4.148499, mean_eps: 0.603377\n",
      " 1103039/3750000: episode: 1560, duration: 5.571s, episode steps: 751, steps per second: 135, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.019261, mae: 3.427740, mean_q: 4.125024, mean_eps: 0.603042\n",
      " 1104361/3750000: episode: 1561, duration: 9.949s, episode steps: 1322, steps per second: 133, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.016525, mae: 3.478629, mean_q: 4.185661, mean_eps: 0.602668\n",
      " 1105404/3750000: episode: 1562, duration: 7.859s, episode steps: 1043, steps per second: 133, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.021496, mae: 3.441286, mean_q: 4.138159, mean_eps: 0.602240\n",
      " 1105805/3750000: episode: 1563, duration: 3.010s, episode steps: 401, steps per second: 133, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.016698, mae: 3.413089, mean_q: 4.104705, mean_eps: 0.601980\n",
      " 1106430/3750000: episode: 1564, duration: 4.676s, episode steps: 625, steps per second: 134, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.020842, mae: 3.411569, mean_q: 4.106251, mean_eps: 0.601797\n",
      " 1107639/3750000: episode: 1565, duration: 9.046s, episode steps: 1209, steps per second: 134, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.017745, mae: 3.397259, mean_q: 4.090823, mean_eps: 0.601469\n",
      " 1108155/3750000: episode: 1566, duration: 3.906s, episode steps: 516, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.021493, mae: 3.415229, mean_q: 4.117428, mean_eps: 0.601160\n",
      " 1108706/3750000: episode: 1567, duration: 4.244s, episode steps: 551, steps per second: 130, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.021614, mae: 3.459353, mean_q: 4.173545, mean_eps: 0.600965\n",
      " 1109723/3750000: episode: 1568, duration: 7.642s, episode steps: 1017, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.015266, mae: 3.457920, mean_q: 4.164631, mean_eps: 0.600681\n",
      " 1110208/3750000: episode: 1569, duration: 3.615s, episode steps: 485, steps per second: 134, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.015447, mae: 3.404506, mean_q: 4.098914, mean_eps: 0.600411\n",
      " 1111418/3750000: episode: 1570, duration: 9.054s, episode steps: 1210, steps per second: 134, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 0.019131, mae: 3.444865, mean_q: 4.151417, mean_eps: 0.600108\n",
      " 1111880/3750000: episode: 1571, duration: 3.553s, episode steps: 462, steps per second: 130, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.017206, mae: 3.423650, mean_q: 4.121873, mean_eps: 0.599810\n",
      " 1112415/3750000: episode: 1572, duration: 4.018s, episode steps: 535, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.018907, mae: 3.469743, mean_q: 4.185388, mean_eps: 0.599630\n",
      " 1113034/3750000: episode: 1573, duration: 4.714s, episode steps: 619, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.018273, mae: 3.425461, mean_q: 4.120248, mean_eps: 0.599421\n",
      " 1113434/3750000: episode: 1574, duration: 3.004s, episode steps: 400, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.022129, mae: 3.478715, mean_q: 4.184215, mean_eps: 0.599237\n",
      " 1114385/3750000: episode: 1575, duration: 7.142s, episode steps: 951, steps per second: 133, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.019209, mae: 3.440782, mean_q: 4.142590, mean_eps: 0.598992\n",
      " 1115096/3750000: episode: 1576, duration: 5.362s, episode steps: 711, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.016663, mae: 3.494900, mean_q: 4.202103, mean_eps: 0.598694\n",
      " 1115768/3750000: episode: 1577, duration: 5.082s, episode steps: 672, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.019658, mae: 3.424467, mean_q: 4.123572, mean_eps: 0.598445\n",
      " 1116297/3750000: episode: 1578, duration: 3.927s, episode steps: 529, steps per second: 135, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.091 [0.000, 5.000],  loss: 0.013739, mae: 3.470114, mean_q: 4.174439, mean_eps: 0.598229\n",
      " 1117393/3750000: episode: 1579, duration: 8.277s, episode steps: 1096, steps per second: 132, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.018122, mae: 3.472548, mean_q: 4.179071, mean_eps: 0.597938\n",
      " 1117910/3750000: episode: 1580, duration: 3.874s, episode steps: 517, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.027256, mae: 3.421943, mean_q: 4.116455, mean_eps: 0.597646\n",
      " 1118375/3750000: episode: 1581, duration: 3.477s, episode steps: 465, steps per second: 134, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.028894, mae: 3.411002, mean_q: 4.099918, mean_eps: 0.597470\n",
      " 1119016/3750000: episode: 1582, duration: 4.834s, episode steps: 641, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.015505, mae: 3.399822, mean_q: 4.094511, mean_eps: 0.597272\n",
      " 1119588/3750000: episode: 1583, duration: 4.392s, episode steps: 572, steps per second: 130, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.014194, mae: 3.414833, mean_q: 4.110686, mean_eps: 0.597052\n",
      " 1120537/3750000: episode: 1584, duration: 7.163s, episode steps: 949, steps per second: 132, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.017517, mae: 3.415084, mean_q: 4.109121, mean_eps: 0.596778\n",
      " 1121417/3750000: episode: 1585, duration: 6.623s, episode steps: 880, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.021105, mae: 3.418086, mean_q: 4.115920, mean_eps: 0.596451\n",
      " 1122064/3750000: episode: 1586, duration: 4.938s, episode steps: 647, steps per second: 131, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.736 [0.000, 5.000],  loss: 0.018149, mae: 3.424979, mean_q: 4.125016, mean_eps: 0.596174\n",
      " 1123080/3750000: episode: 1587, duration: 7.624s, episode steps: 1016, steps per second: 133, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.017420, mae: 3.437256, mean_q: 4.134973, mean_eps: 0.595875\n",
      " 1123403/3750000: episode: 1588, duration: 2.529s, episode steps: 323, steps per second: 128, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 3.121 [0.000, 5.000],  loss: 0.016072, mae: 3.385962, mean_q: 4.069718, mean_eps: 0.595634\n",
      " 1124229/3750000: episode: 1589, duration: 6.227s, episode steps: 826, steps per second: 133, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.020766, mae: 3.389517, mean_q: 4.078611, mean_eps: 0.595425\n",
      " 1124899/3750000: episode: 1590, duration: 4.979s, episode steps: 670, steps per second: 135, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.020325, mae: 3.437390, mean_q: 4.151249, mean_eps: 0.595158\n",
      " 1125330/3750000: episode: 1591, duration: 3.278s, episode steps: 431, steps per second: 131, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.923 [0.000, 5.000],  loss: 0.015191, mae: 3.366429, mean_q: 4.056874, mean_eps: 0.594960\n",
      " 1126226/3750000: episode: 1592, duration: 6.758s, episode steps: 896, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.021728, mae: 3.401811, mean_q: 4.097423, mean_eps: 0.594719\n",
      " 1127238/3750000: episode: 1593, duration: 7.537s, episode steps: 1012, steps per second: 134, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.021930, mae: 3.387378, mean_q: 4.072357, mean_eps: 0.594377\n",
      " 1127756/3750000: episode: 1594, duration: 3.903s, episode steps: 518, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.021262, mae: 3.370860, mean_q: 4.050340, mean_eps: 0.594104\n",
      " 1128345/3750000: episode: 1595, duration: 4.518s, episode steps: 589, steps per second: 130, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.014010, mae: 3.355834, mean_q: 4.036830, mean_eps: 0.593902\n",
      " 1129017/3750000: episode: 1596, duration: 5.046s, episode steps: 672, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.021154, mae: 3.349349, mean_q: 4.029342, mean_eps: 0.593675\n",
      " 1129605/3750000: episode: 1597, duration: 4.446s, episode steps: 588, steps per second: 132, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.017947, mae: 3.364810, mean_q: 4.047235, mean_eps: 0.593448\n",
      " 1130120/3750000: episode: 1598, duration: 3.939s, episode steps: 515, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.017885, mae: 3.319884, mean_q: 3.992972, mean_eps: 0.593250\n",
      " 1131256/3750000: episode: 1599, duration: 8.526s, episode steps: 1136, steps per second: 133, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.016673, mae: 3.397081, mean_q: 4.083210, mean_eps: 0.592955\n",
      " 1131668/3750000: episode: 1600, duration: 3.199s, episode steps: 412, steps per second: 129, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.716 [0.000, 5.000],  loss: 0.018698, mae: 3.398422, mean_q: 4.092492, mean_eps: 0.592674\n",
      " 1132065/3750000: episode: 1601, duration: 3.060s, episode steps: 397, steps per second: 130, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.013732, mae: 3.376920, mean_q: 4.063221, mean_eps: 0.592527\n",
      " 1132563/3750000: episode: 1602, duration: 3.808s, episode steps: 498, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.023147, mae: 3.331308, mean_q: 4.008680, mean_eps: 0.592365\n",
      " 1133426/3750000: episode: 1603, duration: 6.474s, episode steps: 863, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.019496, mae: 3.360086, mean_q: 4.040994, mean_eps: 0.592120\n",
      " 1134390/3750000: episode: 1604, duration: 7.309s, episode steps: 964, steps per second: 132, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.016566, mae: 3.356552, mean_q: 4.040726, mean_eps: 0.591792\n",
      " 1135137/3750000: episode: 1605, duration: 5.579s, episode steps: 747, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.030319, mae: 3.333132, mean_q: 4.018790, mean_eps: 0.591486\n",
      " 1135723/3750000: episode: 1606, duration: 4.457s, episode steps: 586, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.019055, mae: 3.321367, mean_q: 3.997693, mean_eps: 0.591245\n",
      " 1136152/3750000: episode: 1607, duration: 3.226s, episode steps: 429, steps per second: 133, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.846 [0.000, 5.000],  loss: 0.021755, mae: 3.327328, mean_q: 4.006156, mean_eps: 0.591062\n",
      " 1137078/3750000: episode: 1608, duration: 7.003s, episode steps: 926, steps per second: 132, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.021542, mae: 3.335628, mean_q: 4.013655, mean_eps: 0.590820\n",
      " 1137835/3750000: episode: 1609, duration: 5.716s, episode steps: 757, steps per second: 132, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.053 [0.000, 5.000],  loss: 0.022226, mae: 3.313925, mean_q: 3.987154, mean_eps: 0.590518\n",
      " 1138703/3750000: episode: 1610, duration: 6.585s, episode steps: 868, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.019761, mae: 3.304216, mean_q: 3.970935, mean_eps: 0.590223\n",
      " 1140090/3750000: episode: 1611, duration: 10.393s, episode steps: 1387, steps per second: 133, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.017968, mae: 3.330052, mean_q: 4.005302, mean_eps: 0.589816\n",
      " 1141000/3750000: episode: 1612, duration: 6.987s, episode steps: 910, steps per second: 130, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.016238, mae: 3.342385, mean_q: 4.022299, mean_eps: 0.589406\n",
      " 1141657/3750000: episode: 1613, duration: 4.924s, episode steps: 657, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.014927, mae: 3.319606, mean_q: 3.998058, mean_eps: 0.589125\n",
      " 1142126/3750000: episode: 1614, duration: 3.562s, episode steps: 469, steps per second: 132, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.970 [0.000, 5.000],  loss: 0.017282, mae: 3.292343, mean_q: 3.957568, mean_eps: 0.588920\n",
      " 1142674/3750000: episode: 1615, duration: 4.107s, episode steps: 548, steps per second: 133, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.015371, mae: 3.312199, mean_q: 3.982606, mean_eps: 0.588736\n",
      " 1143539/3750000: episode: 1616, duration: 6.465s, episode steps: 865, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014898, mae: 3.340049, mean_q: 4.021356, mean_eps: 0.588484\n",
      " 1144417/3750000: episode: 1617, duration: 6.571s, episode steps: 878, steps per second: 134, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.022920, mae: 3.312708, mean_q: 3.984324, mean_eps: 0.588171\n",
      " 1144992/3750000: episode: 1618, duration: 4.403s, episode steps: 575, steps per second: 131, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.018453, mae: 3.322653, mean_q: 4.004187, mean_eps: 0.587908\n",
      " 1145998/3750000: episode: 1619, duration: 7.520s, episode steps: 1006, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.020969, mae: 3.321985, mean_q: 3.998579, mean_eps: 0.587624\n",
      " 1146499/3750000: episode: 1620, duration: 3.757s, episode steps: 501, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.010048, mae: 3.329583, mean_q: 4.016087, mean_eps: 0.587354\n",
      " 1147307/3750000: episode: 1621, duration: 6.142s, episode steps: 808, steps per second: 132, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.015994, mae: 3.327480, mean_q: 4.001490, mean_eps: 0.587116\n",
      " 1147797/3750000: episode: 1622, duration: 3.624s, episode steps: 490, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.019877, mae: 3.325309, mean_q: 3.999291, mean_eps: 0.586882\n",
      " 1148869/3750000: episode: 1623, duration: 8.082s, episode steps: 1072, steps per second: 133, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.021014, mae: 3.337085, mean_q: 4.021553, mean_eps: 0.586601\n",
      " 1149430/3750000: episode: 1624, duration: 4.271s, episode steps: 561, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.020002, mae: 3.354433, mean_q: 4.034033, mean_eps: 0.586306\n",
      " 1150168/3750000: episode: 1625, duration: 5.514s, episode steps: 738, steps per second: 134, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.020950, mae: 3.358691, mean_q: 4.036707, mean_eps: 0.586072\n",
      " 1151013/3750000: episode: 1626, duration: 6.287s, episode steps: 845, steps per second: 134, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.015591, mae: 3.365102, mean_q: 4.056867, mean_eps: 0.585788\n",
      " 1151567/3750000: episode: 1627, duration: 4.219s, episode steps: 554, steps per second: 131, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.011509, mae: 3.341723, mean_q: 4.024741, mean_eps: 0.585536\n",
      " 1152398/3750000: episode: 1628, duration: 6.231s, episode steps: 831, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.019157, mae: 3.377725, mean_q: 4.060918, mean_eps: 0.585287\n",
      " 1153311/3750000: episode: 1629, duration: 6.925s, episode steps: 913, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.013216, mae: 3.323453, mean_q: 4.002984, mean_eps: 0.584974\n",
      " 1153781/3750000: episode: 1630, duration: 3.597s, episode steps: 470, steps per second: 131, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013726, mae: 3.237676, mean_q: 3.896280, mean_eps: 0.584722\n",
      " 1154447/3750000: episode: 1631, duration: 4.960s, episode steps: 666, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.019493, mae: 3.341794, mean_q: 4.019910, mean_eps: 0.584517\n",
      " 1154841/3750000: episode: 1632, duration: 2.986s, episode steps: 394, steps per second: 132, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.011772, mae: 3.347235, mean_q: 4.033982, mean_eps: 0.584326\n",
      " 1155673/3750000: episode: 1633, duration: 6.261s, episode steps: 832, steps per second: 133, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.016506, mae: 3.325450, mean_q: 4.003333, mean_eps: 0.584106\n",
      " 1156161/3750000: episode: 1634, duration: 3.770s, episode steps: 488, steps per second: 129, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.015892, mae: 3.355127, mean_q: 4.038074, mean_eps: 0.583869\n",
      " 1156720/3750000: episode: 1635, duration: 4.193s, episode steps: 559, steps per second: 133, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.019315, mae: 3.335238, mean_q: 4.008065, mean_eps: 0.583682\n",
      " 1157478/3750000: episode: 1636, duration: 5.701s, episode steps: 758, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.021692, mae: 3.357839, mean_q: 4.038360, mean_eps: 0.583448\n",
      " 1158190/3750000: episode: 1637, duration: 5.399s, episode steps: 712, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.022223, mae: 3.304867, mean_q: 3.981353, mean_eps: 0.583181\n",
      " 1158946/3750000: episode: 1638, duration: 5.649s, episode steps: 756, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.017891, mae: 3.377608, mean_q: 4.061592, mean_eps: 0.582915\n",
      " 1159454/3750000: episode: 1639, duration: 3.741s, episode steps: 508, steps per second: 136, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.024079, mae: 3.350358, mean_q: 4.026566, mean_eps: 0.582688\n",
      " 1160084/3750000: episode: 1640, duration: 4.810s, episode steps: 630, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.021486, mae: 3.325575, mean_q: 4.003930, mean_eps: 0.582483\n",
      " 1161138/3750000: episode: 1641, duration: 7.809s, episode steps: 1054, steps per second: 135, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.017492, mae: 3.319901, mean_q: 3.994321, mean_eps: 0.582180\n",
      " 1161802/3750000: episode: 1642, duration: 5.096s, episode steps: 664, steps per second: 130, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.753 [0.000, 5.000],  loss: 0.020502, mae: 3.365236, mean_q: 4.051665, mean_eps: 0.581871\n",
      " 1162788/3750000: episode: 1643, duration: 7.303s, episode steps: 986, steps per second: 135, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.021388, mae: 3.343494, mean_q: 4.022454, mean_eps: 0.581572\n",
      " 1163558/3750000: episode: 1644, duration: 5.842s, episode steps: 770, steps per second: 132, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.019022, mae: 3.356114, mean_q: 4.037638, mean_eps: 0.581259\n",
      " 1164030/3750000: episode: 1645, duration: 3.656s, episode steps: 472, steps per second: 129, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.016214, mae: 3.332849, mean_q: 4.007545, mean_eps: 0.581036\n",
      " 1164705/3750000: episode: 1646, duration: 5.054s, episode steps: 675, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.016896, mae: 3.380454, mean_q: 4.067035, mean_eps: 0.580827\n",
      " 1165222/3750000: episode: 1647, duration: 3.890s, episode steps: 517, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.019467, mae: 3.348228, mean_q: 4.029786, mean_eps: 0.580611\n",
      " 1166120/3750000: episode: 1648, duration: 6.784s, episode steps: 898, steps per second: 132, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015658, mae: 3.362663, mean_q: 4.048582, mean_eps: 0.580359\n",
      " 1167003/3750000: episode: 1649, duration: 6.693s, episode steps: 883, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.018899, mae: 3.415419, mean_q: 4.110094, mean_eps: 0.580038\n",
      " 1167507/3750000: episode: 1650, duration: 3.742s, episode steps: 504, steps per second: 135, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.017373, mae: 3.361768, mean_q: 4.049400, mean_eps: 0.579786\n",
      " 1168370/3750000: episode: 1651, duration: 6.778s, episode steps: 863, steps per second: 127, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.016410, mae: 3.462743, mean_q: 4.166920, mean_eps: 0.579542\n",
      " 1169053/3750000: episode: 1652, duration: 5.161s, episode steps: 683, steps per second: 132, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.018470, mae: 3.453210, mean_q: 4.156523, mean_eps: 0.579264\n",
      " 1169548/3750000: episode: 1653, duration: 3.833s, episode steps: 495, steps per second: 129, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.014655, mae: 3.374110, mean_q: 4.057317, mean_eps: 0.579052\n",
      " 1170508/3750000: episode: 1654, duration: 7.244s, episode steps: 960, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.012919, mae: 3.393494, mean_q: 4.082045, mean_eps: 0.578789\n",
      " 1171159/3750000: episode: 1655, duration: 4.833s, episode steps: 651, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.021871, mae: 3.412195, mean_q: 4.101020, mean_eps: 0.578501\n",
      " 1171806/3750000: episode: 1656, duration: 4.897s, episode steps: 647, steps per second: 132, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.020797, mae: 3.413796, mean_q: 4.102295, mean_eps: 0.578267\n",
      " 1172132/3750000: episode: 1657, duration: 2.453s, episode steps: 326, steps per second: 133, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.010993, mae: 3.354570, mean_q: 4.034562, mean_eps: 0.578091\n",
      " 1173189/3750000: episode: 1658, duration: 7.942s, episode steps: 1057, steps per second: 133, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.021988, mae: 3.405591, mean_q: 4.094940, mean_eps: 0.577842\n",
      " 1173920/3750000: episode: 1659, duration: 5.486s, episode steps: 731, steps per second: 133, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.019867, mae: 3.401955, mean_q: 4.094659, mean_eps: 0.577522\n",
      " 1174973/3750000: episode: 1660, duration: 7.978s, episode steps: 1053, steps per second: 132, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.015670, mae: 3.376966, mean_q: 4.064883, mean_eps: 0.577202\n",
      " 1175478/3750000: episode: 1661, duration: 3.772s, episode steps: 505, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.396 [0.000, 5.000],  loss: 0.013496, mae: 3.419566, mean_q: 4.134312, mean_eps: 0.576921\n",
      " 1176278/3750000: episode: 1662, duration: 6.006s, episode steps: 800, steps per second: 133, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.014463, mae: 3.376579, mean_q: 4.067075, mean_eps: 0.576687\n",
      " 1176912/3750000: episode: 1663, duration: 4.849s, episode steps: 634, steps per second: 131, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.016004, mae: 3.389191, mean_q: 4.078285, mean_eps: 0.576428\n",
      " 1177402/3750000: episode: 1664, duration: 3.743s, episode steps: 490, steps per second: 131, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.019658, mae: 3.333153, mean_q: 4.009649, mean_eps: 0.576222\n",
      " 1178120/3750000: episode: 1665, duration: 5.404s, episode steps: 718, steps per second: 133, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.024477, mae: 3.390764, mean_q: 4.081015, mean_eps: 0.576006\n",
      " 1178725/3750000: episode: 1666, duration: 4.668s, episode steps: 605, steps per second: 130, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.016964, mae: 3.429155, mean_q: 4.122971, mean_eps: 0.575769\n",
      " 1179418/3750000: episode: 1667, duration: 5.142s, episode steps: 693, steps per second: 135, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.016980, mae: 3.407832, mean_q: 4.110238, mean_eps: 0.575535\n",
      " 1180615/3750000: episode: 1668, duration: 8.952s, episode steps: 1197, steps per second: 134, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.018495, mae: 3.379258, mean_q: 4.069472, mean_eps: 0.575196\n",
      " 1181508/3750000: episode: 1669, duration: 6.809s, episode steps: 893, steps per second: 131, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.014781, mae: 3.359989, mean_q: 4.049974, mean_eps: 0.574818\n",
      " 1181866/3750000: episode: 1670, duration: 2.693s, episode steps: 358, steps per second: 133, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.026549, mae: 3.375629, mean_q: 4.065462, mean_eps: 0.574592\n",
      " 1182878/3750000: episode: 1671, duration: 7.509s, episode steps: 1012, steps per second: 135, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015067, mae: 3.419001, mean_q: 4.125433, mean_eps: 0.574347\n",
      " 1183889/3750000: episode: 1672, duration: 7.690s, episode steps: 1011, steps per second: 131, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.023832, mae: 3.364351, mean_q: 4.046227, mean_eps: 0.573983\n",
      " 1184562/3750000: episode: 1673, duration: 5.068s, episode steps: 673, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.014666, mae: 3.418520, mean_q: 4.116654, mean_eps: 0.573677\n",
      " 1185088/3750000: episode: 1674, duration: 3.965s, episode steps: 526, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.015756, mae: 3.393756, mean_q: 4.084502, mean_eps: 0.573461\n",
      " 1185971/3750000: episode: 1675, duration: 6.620s, episode steps: 883, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.017788, mae: 3.357963, mean_q: 4.039713, mean_eps: 0.573209\n",
      " 1186459/3750000: episode: 1676, duration: 3.634s, episode steps: 488, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.017760, mae: 3.377863, mean_q: 4.062128, mean_eps: 0.572964\n",
      " 1186966/3750000: episode: 1677, duration: 3.855s, episode steps: 507, steps per second: 132, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.031696, mae: 3.319195, mean_q: 3.995996, mean_eps: 0.572784\n",
      " 1187634/3750000: episode: 1678, duration: 5.112s, episode steps: 668, steps per second: 131, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.013982, mae: 3.387615, mean_q: 4.078607, mean_eps: 0.572572\n",
      " 1188295/3750000: episode: 1679, duration: 5.007s, episode steps: 661, steps per second: 132, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.024542, mae: 3.325421, mean_q: 3.995700, mean_eps: 0.572334\n",
      " 1189081/3750000: episode: 1680, duration: 5.957s, episode steps: 786, steps per second: 132, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.019173, mae: 3.385795, mean_q: 4.071431, mean_eps: 0.572072\n",
      " 1189896/3750000: episode: 1681, duration: 6.081s, episode steps: 815, steps per second: 134, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.018196, mae: 3.372357, mean_q: 4.051087, mean_eps: 0.571784\n",
      " 1190922/3750000: episode: 1682, duration: 7.714s, episode steps: 1026, steps per second: 133, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.018891, mae: 3.380501, mean_q: 4.066378, mean_eps: 0.571452\n",
      " 1191436/3750000: episode: 1683, duration: 3.818s, episode steps: 514, steps per second: 135, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.023850, mae: 3.337649, mean_q: 4.023991, mean_eps: 0.571175\n",
      " 1192100/3750000: episode: 1684, duration: 5.061s, episode steps: 664, steps per second: 131, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.020355, mae: 3.388881, mean_q: 4.077522, mean_eps: 0.570966\n",
      " 1192841/3750000: episode: 1685, duration: 5.609s, episode steps: 741, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.020200, mae: 3.383746, mean_q: 4.065365, mean_eps: 0.570711\n",
      " 1193807/3750000: episode: 1686, duration: 7.298s, episode steps: 966, steps per second: 132, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.951 [0.000, 5.000],  loss: 0.017635, mae: 3.362524, mean_q: 4.051169, mean_eps: 0.570401\n",
      " 1194204/3750000: episode: 1687, duration: 2.983s, episode steps: 397, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.020900, mae: 3.361981, mean_q: 4.046172, mean_eps: 0.570156\n",
      " 1194733/3750000: episode: 1688, duration: 3.931s, episode steps: 529, steps per second: 135, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.018369, mae: 3.283004, mean_q: 3.952355, mean_eps: 0.569991\n",
      " 1195476/3750000: episode: 1689, duration: 5.584s, episode steps: 743, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.016785, mae: 3.309400, mean_q: 3.981373, mean_eps: 0.569764\n",
      " 1196112/3750000: episode: 1690, duration: 4.881s, episode steps: 636, steps per second: 130, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017483, mae: 3.306502, mean_q: 3.988525, mean_eps: 0.569516\n",
      " 1197399/3750000: episode: 1691, duration: 9.561s, episode steps: 1287, steps per second: 135, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.020246, mae: 3.308340, mean_q: 3.980149, mean_eps: 0.569170\n",
      " 1197850/3750000: episode: 1692, duration: 3.435s, episode steps: 451, steps per second: 131, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.012785, mae: 3.320473, mean_q: 3.996027, mean_eps: 0.568857\n",
      " 1198785/3750000: episode: 1693, duration: 7.055s, episode steps: 935, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.018985, mae: 3.301877, mean_q: 3.974338, mean_eps: 0.568605\n",
      " 1199568/3750000: episode: 1694, duration: 5.863s, episode steps: 783, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.017872, mae: 3.344541, mean_q: 4.022527, mean_eps: 0.568295\n",
      " 1200687/3750000: episode: 1695, duration: 8.437s, episode steps: 1119, steps per second: 133, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.018005, mae: 3.328311, mean_q: 4.004027, mean_eps: 0.567953\n",
      " 1201776/3750000: episode: 1696, duration: 8.082s, episode steps: 1089, steps per second: 135, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.016626, mae: 3.385344, mean_q: 4.070935, mean_eps: 0.567557\n",
      " 1202876/3750000: episode: 1697, duration: 8.278s, episode steps: 1100, steps per second: 133, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.017404, mae: 3.364461, mean_q: 4.048452, mean_eps: 0.567165\n",
      " 1203449/3750000: episode: 1698, duration: 4.379s, episode steps: 573, steps per second: 131, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.020703, mae: 3.371134, mean_q: 4.068575, mean_eps: 0.566862\n",
      " 1204306/3750000: episode: 1699, duration: 6.504s, episode steps: 857, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.019546, mae: 3.307718, mean_q: 3.982308, mean_eps: 0.566603\n",
      " 1205515/3750000: episode: 1700, duration: 8.952s, episode steps: 1209, steps per second: 135, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.012289, mae: 3.346995, mean_q: 4.025806, mean_eps: 0.566232\n",
      " 1206306/3750000: episode: 1701, duration: 6.048s, episode steps: 791, steps per second: 131, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.017356, mae: 3.414028, mean_q: 4.105633, mean_eps: 0.565872\n",
      " 1206931/3750000: episode: 1702, duration: 4.744s, episode steps: 625, steps per second: 132, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.025307, mae: 3.346778, mean_q: 4.030632, mean_eps: 0.565617\n",
      " 1207392/3750000: episode: 1703, duration: 3.458s, episode steps: 461, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.018266, mae: 3.368487, mean_q: 4.057472, mean_eps: 0.565422\n",
      " 1208227/3750000: episode: 1704, duration: 6.304s, episode steps: 835, steps per second: 132, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.015870, mae: 3.382134, mean_q: 4.078336, mean_eps: 0.565188\n",
      " 1208976/3750000: episode: 1705, duration: 5.660s, episode steps: 749, steps per second: 132, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.016079, mae: 3.369507, mean_q: 4.063324, mean_eps: 0.564904\n",
      " 1210014/3750000: episode: 1706, duration: 7.817s, episode steps: 1038, steps per second: 133, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.018297, mae: 3.383669, mean_q: 4.074759, mean_eps: 0.564584\n",
      " 1210886/3750000: episode: 1707, duration: 6.623s, episode steps: 872, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.023570, mae: 3.380758, mean_q: 4.065503, mean_eps: 0.564238\n",
      " 1211558/3750000: episode: 1708, duration: 4.996s, episode steps: 672, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.017009, mae: 3.435331, mean_q: 4.129254, mean_eps: 0.563961\n",
      " 1212095/3750000: episode: 1709, duration: 4.040s, episode steps: 537, steps per second: 133, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.022728, mae: 3.407965, mean_q: 4.110912, mean_eps: 0.563745\n",
      " 1212860/3750000: episode: 1710, duration: 5.834s, episode steps: 765, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.018274, mae: 3.376685, mean_q: 4.067118, mean_eps: 0.563511\n",
      " 1213416/3750000: episode: 1711, duration: 4.261s, episode steps: 556, steps per second: 130, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.018110, mae: 3.439137, mean_q: 4.135966, mean_eps: 0.563273\n",
      " 1214147/3750000: episode: 1712, duration: 5.698s, episode steps: 731, steps per second: 128, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.016440, mae: 3.364737, mean_q: 4.046158, mean_eps: 0.563039\n",
      " 1214983/3750000: episode: 1713, duration: 6.324s, episode steps: 836, steps per second: 132, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.016313, mae: 3.367001, mean_q: 4.049593, mean_eps: 0.562755\n",
      " 1215902/3750000: episode: 1714, duration: 7.018s, episode steps: 919, steps per second: 131, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.019199, mae: 3.352393, mean_q: 4.035830, mean_eps: 0.562438\n",
      " 1216804/3750000: episode: 1715, duration: 6.804s, episode steps: 902, steps per second: 133, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.019371, mae: 3.386048, mean_q: 4.069767, mean_eps: 0.562110\n",
      " 1217811/3750000: episode: 1716, duration: 7.545s, episode steps: 1007, steps per second: 133, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.013934, mae: 3.342508, mean_q: 4.019206, mean_eps: 0.561768\n",
      " 1218191/3750000: episode: 1717, duration: 2.857s, episode steps: 380, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.017692, mae: 3.352533, mean_q: 4.028024, mean_eps: 0.561520\n",
      " 1218752/3750000: episode: 1718, duration: 4.230s, episode steps: 561, steps per second: 133, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.018382, mae: 3.328468, mean_q: 4.004745, mean_eps: 0.561351\n",
      " 1219349/3750000: episode: 1719, duration: 4.557s, episode steps: 597, steps per second: 131, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.019094, mae: 3.284909, mean_q: 3.951409, mean_eps: 0.561142\n",
      " 1220026/3750000: episode: 1720, duration: 5.113s, episode steps: 677, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.019892, mae: 3.341443, mean_q: 4.020919, mean_eps: 0.560912\n",
      " 1220567/3750000: episode: 1721, duration: 4.053s, episode steps: 541, steps per second: 133, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.015325, mae: 3.299414, mean_q: 3.970548, mean_eps: 0.560692\n",
      " 1221229/3750000: episode: 1722, duration: 4.985s, episode steps: 662, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.015095, mae: 3.324319, mean_q: 4.003221, mean_eps: 0.560476\n",
      " 1222191/3750000: episode: 1723, duration: 7.229s, episode steps: 962, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.024519, mae: 3.317434, mean_q: 3.987072, mean_eps: 0.560184\n",
      " 1222706/3750000: episode: 1724, duration: 3.850s, episode steps: 515, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.012198, mae: 3.308573, mean_q: 3.979992, mean_eps: 0.559918\n",
      " 1223261/3750000: episode: 1725, duration: 4.198s, episode steps: 555, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.016655, mae: 3.283053, mean_q: 3.945798, mean_eps: 0.559724\n",
      " 1223832/3750000: episode: 1726, duration: 4.253s, episode steps: 571, steps per second: 134, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.026131, mae: 3.290173, mean_q: 3.968903, mean_eps: 0.559522\n",
      " 1224300/3750000: episode: 1727, duration: 3.563s, episode steps: 468, steps per second: 131, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.022176, mae: 3.250702, mean_q: 3.919449, mean_eps: 0.559338\n",
      " 1225052/3750000: episode: 1728, duration: 5.661s, episode steps: 752, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.014019, mae: 3.256746, mean_q: 3.923609, mean_eps: 0.559119\n",
      " 1225761/3750000: episode: 1729, duration: 5.448s, episode steps: 709, steps per second: 130, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.018615, mae: 3.233930, mean_q: 3.888493, mean_eps: 0.558852\n",
      " 1226375/3750000: episode: 1730, duration: 4.533s, episode steps: 614, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.016281, mae: 3.293499, mean_q: 3.961036, mean_eps: 0.558615\n",
      " 1227593/3750000: episode: 1731, duration: 9.183s, episode steps: 1218, steps per second: 133, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.016958, mae: 3.234641, mean_q: 3.891340, mean_eps: 0.558287\n",
      " 1228319/3750000: episode: 1732, duration: 5.401s, episode steps: 726, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.018875, mae: 3.247553, mean_q: 3.911234, mean_eps: 0.557938\n",
      " 1229107/3750000: episode: 1733, duration: 5.956s, episode steps: 788, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.016407, mae: 3.268201, mean_q: 3.932125, mean_eps: 0.557664\n",
      " 1229528/3750000: episode: 1734, duration: 3.156s, episode steps: 421, steps per second: 133, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.076 [0.000, 5.000],  loss: 0.011249, mae: 3.280764, mean_q: 3.948764, mean_eps: 0.557445\n",
      " 1230385/3750000: episode: 1735, duration: 6.487s, episode steps: 857, steps per second: 132, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.019876, mae: 3.268260, mean_q: 3.935029, mean_eps: 0.557214\n",
      " 1230737/3750000: episode: 1736, duration: 2.614s, episode steps: 352, steps per second: 135, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.015205, mae: 3.275049, mean_q: 3.938813, mean_eps: 0.556998\n",
      " 1231622/3750000: episode: 1737, duration: 6.788s, episode steps: 885, steps per second: 130, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.021687, mae: 3.295151, mean_q: 3.961365, mean_eps: 0.556775\n",
      " 1232112/3750000: episode: 1738, duration: 3.723s, episode steps: 490, steps per second: 132, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.018454, mae: 3.270023, mean_q: 3.937831, mean_eps: 0.556527\n",
      " 1232805/3750000: episode: 1739, duration: 5.219s, episode steps: 693, steps per second: 133, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.018920, mae: 3.303667, mean_q: 3.976017, mean_eps: 0.556314\n",
      " 1233678/3750000: episode: 1740, duration: 6.501s, episode steps: 873, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.016687, mae: 3.283582, mean_q: 3.954779, mean_eps: 0.556034\n",
      " 1234442/3750000: episode: 1741, duration: 5.846s, episode steps: 764, steps per second: 131, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.017026, mae: 3.280406, mean_q: 3.947669, mean_eps: 0.555738\n",
      " 1235391/3750000: episode: 1742, duration: 7.088s, episode steps: 949, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.015642, mae: 3.275470, mean_q: 3.940323, mean_eps: 0.555429\n",
      " 1235918/3750000: episode: 1743, duration: 3.939s, episode steps: 527, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.017569, mae: 3.347717, mean_q: 4.032823, mean_eps: 0.555166\n",
      " 1236355/3750000: episode: 1744, duration: 3.331s, episode steps: 437, steps per second: 131, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.661 [0.000, 5.000],  loss: 0.011925, mae: 3.325747, mean_q: 4.008229, mean_eps: 0.554993\n",
      " 1236875/3750000: episode: 1745, duration: 3.905s, episode steps: 520, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.690 [0.000, 5.000],  loss: 0.020125, mae: 3.321517, mean_q: 3.996813, mean_eps: 0.554820\n",
      " 1237367/3750000: episode: 1746, duration: 3.729s, episode steps: 492, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.020016, mae: 3.314972, mean_q: 3.987858, mean_eps: 0.554637\n",
      " 1238050/3750000: episode: 1747, duration: 5.106s, episode steps: 683, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.833 [0.000, 5.000],  loss: 0.020037, mae: 3.293397, mean_q: 3.960250, mean_eps: 0.554424\n",
      " 1238904/3750000: episode: 1748, duration: 6.490s, episode steps: 854, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.014532, mae: 3.310856, mean_q: 3.980003, mean_eps: 0.554147\n",
      " 1239906/3750000: episode: 1749, duration: 7.485s, episode steps: 1002, steps per second: 134, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.017808, mae: 3.264056, mean_q: 3.923698, mean_eps: 0.553812\n",
      " 1240342/3750000: episode: 1750, duration: 3.361s, episode steps: 436, steps per second: 130, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.016549, mae: 3.304799, mean_q: 3.974448, mean_eps: 0.553553\n",
      " 1241212/3750000: episode: 1751, duration: 6.459s, episode steps: 870, steps per second: 135, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.017543, mae: 3.263162, mean_q: 3.935588, mean_eps: 0.553319\n",
      " 1241753/3750000: episode: 1752, duration: 4.042s, episode steps: 541, steps per second: 134, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.020635, mae: 3.265608, mean_q: 3.924461, mean_eps: 0.553067\n",
      " 1242487/3750000: episode: 1753, duration: 5.562s, episode steps: 734, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.018758, mae: 3.265561, mean_q: 3.927110, mean_eps: 0.552837\n",
      " 1243368/3750000: episode: 1754, duration: 6.661s, episode steps: 881, steps per second: 132, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.024109, mae: 3.257679, mean_q: 3.914358, mean_eps: 0.552545\n",
      " 1243922/3750000: episode: 1755, duration: 4.183s, episode steps: 554, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.016887, mae: 3.288305, mean_q: 3.954409, mean_eps: 0.552286\n",
      " 1244797/3750000: episode: 1756, duration: 6.534s, episode steps: 875, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.017808, mae: 3.249265, mean_q: 3.906923, mean_eps: 0.552030\n",
      " 1245299/3750000: episode: 1757, duration: 3.784s, episode steps: 502, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.014958, mae: 3.239841, mean_q: 3.899264, mean_eps: 0.551786\n",
      " 1246317/3750000: episode: 1758, duration: 7.634s, episode steps: 1018, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.019088, mae: 3.281951, mean_q: 3.947822, mean_eps: 0.551512\n",
      " 1246987/3750000: episode: 1759, duration: 5.174s, episode steps: 670, steps per second: 129, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.022326, mae: 3.272521, mean_q: 3.936087, mean_eps: 0.551206\n",
      " 1247764/3750000: episode: 1760, duration: 5.920s, episode steps: 777, steps per second: 131, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.017546, mae: 3.250783, mean_q: 3.913197, mean_eps: 0.550943\n",
      " 1248456/3750000: episode: 1761, duration: 5.134s, episode steps: 692, steps per second: 135, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.015873, mae: 3.318655, mean_q: 3.997820, mean_eps: 0.550680\n",
      " 1249556/3750000: episode: 1762, duration: 8.298s, episode steps: 1100, steps per second: 133, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.016015, mae: 3.276141, mean_q: 3.941130, mean_eps: 0.550360\n",
      " 1250588/3750000: episode: 1763, duration: 7.859s, episode steps: 1032, steps per second: 131, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.016228, mae: 3.275212, mean_q: 3.944444, mean_eps: 0.549975\n",
      " 1251129/3750000: episode: 1764, duration: 4.119s, episode steps: 541, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.016079, mae: 3.303115, mean_q: 3.976682, mean_eps: 0.549690\n",
      " 1251649/3750000: episode: 1765, duration: 3.932s, episode steps: 520, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.014134, mae: 3.265745, mean_q: 3.927130, mean_eps: 0.549500\n",
      " 1252167/3750000: episode: 1766, duration: 3.893s, episode steps: 518, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.015233, mae: 3.287621, mean_q: 3.956824, mean_eps: 0.549312\n",
      " 1252550/3750000: episode: 1767, duration: 2.851s, episode steps: 383, steps per second: 134, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013571, mae: 3.241982, mean_q: 3.903008, mean_eps: 0.549150\n",
      " 1253018/3750000: episode: 1768, duration: 3.517s, episode steps: 468, steps per second: 133, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.016139, mae: 3.282303, mean_q: 3.944869, mean_eps: 0.548999\n",
      " 1254043/3750000: episode: 1769, duration: 7.724s, episode steps: 1025, steps per second: 133, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.015063, mae: 3.274539, mean_q: 3.938067, mean_eps: 0.548729\n",
      " 1255446/3750000: episode: 1770, duration: 10.534s, episode steps: 1403, steps per second: 133, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.017484, mae: 3.255912, mean_q: 3.921863, mean_eps: 0.548290\n",
      " 1256406/3750000: episode: 1771, duration: 7.242s, episode steps: 960, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.020516, mae: 3.266298, mean_q: 3.931314, mean_eps: 0.547865\n",
      " 1256910/3750000: episode: 1772, duration: 3.804s, episode steps: 504, steps per second: 133, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.022185, mae: 3.212785, mean_q: 3.867072, mean_eps: 0.547602\n",
      " 1257813/3750000: episode: 1773, duration: 7.024s, episode steps: 903, steps per second: 129, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.015733, mae: 3.298471, mean_q: 3.967181, mean_eps: 0.547350\n",
      " 1258486/3750000: episode: 1774, duration: 5.132s, episode steps: 673, steps per second: 131, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.108 [0.000, 5.000],  loss: 0.019385, mae: 3.262182, mean_q: 3.930477, mean_eps: 0.547066\n",
      " 1259551/3750000: episode: 1775, duration: 8.001s, episode steps: 1065, steps per second: 133, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.018677, mae: 3.246669, mean_q: 3.904317, mean_eps: 0.546753\n",
      " 1259986/3750000: episode: 1776, duration: 3.300s, episode steps: 435, steps per second: 132, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.018467, mae: 3.261223, mean_q: 3.916981, mean_eps: 0.546483\n",
      " 1260472/3750000: episode: 1777, duration: 3.612s, episode steps: 486, steps per second: 135, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015321, mae: 3.284725, mean_q: 3.945200, mean_eps: 0.546317\n",
      " 1260928/3750000: episode: 1778, duration: 3.448s, episode steps: 456, steps per second: 132, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.011067, mae: 3.300377, mean_q: 3.969967, mean_eps: 0.546148\n",
      " 1261530/3750000: episode: 1779, duration: 4.568s, episode steps: 602, steps per second: 132, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.018481, mae: 3.281238, mean_q: 3.950083, mean_eps: 0.545957\n",
      " 1261929/3750000: episode: 1780, duration: 2.991s, episode steps: 399, steps per second: 133, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.015030, mae: 3.274923, mean_q: 3.941859, mean_eps: 0.545777\n",
      " 1262421/3750000: episode: 1781, duration: 3.721s, episode steps: 492, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.018925, mae: 3.280197, mean_q: 3.942564, mean_eps: 0.545615\n",
      " 1263040/3750000: episode: 1782, duration: 4.669s, episode steps: 619, steps per second: 133, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.018193, mae: 3.219394, mean_q: 3.872148, mean_eps: 0.545417\n",
      " 1263499/3750000: episode: 1783, duration: 3.483s, episode steps: 459, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.059 [0.000, 5.000],  loss: 0.016148, mae: 3.243099, mean_q: 3.902928, mean_eps: 0.545226\n",
      " 1263845/3750000: episode: 1784, duration: 2.703s, episode steps: 346, steps per second: 128, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.012793, mae: 3.211852, mean_q: 3.865004, mean_eps: 0.545079\n",
      " 1264580/3750000: episode: 1785, duration: 5.538s, episode steps: 735, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.725 [0.000, 5.000],  loss: 0.016011, mae: 3.281066, mean_q: 3.947339, mean_eps: 0.544884\n",
      " 1265463/3750000: episode: 1786, duration: 6.698s, episode steps: 883, steps per second: 132, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.021546, mae: 3.236793, mean_q: 3.894325, mean_eps: 0.544593\n",
      " 1266153/3750000: episode: 1787, duration: 5.195s, episode steps: 690, steps per second: 133, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.945 [0.000, 5.000],  loss: 0.019867, mae: 3.343079, mean_q: 4.016975, mean_eps: 0.544308\n",
      " 1266656/3750000: episode: 1788, duration: 3.795s, episode steps: 503, steps per second: 133, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.016475, mae: 3.352893, mean_q: 4.036428, mean_eps: 0.544096\n",
      " 1267169/3750000: episode: 1789, duration: 3.903s, episode steps: 513, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.710 [0.000, 5.000],  loss: 0.015959, mae: 3.354765, mean_q: 4.038831, mean_eps: 0.543912\n",
      " 1267878/3750000: episode: 1790, duration: 5.313s, episode steps: 709, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011653, mae: 3.314341, mean_q: 3.987374, mean_eps: 0.543693\n",
      " 1268377/3750000: episode: 1791, duration: 3.730s, episode steps: 499, steps per second: 134, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.856 [0.000, 5.000],  loss: 0.016848, mae: 3.384024, mean_q: 4.074577, mean_eps: 0.543477\n",
      " 1268751/3750000: episode: 1792, duration: 2.839s, episode steps: 374, steps per second: 132, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.012722, mae: 3.311037, mean_q: 3.983207, mean_eps: 0.543318\n",
      " 1269444/3750000: episode: 1793, duration: 5.224s, episode steps: 693, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.014575, mae: 3.355944, mean_q: 4.038835, mean_eps: 0.543124\n",
      " 1269977/3750000: episode: 1794, duration: 3.964s, episode steps: 533, steps per second: 134, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.013967, mae: 3.315846, mean_q: 3.987629, mean_eps: 0.542904\n",
      " 1270807/3750000: episode: 1795, duration: 6.284s, episode steps: 830, steps per second: 132, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.011872, mae: 3.368212, mean_q: 4.049875, mean_eps: 0.542660\n",
      " 1271669/3750000: episode: 1796, duration: 6.455s, episode steps: 862, steps per second: 134, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.023540, mae: 3.324095, mean_q: 3.999476, mean_eps: 0.542354\n",
      " 1272163/3750000: episode: 1797, duration: 3.782s, episode steps: 494, steps per second: 131, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.020382, mae: 3.374550, mean_q: 4.052986, mean_eps: 0.542109\n",
      " 1272830/3750000: episode: 1798, duration: 4.959s, episode steps: 667, steps per second: 135, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.015764, mae: 3.333219, mean_q: 4.004221, mean_eps: 0.541900\n",
      " 1273769/3750000: episode: 1799, duration: 7.046s, episode steps: 939, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.014735, mae: 3.319227, mean_q: 3.990195, mean_eps: 0.541612\n",
      " 1274627/3750000: episode: 1800, duration: 6.476s, episode steps: 858, steps per second: 132, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.018261, mae: 3.350393, mean_q: 4.030178, mean_eps: 0.541288\n",
      " 1275389/3750000: episode: 1801, duration: 5.802s, episode steps: 762, steps per second: 131, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.012996, mae: 3.268689, mean_q: 3.931278, mean_eps: 0.540996\n",
      " 1276019/3750000: episode: 1802, duration: 4.721s, episode steps: 630, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.018894, mae: 3.322985, mean_q: 3.993127, mean_eps: 0.540748\n",
      " 1276741/3750000: episode: 1803, duration: 5.555s, episode steps: 722, steps per second: 130, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.018987, mae: 3.320567, mean_q: 3.995765, mean_eps: 0.540503\n",
      " 1277685/3750000: episode: 1804, duration: 7.069s, episode steps: 944, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.021299, mae: 3.298227, mean_q: 3.974021, mean_eps: 0.540201\n",
      " 1278396/3750000: episode: 1805, duration: 5.295s, episode steps: 711, steps per second: 134, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.023676, mae: 3.315340, mean_q: 3.989251, mean_eps: 0.539906\n",
      " 1279179/3750000: episode: 1806, duration: 5.918s, episode steps: 783, steps per second: 132, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.024675, mae: 3.292404, mean_q: 3.957723, mean_eps: 0.539639\n",
      " 1280189/3750000: episode: 1807, duration: 7.579s, episode steps: 1010, steps per second: 133, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.017997, mae: 3.327834, mean_q: 4.005213, mean_eps: 0.539315\n",
      " 1281594/3750000: episode: 1808, duration: 10.561s, episode steps: 1405, steps per second: 133, episode reward: 16.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.017255, mae: 3.295361, mean_q: 3.963900, mean_eps: 0.538880\n",
      " 1282217/3750000: episode: 1809, duration: 4.651s, episode steps: 623, steps per second: 134, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.014572, mae: 3.272281, mean_q: 3.932629, mean_eps: 0.538516\n",
      " 1282739/3750000: episode: 1810, duration: 3.930s, episode steps: 522, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.016483, mae: 3.311421, mean_q: 3.989713, mean_eps: 0.538311\n",
      " 1283771/3750000: episode: 1811, duration: 7.710s, episode steps: 1032, steps per second: 134, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.018864, mae: 3.279343, mean_q: 3.943147, mean_eps: 0.538030\n",
      " 1284762/3750000: episode: 1812, duration: 7.489s, episode steps: 991, steps per second: 132, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.014104, mae: 3.283188, mean_q: 3.954078, mean_eps: 0.537663\n",
      " 1285578/3750000: episode: 1813, duration: 6.182s, episode steps: 816, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.019868, mae: 3.313986, mean_q: 3.988143, mean_eps: 0.537339\n",
      " 1286294/3750000: episode: 1814, duration: 5.476s, episode steps: 716, steps per second: 131, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.013971, mae: 3.295274, mean_q: 3.965342, mean_eps: 0.537065\n",
      " 1286933/3750000: episode: 1815, duration: 4.914s, episode steps: 639, steps per second: 130, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.019584, mae: 3.303311, mean_q: 3.976378, mean_eps: 0.536820\n",
      " 1287950/3750000: episode: 1816, duration: 7.692s, episode steps: 1017, steps per second: 132, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.017143, mae: 3.303816, mean_q: 3.978818, mean_eps: 0.536522\n",
      " 1288846/3750000: episode: 1817, duration: 6.736s, episode steps: 896, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.014753, mae: 3.301498, mean_q: 3.978255, mean_eps: 0.536176\n",
      " 1289514/3750000: episode: 1818, duration: 5.053s, episode steps: 668, steps per second: 132, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.017529, mae: 3.275691, mean_q: 3.939833, mean_eps: 0.535895\n",
      " 1290081/3750000: episode: 1819, duration: 4.329s, episode steps: 567, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.016538, mae: 3.269197, mean_q: 3.937667, mean_eps: 0.535672\n",
      " 1290459/3750000: episode: 1820, duration: 2.766s, episode steps: 378, steps per second: 137, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.012359, mae: 3.312442, mean_q: 3.982285, mean_eps: 0.535503\n",
      " 1291538/3750000: episode: 1821, duration: 8.109s, episode steps: 1079, steps per second: 133, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.018131, mae: 3.284309, mean_q: 3.950363, mean_eps: 0.535244\n",
      " 1292062/3750000: episode: 1822, duration: 3.990s, episode steps: 524, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.016009, mae: 3.222483, mean_q: 3.880949, mean_eps: 0.534952\n",
      " 1292610/3750000: episode: 1823, duration: 4.061s, episode steps: 548, steps per second: 135, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.015104, mae: 3.264156, mean_q: 3.923718, mean_eps: 0.534758\n",
      " 1293276/3750000: episode: 1824, duration: 4.994s, episode steps: 666, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.167 [0.000, 5.000],  loss: 0.018154, mae: 3.269631, mean_q: 3.933551, mean_eps: 0.534542\n",
      " 1293993/3750000: episode: 1825, duration: 5.432s, episode steps: 717, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.013102, mae: 3.287489, mean_q: 3.957646, mean_eps: 0.534293\n",
      " 1294739/3750000: episode: 1826, duration: 5.570s, episode steps: 746, steps per second: 134, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.015672, mae: 3.210262, mean_q: 3.865904, mean_eps: 0.534030\n",
      " 1295401/3750000: episode: 1827, duration: 5.145s, episode steps: 662, steps per second: 129, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.013764, mae: 3.245946, mean_q: 3.905939, mean_eps: 0.533775\n",
      " 1296135/3750000: episode: 1828, duration: 5.493s, episode steps: 734, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.014901, mae: 3.225501, mean_q: 3.879666, mean_eps: 0.533523\n",
      " 1296505/3750000: episode: 1829, duration: 2.829s, episode steps: 370, steps per second: 131, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.014251, mae: 3.188285, mean_q: 3.832302, mean_eps: 0.533325\n",
      " 1297311/3750000: episode: 1830, duration: 6.040s, episode steps: 806, steps per second: 133, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.013448, mae: 3.219785, mean_q: 3.870806, mean_eps: 0.533112\n",
      " 1297938/3750000: episode: 1831, duration: 4.734s, episode steps: 627, steps per second: 132, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.016741, mae: 3.225427, mean_q: 3.881546, mean_eps: 0.532857\n",
      " 1298916/3750000: episode: 1832, duration: 7.358s, episode steps: 978, steps per second: 133, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.018374, mae: 3.258526, mean_q: 3.922852, mean_eps: 0.532569\n",
      " 1299324/3750000: episode: 1833, duration: 3.109s, episode steps: 408, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.019794, mae: 3.230496, mean_q: 3.882244, mean_eps: 0.532317\n",
      " 1300343/3750000: episode: 1834, duration: 7.689s, episode steps: 1019, steps per second: 133, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.019252, mae: 3.213095, mean_q: 3.863250, mean_eps: 0.532058\n",
      " 1301023/3750000: episode: 1835, duration: 5.137s, episode steps: 680, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.020981, mae: 3.192670, mean_q: 3.839687, mean_eps: 0.531752\n",
      " 1301915/3750000: episode: 1836, duration: 6.689s, episode steps: 892, steps per second: 133, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.015892, mae: 3.219105, mean_q: 3.879219, mean_eps: 0.531471\n",
      " 1302456/3750000: episode: 1837, duration: 4.051s, episode steps: 541, steps per second: 134, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.019065, mae: 3.215646, mean_q: 3.878501, mean_eps: 0.531215\n",
      " 1303141/3750000: episode: 1838, duration: 5.195s, episode steps: 685, steps per second: 132, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.013945, mae: 3.264045, mean_q: 3.934866, mean_eps: 0.530992\n",
      " 1304029/3750000: episode: 1839, duration: 6.680s, episode steps: 888, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.017768, mae: 3.168633, mean_q: 3.811407, mean_eps: 0.530708\n",
      " 1304929/3750000: episode: 1840, duration: 6.779s, episode steps: 900, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.017232, mae: 3.159113, mean_q: 3.799782, mean_eps: 0.530387\n",
      " 1305607/3750000: episode: 1841, duration: 5.095s, episode steps: 678, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.014965, mae: 3.157216, mean_q: 3.799396, mean_eps: 0.530103\n",
      " 1306559/3750000: episode: 1842, duration: 7.155s, episode steps: 952, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.015468, mae: 3.144844, mean_q: 3.783981, mean_eps: 0.529811\n",
      " 1306913/3750000: episode: 1843, duration: 2.653s, episode steps: 354, steps per second: 133, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.013723, mae: 3.136059, mean_q: 3.776785, mean_eps: 0.529577\n",
      " 1307670/3750000: episode: 1844, duration: 5.674s, episode steps: 757, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.014529, mae: 3.105836, mean_q: 3.737012, mean_eps: 0.529376\n",
      " 1308155/3750000: episode: 1845, duration: 3.648s, episode steps: 485, steps per second: 133, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.012566, mae: 3.133971, mean_q: 3.770122, mean_eps: 0.529152\n",
      " 1309015/3750000: episode: 1846, duration: 6.489s, episode steps: 860, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.016006, mae: 3.127167, mean_q: 3.757834, mean_eps: 0.528911\n",
      " 1309991/3750000: episode: 1847, duration: 7.336s, episode steps: 976, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.012141, mae: 3.120066, mean_q: 3.756861, mean_eps: 0.528580\n",
      " 1311175/3750000: episode: 1848, duration: 8.935s, episode steps: 1184, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.022626, mae: 3.139117, mean_q: 3.775903, mean_eps: 0.528191\n",
      " 1311901/3750000: episode: 1849, duration: 5.510s, episode steps: 726, steps per second: 132, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.018341, mae: 3.090818, mean_q: 3.718425, mean_eps: 0.527846\n",
      " 1312408/3750000: episode: 1850, duration: 3.792s, episode steps: 507, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.016920, mae: 3.100826, mean_q: 3.729334, mean_eps: 0.527622\n",
      " 1312776/3750000: episode: 1851, duration: 2.780s, episode steps: 368, steps per second: 132, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.015325, mae: 3.137278, mean_q: 3.779500, mean_eps: 0.527468\n",
      " 1313334/3750000: episode: 1852, duration: 4.217s, episode steps: 558, steps per second: 132, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.011855, mae: 3.163689, mean_q: 3.810778, mean_eps: 0.527302\n",
      " 1314269/3750000: episode: 1853, duration: 7.020s, episode steps: 935, steps per second: 133, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.012346, mae: 3.101099, mean_q: 3.730839, mean_eps: 0.527032\n",
      " 1314759/3750000: episode: 1854, duration: 3.684s, episode steps: 490, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.019451, mae: 3.083234, mean_q: 3.712082, mean_eps: 0.526776\n",
      " 1315774/3750000: episode: 1855, duration: 7.634s, episode steps: 1015, steps per second: 133, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.020669, mae: 3.085580, mean_q: 3.709705, mean_eps: 0.526506\n",
      " 1316673/3750000: episode: 1856, duration: 6.769s, episode steps: 899, steps per second: 133, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.015747, mae: 3.072466, mean_q: 3.697254, mean_eps: 0.526161\n",
      " 1317208/3750000: episode: 1857, duration: 4.082s, episode steps: 535, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.013405, mae: 3.078142, mean_q: 3.703590, mean_eps: 0.525902\n",
      " 1317838/3750000: episode: 1858, duration: 4.666s, episode steps: 630, steps per second: 135, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.892 [0.000, 5.000],  loss: 0.023241, mae: 3.079567, mean_q: 3.704287, mean_eps: 0.525693\n",
      " 1319455/3750000: episode: 1859, duration: 12.083s, episode steps: 1617, steps per second: 134, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.016714, mae: 3.105907, mean_q: 3.736828, mean_eps: 0.525290\n",
      " 1320374/3750000: episode: 1860, duration: 6.886s, episode steps: 919, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.019250, mae: 3.095523, mean_q: 3.730185, mean_eps: 0.524832\n",
      " 1321073/3750000: episode: 1861, duration: 5.355s, episode steps: 699, steps per second: 131, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.014314, mae: 3.094596, mean_q: 3.718143, mean_eps: 0.524541\n",
      " 1321877/3750000: episode: 1862, duration: 5.985s, episode steps: 804, steps per second: 134, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.170 [0.000, 5.000],  loss: 0.017957, mae: 3.067053, mean_q: 3.688677, mean_eps: 0.524271\n",
      " 1322365/3750000: episode: 1863, duration: 3.711s, episode steps: 488, steps per second: 132, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.010810, mae: 3.126667, mean_q: 3.761917, mean_eps: 0.524037\n",
      " 1323363/3750000: episode: 1864, duration: 7.573s, episode steps: 998, steps per second: 132, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.024276, mae: 3.094470, mean_q: 3.721109, mean_eps: 0.523767\n",
      " 1324160/3750000: episode: 1865, duration: 6.005s, episode steps: 797, steps per second: 133, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 0.013171, mae: 3.109319, mean_q: 3.750694, mean_eps: 0.523446\n",
      " 1324498/3750000: episode: 1866, duration: 2.541s, episode steps: 338, steps per second: 133, episode reward:  7.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.014312, mae: 3.151671, mean_q: 3.789882, mean_eps: 0.523245\n",
      " 1325175/3750000: episode: 1867, duration: 5.116s, episode steps: 677, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.015644, mae: 3.105187, mean_q: 3.731407, mean_eps: 0.523061\n",
      " 1325732/3750000: episode: 1868, duration: 4.214s, episode steps: 557, steps per second: 132, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.014888, mae: 3.099053, mean_q: 3.724530, mean_eps: 0.522838\n",
      " 1326659/3750000: episode: 1869, duration: 6.970s, episode steps: 927, steps per second: 133, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.014148, mae: 3.123967, mean_q: 3.762751, mean_eps: 0.522572\n",
      " 1327235/3750000: episode: 1870, duration: 4.392s, episode steps: 576, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.726 [0.000, 5.000],  loss: 0.014784, mae: 3.135503, mean_q: 3.780035, mean_eps: 0.522302\n",
      " 1328031/3750000: episode: 1871, duration: 6.146s, episode steps: 796, steps per second: 130, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.014132, mae: 3.127399, mean_q: 3.762027, mean_eps: 0.522053\n",
      " 1328520/3750000: episode: 1872, duration: 3.730s, episode steps: 489, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.014772, mae: 3.165860, mean_q: 3.816116, mean_eps: 0.521823\n",
      " 1329374/3750000: episode: 1873, duration: 6.456s, episode steps: 854, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.015844, mae: 3.118696, mean_q: 3.752367, mean_eps: 0.521582\n",
      " 1329816/3750000: episode: 1874, duration: 3.341s, episode steps: 442, steps per second: 132, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.020223, mae: 3.155031, mean_q: 3.792521, mean_eps: 0.521348\n",
      " 1330295/3750000: episode: 1875, duration: 3.625s, episode steps: 479, steps per second: 132, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.018781, mae: 3.181661, mean_q: 3.832407, mean_eps: 0.521182\n",
      " 1330709/3750000: episode: 1876, duration: 3.155s, episode steps: 414, steps per second: 131, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.014964, mae: 3.151818, mean_q: 3.798880, mean_eps: 0.521020\n",
      " 1331765/3750000: episode: 1877, duration: 7.995s, episode steps: 1056, steps per second: 132, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.014381, mae: 3.109453, mean_q: 3.742440, mean_eps: 0.520754\n",
      " 1332420/3750000: episode: 1878, duration: 4.934s, episode steps: 655, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.016288, mae: 3.167781, mean_q: 3.824959, mean_eps: 0.520448\n",
      " 1332768/3750000: episode: 1879, duration: 2.667s, episode steps: 348, steps per second: 130, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.015526, mae: 3.182905, mean_q: 3.839215, mean_eps: 0.520268\n",
      " 1333469/3750000: episode: 1880, duration: 5.285s, episode steps: 701, steps per second: 133, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.018946, mae: 3.214181, mean_q: 3.867998, mean_eps: 0.520077\n",
      " 1334326/3750000: episode: 1881, duration: 6.489s, episode steps: 857, steps per second: 132, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.016400, mae: 3.195584, mean_q: 3.846386, mean_eps: 0.519796\n",
      " 1334678/3750000: episode: 1882, duration: 2.581s, episode steps: 352, steps per second: 136, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.020957, mae: 3.262179, mean_q: 3.920207, mean_eps: 0.519580\n",
      " 1335360/3750000: episode: 1883, duration: 5.219s, episode steps: 682, steps per second: 131, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.021905, mae: 3.175744, mean_q: 3.819238, mean_eps: 0.519396\n",
      " 1336203/3750000: episode: 1884, duration: 6.471s, episode steps: 843, steps per second: 130, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.019206, mae: 3.157924, mean_q: 3.802118, mean_eps: 0.519119\n",
      " 1336700/3750000: episode: 1885, duration: 3.766s, episode steps: 497, steps per second: 132, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.020668, mae: 3.136056, mean_q: 3.770356, mean_eps: 0.518878\n",
      " 1337675/3750000: episode: 1886, duration: 7.327s, episode steps: 975, steps per second: 133, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.017333, mae: 3.138326, mean_q: 3.780344, mean_eps: 0.518615\n",
      " 1338702/3750000: episode: 1887, duration: 7.837s, episode steps: 1027, steps per second: 131, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.014221, mae: 3.163385, mean_q: 3.809561, mean_eps: 0.518252\n",
      " 1339764/3750000: episode: 1888, duration: 7.927s, episode steps: 1062, steps per second: 134, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.014794, mae: 3.137051, mean_q: 3.787966, mean_eps: 0.517874\n",
      " 1340434/3750000: episode: 1889, duration: 5.077s, episode steps: 670, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.014719, mae: 3.163493, mean_q: 3.817169, mean_eps: 0.517564\n",
      " 1340742/3750000: episode: 1890, duration: 2.366s, episode steps: 308, steps per second: 130, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.013346, mae: 3.097408, mean_q: 3.728868, mean_eps: 0.517388\n",
      " 1341450/3750000: episode: 1891, duration: 5.289s, episode steps: 708, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.015533, mae: 3.135830, mean_q: 3.776295, mean_eps: 0.517204\n",
      " 1342154/3750000: episode: 1892, duration: 5.319s, episode steps: 704, steps per second: 132, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.949 [0.000, 5.000],  loss: 0.019456, mae: 3.176354, mean_q: 3.826878, mean_eps: 0.516952\n",
      " 1342923/3750000: episode: 1893, duration: 5.857s, episode steps: 769, steps per second: 131, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.020400, mae: 3.099754, mean_q: 3.734413, mean_eps: 0.516686\n",
      " 1343647/3750000: episode: 1894, duration: 5.421s, episode steps: 724, steps per second: 134, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.018420, mae: 3.111955, mean_q: 3.746429, mean_eps: 0.516416\n",
      " 1344290/3750000: episode: 1895, duration: 4.846s, episode steps: 643, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.586 [0.000, 5.000],  loss: 0.013063, mae: 3.086130, mean_q: 3.729648, mean_eps: 0.516171\n",
      " 1345064/3750000: episode: 1896, duration: 5.829s, episode steps: 774, steps per second: 133, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.017641, mae: 3.117631, mean_q: 3.753895, mean_eps: 0.515915\n",
      " 1345806/3750000: episode: 1897, duration: 5.545s, episode steps: 742, steps per second: 134, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.015843, mae: 3.104798, mean_q: 3.737400, mean_eps: 0.515642\n",
      " 1346728/3750000: episode: 1898, duration: 6.957s, episode steps: 922, steps per second: 133, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.022820, mae: 3.103115, mean_q: 3.732631, mean_eps: 0.515343\n",
      " 1347419/3750000: episode: 1899, duration: 5.117s, episode steps: 691, steps per second: 135, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012678, mae: 3.102393, mean_q: 3.733945, mean_eps: 0.515055\n",
      " 1348292/3750000: episode: 1900, duration: 6.574s, episode steps: 873, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.015910, mae: 3.107897, mean_q: 3.742675, mean_eps: 0.514774\n",
      " 1349058/3750000: episode: 1901, duration: 5.787s, episode steps: 766, steps per second: 132, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.021207, mae: 3.042151, mean_q: 3.661209, mean_eps: 0.514479\n",
      " 1349593/3750000: episode: 1902, duration: 4.083s, episode steps: 535, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.018213, mae: 3.135561, mean_q: 3.774793, mean_eps: 0.514245\n",
      " 1350698/3750000: episode: 1903, duration: 8.312s, episode steps: 1105, steps per second: 133, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.018118, mae: 3.130268, mean_q: 3.766746, mean_eps: 0.513950\n",
      " 1351333/3750000: episode: 1904, duration: 4.804s, episode steps: 635, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.018429, mae: 3.104806, mean_q: 3.742958, mean_eps: 0.513636\n",
      " 1351977/3750000: episode: 1905, duration: 4.791s, episode steps: 644, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.172 [0.000, 5.000],  loss: 0.014146, mae: 3.124965, mean_q: 3.772993, mean_eps: 0.513406\n",
      " 1352821/3750000: episode: 1906, duration: 6.459s, episode steps: 844, steps per second: 131, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014533, mae: 3.126469, mean_q: 3.765863, mean_eps: 0.513136\n",
      " 1353899/3750000: episode: 1907, duration: 7.986s, episode steps: 1078, steps per second: 135, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.018246, mae: 3.132000, mean_q: 3.770517, mean_eps: 0.512790\n",
      " 1354533/3750000: episode: 1908, duration: 4.790s, episode steps: 634, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014641, mae: 3.191183, mean_q: 3.839678, mean_eps: 0.512484\n",
      " 1355241/3750000: episode: 1909, duration: 5.422s, episode steps: 708, steps per second: 131, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.015310, mae: 3.143311, mean_q: 3.782117, mean_eps: 0.512240\n",
      " 1355831/3750000: episode: 1910, duration: 4.375s, episode steps: 590, steps per second: 135, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.019682, mae: 3.164509, mean_q: 3.803334, mean_eps: 0.512006\n",
      " 1356334/3750000: episode: 1911, duration: 3.773s, episode steps: 503, steps per second: 133, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.016004, mae: 3.067129, mean_q: 3.689436, mean_eps: 0.511811\n",
      " 1356899/3750000: episode: 1912, duration: 4.192s, episode steps: 565, steps per second: 135, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.846 [0.000, 5.000],  loss: 0.014328, mae: 3.127100, mean_q: 3.763435, mean_eps: 0.511620\n",
      " 1357642/3750000: episode: 1913, duration: 5.684s, episode steps: 743, steps per second: 131, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.023322, mae: 3.122902, mean_q: 3.758343, mean_eps: 0.511383\n",
      " 1358118/3750000: episode: 1914, duration: 3.493s, episode steps: 476, steps per second: 136, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.019084, mae: 3.125084, mean_q: 3.759144, mean_eps: 0.511163\n",
      " 1358591/3750000: episode: 1915, duration: 3.616s, episode steps: 473, steps per second: 131, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.012860, mae: 3.128521, mean_q: 3.767060, mean_eps: 0.510994\n",
      " 1359131/3750000: episode: 1916, duration: 4.148s, episode steps: 540, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.016028, mae: 3.098699, mean_q: 3.733072, mean_eps: 0.510810\n",
      " 1360091/3750000: episode: 1917, duration: 7.156s, episode steps: 960, steps per second: 134, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.011488, mae: 3.113954, mean_q: 3.745923, mean_eps: 0.510540\n",
      " 1360938/3750000: episode: 1918, duration: 6.339s, episode steps: 847, steps per second: 134, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.015564, mae: 3.113652, mean_q: 3.746241, mean_eps: 0.510216\n",
      " 1361283/3750000: episode: 1919, duration: 2.690s, episode steps: 345, steps per second: 128, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.017435, mae: 3.072732, mean_q: 3.690781, mean_eps: 0.510000\n",
      " 1362034/3750000: episode: 1920, duration: 5.612s, episode steps: 751, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.013550, mae: 3.100252, mean_q: 3.732483, mean_eps: 0.509802\n",
      " 1362858/3750000: episode: 1921, duration: 6.146s, episode steps: 824, steps per second: 134, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.025812, mae: 3.070256, mean_q: 3.699201, mean_eps: 0.509522\n",
      " 1363739/3750000: episode: 1922, duration: 6.638s, episode steps: 881, steps per second: 133, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.019484, mae: 3.112686, mean_q: 3.746437, mean_eps: 0.509216\n",
      " 1364263/3750000: episode: 1923, duration: 4.003s, episode steps: 524, steps per second: 131, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.012902, mae: 3.087238, mean_q: 3.724516, mean_eps: 0.508960\n",
      " 1365431/3750000: episode: 1924, duration: 8.712s, episode steps: 1168, steps per second: 134, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.015812, mae: 3.067055, mean_q: 3.693771, mean_eps: 0.508654\n",
      " 1366621/3750000: episode: 1925, duration: 8.990s, episode steps: 1190, steps per second: 132, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.014473, mae: 3.091800, mean_q: 3.717532, mean_eps: 0.508229\n",
      " 1367023/3750000: episode: 1926, duration: 3.019s, episode steps: 402, steps per second: 133, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.928 [0.000, 5.000],  loss: 0.013365, mae: 3.072536, mean_q: 3.698644, mean_eps: 0.507941\n",
      " 1367526/3750000: episode: 1927, duration: 3.802s, episode steps: 503, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.789 [0.000, 5.000],  loss: 0.019854, mae: 3.157401, mean_q: 3.800964, mean_eps: 0.507779\n",
      " 1367971/3750000: episode: 1928, duration: 3.377s, episode steps: 445, steps per second: 132, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.009596, mae: 3.119262, mean_q: 3.758128, mean_eps: 0.507610\n",
      " 1368909/3750000: episode: 1929, duration: 7.056s, episode steps: 938, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.013455, mae: 3.029883, mean_q: 3.642405, mean_eps: 0.507362\n",
      " 1369572/3750000: episode: 1930, duration: 4.940s, episode steps: 663, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.018582, mae: 3.108004, mean_q: 3.736897, mean_eps: 0.507074\n",
      " 1370105/3750000: episode: 1931, duration: 4.085s, episode steps: 533, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.011412, mae: 3.036005, mean_q: 3.656620, mean_eps: 0.506858\n",
      " 1370754/3750000: episode: 1932, duration: 4.826s, episode steps: 649, steps per second: 134, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.012813, mae: 3.000466, mean_q: 3.606650, mean_eps: 0.506645\n",
      " 1371872/3750000: episode: 1933, duration: 8.417s, episode steps: 1118, steps per second: 133, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.013989, mae: 3.056807, mean_q: 3.679214, mean_eps: 0.506328\n",
      " 1372579/3750000: episode: 1934, duration: 5.408s, episode steps: 707, steps per second: 131, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.849 [0.000, 5.000],  loss: 0.022638, mae: 3.010398, mean_q: 3.621789, mean_eps: 0.506001\n",
      " 1373197/3750000: episode: 1935, duration: 4.760s, episode steps: 618, steps per second: 130, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015762, mae: 3.005045, mean_q: 3.618356, mean_eps: 0.505763\n",
      " 1374164/3750000: episode: 1936, duration: 7.389s, episode steps: 967, steps per second: 131, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015136, mae: 3.040933, mean_q: 3.658586, mean_eps: 0.505475\n",
      " 1375180/3750000: episode: 1937, duration: 7.680s, episode steps: 1016, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.038 [0.000, 5.000],  loss: 0.017148, mae: 3.035854, mean_q: 3.650611, mean_eps: 0.505119\n",
      " 1375945/3750000: episode: 1938, duration: 5.779s, episode steps: 765, steps per second: 132, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.830 [0.000, 5.000],  loss: 0.017691, mae: 3.031914, mean_q: 3.651200, mean_eps: 0.504798\n",
      " 1376922/3750000: episode: 1939, duration: 7.407s, episode steps: 977, steps per second: 132, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.014862, mae: 3.050084, mean_q: 3.673390, mean_eps: 0.504482\n",
      " 1377782/3750000: episode: 1940, duration: 6.413s, episode steps: 860, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.014105, mae: 3.047721, mean_q: 3.668719, mean_eps: 0.504150\n",
      " 1378315/3750000: episode: 1941, duration: 4.096s, episode steps: 533, steps per second: 130, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.020099, mae: 3.046542, mean_q: 3.667911, mean_eps: 0.503902\n",
      " 1378825/3750000: episode: 1942, duration: 3.847s, episode steps: 510, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.014559, mae: 3.028958, mean_q: 3.645763, mean_eps: 0.503715\n",
      " 1379386/3750000: episode: 1943, duration: 4.188s, episode steps: 561, steps per second: 134, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.016621, mae: 3.110177, mean_q: 3.739694, mean_eps: 0.503520\n",
      " 1379771/3750000: episode: 1944, duration: 2.861s, episode steps: 385, steps per second: 135, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.028150, mae: 3.042684, mean_q: 3.664068, mean_eps: 0.503351\n",
      " 1380360/3750000: episode: 1945, duration: 4.507s, episode steps: 589, steps per second: 131, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.018316, mae: 3.092489, mean_q: 3.729713, mean_eps: 0.503178\n",
      " 1381029/3750000: episode: 1946, duration: 5.025s, episode steps: 669, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.013879, mae: 3.057016, mean_q: 3.678900, mean_eps: 0.502952\n",
      " 1381740/3750000: episode: 1947, duration: 5.353s, episode steps: 711, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.013575, mae: 3.110206, mean_q: 3.741676, mean_eps: 0.502703\n",
      " 1382178/3750000: episode: 1948, duration: 3.297s, episode steps: 438, steps per second: 133, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.534 [0.000, 5.000],  loss: 0.017195, mae: 3.061787, mean_q: 3.687285, mean_eps: 0.502498\n",
      " 1382687/3750000: episode: 1949, duration: 3.918s, episode steps: 509, steps per second: 130, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.019962, mae: 3.096967, mean_q: 3.721773, mean_eps: 0.502325\n",
      " 1383332/3750000: episode: 1950, duration: 4.815s, episode steps: 645, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.018406, mae: 3.050075, mean_q: 3.670125, mean_eps: 0.502116\n",
      " 1383837/3750000: episode: 1951, duration: 3.879s, episode steps: 505, steps per second: 130, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.017059, mae: 3.071217, mean_q: 3.692631, mean_eps: 0.501911\n",
      " 1384567/3750000: episode: 1952, duration: 5.582s, episode steps: 730, steps per second: 131, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014894, mae: 3.073674, mean_q: 3.698463, mean_eps: 0.501688\n",
      " 1385413/3750000: episode: 1953, duration: 6.335s, episode steps: 846, steps per second: 134, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.019639, mae: 3.054288, mean_q: 3.670096, mean_eps: 0.501404\n",
      " 1386485/3750000: episode: 1954, duration: 8.076s, episode steps: 1072, steps per second: 133, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.024527, mae: 3.057359, mean_q: 3.676393, mean_eps: 0.501058\n",
      " 1386967/3750000: episode: 1955, duration: 3.833s, episode steps: 482, steps per second: 126, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.017375, mae: 3.106434, mean_q: 3.740599, mean_eps: 0.500777\n",
      " 1387792/3750000: episode: 1956, duration: 6.143s, episode steps: 825, steps per second: 134, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.018447, mae: 3.039785, mean_q: 3.654798, mean_eps: 0.500543\n",
      " 1388161/3750000: episode: 1957, duration: 2.808s, episode steps: 369, steps per second: 131, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.024 [0.000, 5.000],  loss: 0.015286, mae: 3.110749, mean_q: 3.748805, mean_eps: 0.500327\n",
      " 1389078/3750000: episode: 1958, duration: 6.860s, episode steps: 917, steps per second: 134, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.012608, mae: 3.053674, mean_q: 3.672742, mean_eps: 0.500097\n",
      " 1389629/3750000: episode: 1959, duration: 4.192s, episode steps: 551, steps per second: 131, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.514 [0.000, 5.000],  loss: 0.023397, mae: 3.088471, mean_q: 3.717151, mean_eps: 0.499834\n",
      " 1390168/3750000: episode: 1960, duration: 4.082s, episode steps: 539, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.019432, mae: 3.056984, mean_q: 3.674797, mean_eps: 0.499636\n",
      " 1391271/3750000: episode: 1961, duration: 8.432s, episode steps: 1103, steps per second: 131, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.017270, mae: 3.086502, mean_q: 3.709975, mean_eps: 0.499341\n",
      " 1391896/3750000: episode: 1962, duration: 4.662s, episode steps: 625, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.016188, mae: 3.055996, mean_q: 3.684437, mean_eps: 0.499031\n",
      " 1392611/3750000: episode: 1963, duration: 5.448s, episode steps: 715, steps per second: 131, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.017190, mae: 3.067395, mean_q: 3.700037, mean_eps: 0.498790\n",
      " 1393103/3750000: episode: 1964, duration: 3.847s, episode steps: 492, steps per second: 128, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.012704, mae: 3.129645, mean_q: 3.772222, mean_eps: 0.498570\n",
      " 1393761/3750000: episode: 1965, duration: 4.982s, episode steps: 658, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.970 [0.000, 5.000],  loss: 0.011724, mae: 3.065405, mean_q: 3.695745, mean_eps: 0.498362\n",
      " 1394646/3750000: episode: 1966, duration: 6.624s, episode steps: 885, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.014309, mae: 3.091266, mean_q: 3.721048, mean_eps: 0.498084\n",
      " 1395365/3750000: episode: 1967, duration: 5.444s, episode steps: 719, steps per second: 132, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.015886, mae: 3.097110, mean_q: 3.726526, mean_eps: 0.497796\n",
      " 1396294/3750000: episode: 1968, duration: 6.949s, episode steps: 929, steps per second: 134, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.017137, mae: 3.106797, mean_q: 3.740393, mean_eps: 0.497501\n",
      " 1397279/3750000: episode: 1969, duration: 7.410s, episode steps: 985, steps per second: 133, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.019889, mae: 3.122189, mean_q: 3.760532, mean_eps: 0.497159\n",
      " 1398138/3750000: episode: 1970, duration: 6.427s, episode steps: 859, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014043, mae: 3.088715, mean_q: 3.713516, mean_eps: 0.496828\n",
      " 1398579/3750000: episode: 1971, duration: 3.314s, episode steps: 441, steps per second: 133, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.018857, mae: 3.158220, mean_q: 3.799363, mean_eps: 0.496594\n",
      " 1399815/3750000: episode: 1972, duration: 9.310s, episode steps: 1236, steps per second: 133, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.018531, mae: 3.086468, mean_q: 3.713136, mean_eps: 0.496292\n",
      " 1400196/3750000: episode: 1973, duration: 2.880s, episode steps: 381, steps per second: 132, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.012195, mae: 3.124833, mean_q: 3.758774, mean_eps: 0.496000\n",
      " 1400707/3750000: episode: 1974, duration: 3.858s, episode steps: 511, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015298, mae: 3.165560, mean_q: 3.807853, mean_eps: 0.495838\n",
      " 1401582/3750000: episode: 1975, duration: 6.661s, episode steps: 875, steps per second: 131, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.011556, mae: 3.133898, mean_q: 3.769452, mean_eps: 0.495586\n",
      " 1402480/3750000: episode: 1976, duration: 6.819s, episode steps: 898, steps per second: 132, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.015735, mae: 3.162880, mean_q: 3.801875, mean_eps: 0.495269\n",
      " 1403461/3750000: episode: 1977, duration: 7.515s, episode steps: 981, steps per second: 131, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.011264, mae: 3.119459, mean_q: 3.761836, mean_eps: 0.494931\n",
      " 1403988/3750000: episode: 1978, duration: 3.960s, episode steps: 527, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.753 [0.000, 5.000],  loss: 0.016256, mae: 3.142094, mean_q: 3.780117, mean_eps: 0.494657\n",
      " 1404874/3750000: episode: 1979, duration: 6.605s, episode steps: 886, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.018792, mae: 3.160719, mean_q: 3.801997, mean_eps: 0.494405\n",
      " 1405556/3750000: episode: 1980, duration: 5.117s, episode steps: 682, steps per second: 133, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.017393, mae: 3.151897, mean_q: 3.789344, mean_eps: 0.494124\n",
      " 1406832/3750000: episode: 1981, duration: 9.663s, episode steps: 1276, steps per second: 132, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.017069, mae: 3.124654, mean_q: 3.764392, mean_eps: 0.493772\n",
      " 1407745/3750000: episode: 1982, duration: 6.879s, episode steps: 913, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.016507, mae: 3.181201, mean_q: 3.827941, mean_eps: 0.493376\n",
      " 1408112/3750000: episode: 1983, duration: 2.775s, episode steps: 367, steps per second: 132, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.177 [0.000, 5.000],  loss: 0.012912, mae: 3.170312, mean_q: 3.813345, mean_eps: 0.493145\n",
      " 1408767/3750000: episode: 1984, duration: 4.916s, episode steps: 655, steps per second: 133, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.018084, mae: 3.140190, mean_q: 3.776623, mean_eps: 0.492962\n",
      " 1409096/3750000: episode: 1985, duration: 2.481s, episode steps: 329, steps per second: 133, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.745 [0.000, 5.000],  loss: 0.022893, mae: 3.136779, mean_q: 3.776940, mean_eps: 0.492785\n",
      " 1410079/3750000: episode: 1986, duration: 7.377s, episode steps: 983, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.016663, mae: 3.213013, mean_q: 3.866237, mean_eps: 0.492551\n",
      " 1410610/3750000: episode: 1987, duration: 4.015s, episode steps: 531, steps per second: 132, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.017330, mae: 3.179284, mean_q: 3.824943, mean_eps: 0.492278\n",
      " 1411120/3750000: episode: 1988, duration: 3.870s, episode steps: 510, steps per second: 132, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.011020, mae: 3.157386, mean_q: 3.798725, mean_eps: 0.492090\n",
      " 1411834/3750000: episode: 1989, duration: 5.400s, episode steps: 714, steps per second: 132, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.018173, mae: 3.143478, mean_q: 3.778354, mean_eps: 0.491871\n",
      " 1412408/3750000: episode: 1990, duration: 4.391s, episode steps: 574, steps per second: 131, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.014972, mae: 3.169321, mean_q: 3.811080, mean_eps: 0.491637\n",
      " 1413352/3750000: episode: 1991, duration: 7.051s, episode steps: 944, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.587 [0.000, 5.000],  loss: 0.016745, mae: 3.177219, mean_q: 3.820717, mean_eps: 0.491363\n",
      " 1414385/3750000: episode: 1992, duration: 7.794s, episode steps: 1033, steps per second: 133, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.016576, mae: 3.161556, mean_q: 3.804555, mean_eps: 0.491007\n",
      " 1415471/3750000: episode: 1993, duration: 8.105s, episode steps: 1086, steps per second: 134, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.011835, mae: 3.179609, mean_q: 3.829550, mean_eps: 0.490625\n",
      " 1415813/3750000: episode: 1994, duration: 2.565s, episode steps: 342, steps per second: 133, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.014001, mae: 3.177324, mean_q: 3.818857, mean_eps: 0.490370\n",
      " 1416431/3750000: episode: 1995, duration: 4.727s, episode steps: 618, steps per second: 131, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.012295, mae: 3.090531, mean_q: 3.717014, mean_eps: 0.490197\n",
      " 1416863/3750000: episode: 1996, duration: 3.263s, episode steps: 432, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.015315, mae: 3.169194, mean_q: 3.807305, mean_eps: 0.490006\n",
      " 1417293/3750000: episode: 1997, duration: 3.209s, episode steps: 430, steps per second: 134, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.010435, mae: 3.184786, mean_q: 3.833468, mean_eps: 0.489851\n",
      " 1418054/3750000: episode: 1998, duration: 5.663s, episode steps: 761, steps per second: 134, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.899 [0.000, 5.000],  loss: 0.013961, mae: 3.131030, mean_q: 3.767926, mean_eps: 0.489639\n",
      " 1418777/3750000: episode: 1999, duration: 5.453s, episode steps: 723, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.019016, mae: 3.152081, mean_q: 3.795537, mean_eps: 0.489372\n",
      " 1419498/3750000: episode: 2000, duration: 5.422s, episode steps: 721, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.016710, mae: 3.143423, mean_q: 3.784396, mean_eps: 0.489113\n",
      " 1420010/3750000: episode: 2001, duration: 3.932s, episode steps: 512, steps per second: 130, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.017415, mae: 3.139065, mean_q: 3.778049, mean_eps: 0.488890\n",
      " 1420953/3750000: episode: 2002, duration: 7.130s, episode steps: 943, steps per second: 132, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.021315, mae: 3.177187, mean_q: 3.828419, mean_eps: 0.488627\n",
      " 1421375/3750000: episode: 2003, duration: 3.149s, episode steps: 422, steps per second: 134, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.011674, mae: 3.127494, mean_q: 3.761644, mean_eps: 0.488382\n",
      " 1422279/3750000: episode: 2004, duration: 6.758s, episode steps: 904, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.016578, mae: 3.113470, mean_q: 3.746916, mean_eps: 0.488145\n",
      " 1422996/3750000: episode: 2005, duration: 5.480s, episode steps: 717, steps per second: 131, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.016747, mae: 3.115314, mean_q: 3.747332, mean_eps: 0.487853\n",
      " 1424051/3750000: episode: 2006, duration: 7.905s, episode steps: 1055, steps per second: 133, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.014933, mae: 3.108473, mean_q: 3.743095, mean_eps: 0.487533\n",
      " 1424768/3750000: episode: 2007, duration: 5.385s, episode steps: 717, steps per second: 133, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.013377, mae: 3.087251, mean_q: 3.710896, mean_eps: 0.487212\n",
      " 1425539/3750000: episode: 2008, duration: 5.804s, episode steps: 771, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.014884, mae: 3.075588, mean_q: 3.696460, mean_eps: 0.486946\n",
      " 1426296/3750000: episode: 2009, duration: 5.650s, episode steps: 757, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.016385, mae: 3.083974, mean_q: 3.713859, mean_eps: 0.486672\n",
      " 1427289/3750000: episode: 2010, duration: 7.500s, episode steps: 993, steps per second: 132, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.019545, mae: 3.088272, mean_q: 3.718189, mean_eps: 0.486356\n",
      " 1427925/3750000: episode: 2011, duration: 4.759s, episode steps: 636, steps per second: 134, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.012536, mae: 3.063136, mean_q: 3.696385, mean_eps: 0.486060\n",
      " 1428719/3750000: episode: 2012, duration: 5.874s, episode steps: 794, steps per second: 135, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.017418, mae: 3.096239, mean_q: 3.725542, mean_eps: 0.485805\n",
      " 1429171/3750000: episode: 2013, duration: 3.469s, episode steps: 452, steps per second: 130, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.016599, mae: 3.089444, mean_q: 3.714489, mean_eps: 0.485582\n",
      " 1430049/3750000: episode: 2014, duration: 6.541s, episode steps: 878, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.017164, mae: 3.085383, mean_q: 3.715134, mean_eps: 0.485340\n",
      " 1430568/3750000: episode: 2015, duration: 3.870s, episode steps: 519, steps per second: 134, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.019650, mae: 3.038956, mean_q: 3.655493, mean_eps: 0.485088\n",
      " 1431420/3750000: episode: 2016, duration: 6.449s, episode steps: 852, steps per second: 132, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.015236, mae: 3.062959, mean_q: 3.688386, mean_eps: 0.484844\n",
      " 1431884/3750000: episode: 2017, duration: 3.570s, episode steps: 464, steps per second: 130, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.015313, mae: 3.134267, mean_q: 3.771261, mean_eps: 0.484606\n",
      " 1432591/3750000: episode: 2018, duration: 5.227s, episode steps: 707, steps per second: 135, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.015150, mae: 3.112985, mean_q: 3.752301, mean_eps: 0.484394\n",
      " 1433032/3750000: episode: 2019, duration: 3.289s, episode steps: 441, steps per second: 134, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.015915, mae: 3.116975, mean_q: 3.751902, mean_eps: 0.484188\n",
      " 1433914/3750000: episode: 2020, duration: 6.596s, episode steps: 882, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.013141, mae: 3.102435, mean_q: 3.735798, mean_eps: 0.483951\n",
      " 1434690/3750000: episode: 2021, duration: 5.800s, episode steps: 776, steps per second: 134, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.017184, mae: 3.132239, mean_q: 3.771088, mean_eps: 0.483652\n",
      " 1435321/3750000: episode: 2022, duration: 4.726s, episode steps: 631, steps per second: 134, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.015782, mae: 3.162700, mean_q: 3.805811, mean_eps: 0.483396\n",
      " 1435962/3750000: episode: 2023, duration: 4.844s, episode steps: 641, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.042 [0.000, 5.000],  loss: 0.018833, mae: 3.142161, mean_q: 3.786172, mean_eps: 0.483166\n",
      " 1436405/3750000: episode: 2024, duration: 3.272s, episode steps: 443, steps per second: 135, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.022443, mae: 3.108775, mean_q: 3.738922, mean_eps: 0.482972\n",
      " 1437295/3750000: episode: 2025, duration: 6.571s, episode steps: 890, steps per second: 135, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.014026, mae: 3.137267, mean_q: 3.782237, mean_eps: 0.482734\n",
      " 1437876/3750000: episode: 2026, duration: 4.362s, episode steps: 581, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.021165, mae: 3.129828, mean_q: 3.761318, mean_eps: 0.482471\n",
      " 1438423/3750000: episode: 2027, duration: 4.207s, episode steps: 547, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.019767, mae: 3.179872, mean_q: 3.824690, mean_eps: 0.482266\n",
      " 1439366/3750000: episode: 2028, duration: 7.025s, episode steps: 943, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.015418, mae: 3.162523, mean_q: 3.805270, mean_eps: 0.481996\n",
      " 1439864/3750000: episode: 2029, duration: 3.762s, episode steps: 498, steps per second: 132, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.011851, mae: 3.159235, mean_q: 3.805501, mean_eps: 0.481737\n",
      " 1440178/3750000: episode: 2030, duration: 2.282s, episode steps: 314, steps per second: 138, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.013552, mae: 3.101997, mean_q: 3.736947, mean_eps: 0.481593\n",
      " 1441059/3750000: episode: 2031, duration: 6.527s, episode steps: 881, steps per second: 135, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.015490, mae: 3.160528, mean_q: 3.804132, mean_eps: 0.481380\n",
      " 1441557/3750000: episode: 2032, duration: 3.749s, episode steps: 498, steps per second: 133, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.019746, mae: 3.158001, mean_q: 3.796301, mean_eps: 0.481132\n",
      " 1442133/3750000: episode: 2033, duration: 4.384s, episode steps: 576, steps per second: 131, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.017449, mae: 3.106729, mean_q: 3.736176, mean_eps: 0.480938\n",
      " 1443035/3750000: episode: 2034, duration: 6.656s, episode steps: 902, steps per second: 136, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.017042, mae: 3.134203, mean_q: 3.764520, mean_eps: 0.480671\n",
      " 1444254/3750000: episode: 2035, duration: 9.049s, episode steps: 1219, steps per second: 135, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.020681, mae: 3.159641, mean_q: 3.803731, mean_eps: 0.480290\n",
      " 1444944/3750000: episode: 2036, duration: 5.175s, episode steps: 690, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.014976, mae: 3.129465, mean_q: 3.769247, mean_eps: 0.479944\n",
      " 1445615/3750000: episode: 2037, duration: 4.942s, episode steps: 671, steps per second: 136, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.016413, mae: 3.092932, mean_q: 3.723377, mean_eps: 0.479699\n",
      " 1446374/3750000: episode: 2038, duration: 5.675s, episode steps: 759, steps per second: 134, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.018239, mae: 3.148067, mean_q: 3.786331, mean_eps: 0.479444\n",
      " 1446875/3750000: episode: 2039, duration: 3.744s, episode steps: 501, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.058 [0.000, 5.000],  loss: 0.015017, mae: 3.169333, mean_q: 3.813656, mean_eps: 0.479217\n",
      " 1447373/3750000: episode: 2040, duration: 3.728s, episode steps: 498, steps per second: 134, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.017503, mae: 3.139867, mean_q: 3.775026, mean_eps: 0.479037\n",
      " 1447845/3750000: episode: 2041, duration: 3.552s, episode steps: 472, steps per second: 133, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.013149, mae: 3.150618, mean_q: 3.791282, mean_eps: 0.478860\n",
      " 1448204/3750000: episode: 2042, duration: 2.679s, episode steps: 359, steps per second: 134, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.593 [0.000, 5.000],  loss: 0.012069, mae: 3.132860, mean_q: 3.769368, mean_eps: 0.478709\n",
      " 1449095/3750000: episode: 2043, duration: 6.685s, episode steps: 891, steps per second: 133, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.952 [0.000, 5.000],  loss: 0.015341, mae: 3.125927, mean_q: 3.767465, mean_eps: 0.478486\n",
      " 1449745/3750000: episode: 2044, duration: 4.880s, episode steps: 650, steps per second: 133, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.019851, mae: 3.173284, mean_q: 3.820927, mean_eps: 0.478209\n",
      " 1450293/3750000: episode: 2045, duration: 4.039s, episode steps: 548, steps per second: 136, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.914 [0.000, 5.000],  loss: 0.015276, mae: 3.146783, mean_q: 3.784986, mean_eps: 0.477993\n",
      " 1451258/3750000: episode: 2046, duration: 7.184s, episode steps: 965, steps per second: 134, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013444, mae: 3.170347, mean_q: 3.810229, mean_eps: 0.477723\n",
      " 1452139/3750000: episode: 2047, duration: 6.525s, episode steps: 881, steps per second: 135, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.013743, mae: 3.188415, mean_q: 3.835572, mean_eps: 0.477392\n",
      " 1453111/3750000: episode: 2048, duration: 7.291s, episode steps: 972, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.014340, mae: 3.117733, mean_q: 3.752735, mean_eps: 0.477057\n",
      " 1453434/3750000: episode: 2049, duration: 2.384s, episode steps: 323, steps per second: 135, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.016323, mae: 3.158254, mean_q: 3.806600, mean_eps: 0.476823\n",
      " 1454329/3750000: episode: 2050, duration: 6.689s, episode steps: 895, steps per second: 134, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.016855, mae: 3.136552, mean_q: 3.771848, mean_eps: 0.476603\n",
      " 1454953/3750000: episode: 2051, duration: 4.721s, episode steps: 624, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.016611, mae: 3.137014, mean_q: 3.770955, mean_eps: 0.476330\n",
      " 1455989/3750000: episode: 2052, duration: 7.748s, episode steps: 1036, steps per second: 134, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.022221, mae: 3.185900, mean_q: 3.829803, mean_eps: 0.476031\n",
      " 1456580/3750000: episode: 2053, duration: 4.432s, episode steps: 591, steps per second: 133, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.016877, mae: 3.149167, mean_q: 3.784988, mean_eps: 0.475739\n",
      " 1457312/3750000: episode: 2054, duration: 5.569s, episode steps: 732, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.012324, mae: 3.176394, mean_q: 3.822823, mean_eps: 0.475502\n",
      " 1457890/3750000: episode: 2055, duration: 4.284s, episode steps: 578, steps per second: 135, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.018342, mae: 3.162794, mean_q: 3.807518, mean_eps: 0.475264\n",
      " 1458286/3750000: episode: 2056, duration: 2.975s, episode steps: 396, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.019560, mae: 3.182625, mean_q: 3.834209, mean_eps: 0.475088\n",
      " 1458966/3750000: episode: 2057, duration: 5.057s, episode steps: 680, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.016888, mae: 3.195260, mean_q: 3.842275, mean_eps: 0.474893\n",
      " 1459706/3750000: episode: 2058, duration: 5.554s, episode steps: 740, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.018103, mae: 3.157945, mean_q: 3.802270, mean_eps: 0.474638\n",
      " 1460329/3750000: episode: 2059, duration: 4.651s, episode steps: 623, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.017125, mae: 3.194196, mean_q: 3.841749, mean_eps: 0.474393\n",
      " 1461595/3750000: episode: 2060, duration: 9.389s, episode steps: 1266, steps per second: 135, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.018488, mae: 3.130395, mean_q: 3.764838, mean_eps: 0.474054\n",
      " 1462598/3750000: episode: 2061, duration: 7.428s, episode steps: 1003, steps per second: 135, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.018449, mae: 3.178375, mean_q: 3.821132, mean_eps: 0.473648\n",
      " 1463077/3750000: episode: 2062, duration: 3.597s, episode steps: 479, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.021758, mae: 3.142383, mean_q: 3.789402, mean_eps: 0.473381\n",
      " 1463991/3750000: episode: 2063, duration: 6.877s, episode steps: 914, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.019903, mae: 3.137093, mean_q: 3.780827, mean_eps: 0.473129\n",
      " 1465338/3750000: episode: 2064, duration: 9.975s, episode steps: 1347, steps per second: 135, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.014427, mae: 3.161527, mean_q: 3.810662, mean_eps: 0.472722\n",
      " 1466022/3750000: episode: 2065, duration: 5.241s, episode steps: 684, steps per second: 131, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.018228, mae: 3.202834, mean_q: 3.862663, mean_eps: 0.472355\n",
      " 1466622/3750000: episode: 2066, duration: 4.444s, episode steps: 600, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.014056, mae: 3.187437, mean_q: 3.844780, mean_eps: 0.472121\n",
      " 1467284/3750000: episode: 2067, duration: 4.920s, episode steps: 662, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.013919, mae: 3.207079, mean_q: 3.856962, mean_eps: 0.471894\n",
      " 1468024/3750000: episode: 2068, duration: 5.543s, episode steps: 740, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.014359, mae: 3.189415, mean_q: 3.842263, mean_eps: 0.471642\n",
      " 1468507/3750000: episode: 2069, duration: 3.584s, episode steps: 483, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.758 [0.000, 5.000],  loss: 0.013716, mae: 3.224322, mean_q: 3.885043, mean_eps: 0.471423\n",
      " 1468905/3750000: episode: 2070, duration: 2.955s, episode steps: 398, steps per second: 135, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.362 [0.000, 5.000],  loss: 0.018583, mae: 3.215712, mean_q: 3.871152, mean_eps: 0.471264\n",
      " 1469647/3750000: episode: 2071, duration: 5.493s, episode steps: 742, steps per second: 135, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.018503, mae: 3.192068, mean_q: 3.838124, mean_eps: 0.471059\n",
      " 1470246/3750000: episode: 2072, duration: 4.497s, episode steps: 599, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.013399, mae: 3.213124, mean_q: 3.867063, mean_eps: 0.470818\n",
      " 1470936/3750000: episode: 2073, duration: 5.217s, episode steps: 690, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.021136, mae: 3.213613, mean_q: 3.865159, mean_eps: 0.470588\n",
      " 1471742/3750000: episode: 2074, duration: 6.077s, episode steps: 806, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.013688, mae: 3.205685, mean_q: 3.853085, mean_eps: 0.470318\n",
      " 1472387/3750000: episode: 2075, duration: 4.865s, episode steps: 645, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.016820, mae: 3.181166, mean_q: 3.825670, mean_eps: 0.470055\n",
      " 1473600/3750000: episode: 2076, duration: 9.059s, episode steps: 1213, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.019928, mae: 3.229286, mean_q: 3.886454, mean_eps: 0.469724\n",
      " 1474317/3750000: episode: 2077, duration: 5.407s, episode steps: 717, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.018703, mae: 3.255892, mean_q: 3.925686, mean_eps: 0.469378\n",
      " 1474805/3750000: episode: 2078, duration: 3.680s, episode steps: 488, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.009409, mae: 3.192526, mean_q: 3.849681, mean_eps: 0.469158\n",
      " 1475779/3750000: episode: 2079, duration: 7.172s, episode steps: 974, steps per second: 136, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.015280, mae: 3.276863, mean_q: 3.937104, mean_eps: 0.468896\n",
      " 1476580/3750000: episode: 2080, duration: 6.047s, episode steps: 801, steps per second: 132, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.015371, mae: 3.212374, mean_q: 3.862944, mean_eps: 0.468579\n",
      " 1477389/3750000: episode: 2081, duration: 6.049s, episode steps: 809, steps per second: 134, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.017416, mae: 3.282676, mean_q: 3.952856, mean_eps: 0.468287\n",
      " 1477943/3750000: episode: 2082, duration: 4.130s, episode steps: 554, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.016284, mae: 3.191849, mean_q: 3.842346, mean_eps: 0.468039\n",
      " 1478460/3750000: episode: 2083, duration: 3.922s, episode steps: 517, steps per second: 132, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.447 [0.000, 5.000],  loss: 0.021045, mae: 3.154138, mean_q: 3.790420, mean_eps: 0.467848\n",
      " 1479348/3750000: episode: 2084, duration: 6.623s, episode steps: 888, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.235 [0.000, 5.000],  loss: 0.023069, mae: 3.229399, mean_q: 3.885690, mean_eps: 0.467596\n",
      " 1479799/3750000: episode: 2085, duration: 3.297s, episode steps: 451, steps per second: 137, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.020557, mae: 3.211232, mean_q: 3.862825, mean_eps: 0.467355\n",
      " 1480223/3750000: episode: 2086, duration: 3.264s, episode steps: 424, steps per second: 130, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.014667, mae: 3.256489, mean_q: 3.920484, mean_eps: 0.467196\n",
      " 1480961/3750000: episode: 2087, duration: 5.565s, episode steps: 738, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.023498, mae: 3.192359, mean_q: 3.834214, mean_eps: 0.466984\n",
      " 1481765/3750000: episode: 2088, duration: 5.992s, episode steps: 804, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.022003, mae: 3.205096, mean_q: 3.861527, mean_eps: 0.466707\n",
      " 1482650/3750000: episode: 2089, duration: 6.627s, episode steps: 885, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.017686, mae: 3.250214, mean_q: 3.917047, mean_eps: 0.466404\n",
      " 1483116/3750000: episode: 2090, duration: 3.550s, episode steps: 466, steps per second: 131, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.020123, mae: 3.163940, mean_q: 3.813769, mean_eps: 0.466163\n",
      " 1484208/3750000: episode: 2091, duration: 8.105s, episode steps: 1092, steps per second: 135, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.019603, mae: 3.228852, mean_q: 3.881263, mean_eps: 0.465882\n",
      " 1484705/3750000: episode: 2092, duration: 3.700s, episode steps: 497, steps per second: 134, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.032 [0.000, 5.000],  loss: 0.018852, mae: 3.247785, mean_q: 3.905828, mean_eps: 0.465594\n",
      " 1485642/3750000: episode: 2093, duration: 7.004s, episode steps: 937, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.863 [0.000, 5.000],  loss: 0.017043, mae: 3.226677, mean_q: 3.881158, mean_eps: 0.465335\n",
      " 1486590/3750000: episode: 2094, duration: 7.099s, episode steps: 948, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.023029, mae: 3.204806, mean_q: 3.862523, mean_eps: 0.464997\n",
      " 1487286/3750000: episode: 2095, duration: 5.340s, episode steps: 696, steps per second: 130, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.021475, mae: 3.161923, mean_q: 3.807410, mean_eps: 0.464702\n",
      " 1487982/3750000: episode: 2096, duration: 5.198s, episode steps: 696, steps per second: 134, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.015280, mae: 3.199057, mean_q: 3.847703, mean_eps: 0.464450\n",
      " 1488919/3750000: episode: 2097, duration: 6.938s, episode steps: 937, steps per second: 135, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.018402, mae: 3.213135, mean_q: 3.860750, mean_eps: 0.464158\n",
      " 1489540/3750000: episode: 2098, duration: 4.788s, episode steps: 621, steps per second: 130, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.015547, mae: 3.161266, mean_q: 3.804631, mean_eps: 0.463881\n",
      " 1490245/3750000: episode: 2099, duration: 5.299s, episode steps: 705, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.019401, mae: 3.217271, mean_q: 3.872269, mean_eps: 0.463640\n",
      " 1490877/3750000: episode: 2100, duration: 4.651s, episode steps: 632, steps per second: 136, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.016444, mae: 3.179210, mean_q: 3.825229, mean_eps: 0.463398\n",
      " 1491601/3750000: episode: 2101, duration: 5.523s, episode steps: 724, steps per second: 131, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.017606, mae: 3.179533, mean_q: 3.824791, mean_eps: 0.463154\n",
      " 1492107/3750000: episode: 2102, duration: 3.802s, episode steps: 506, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.020929, mae: 3.169156, mean_q: 3.810630, mean_eps: 0.462930\n",
      " 1493039/3750000: episode: 2103, duration: 6.854s, episode steps: 932, steps per second: 136, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.012764, mae: 3.149711, mean_q: 3.788341, mean_eps: 0.462675\n",
      " 1493977/3750000: episode: 2104, duration: 7.031s, episode steps: 938, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.016206, mae: 3.165229, mean_q: 3.808276, mean_eps: 0.462340\n",
      " 1494364/3750000: episode: 2105, duration: 2.969s, episode steps: 387, steps per second: 130, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013104, mae: 3.102897, mean_q: 3.737240, mean_eps: 0.462099\n",
      " 1495307/3750000: episode: 2106, duration: 6.997s, episode steps: 943, steps per second: 135, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.016160, mae: 3.151831, mean_q: 3.794717, mean_eps: 0.461858\n",
      " 1496222/3750000: episode: 2107, duration: 6.857s, episode steps: 915, steps per second: 133, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.017 [0.000, 5.000],  loss: 0.016464, mae: 3.154483, mean_q: 3.803335, mean_eps: 0.461523\n",
      " 1497029/3750000: episode: 2108, duration: 5.964s, episode steps: 807, steps per second: 135, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.019289, mae: 3.202017, mean_q: 3.861286, mean_eps: 0.461213\n",
      " 1497721/3750000: episode: 2109, duration: 5.337s, episode steps: 692, steps per second: 130, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.020226, mae: 3.213012, mean_q: 3.871428, mean_eps: 0.460943\n",
      " 1498297/3750000: episode: 2110, duration: 4.352s, episode steps: 576, steps per second: 132, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.017996, mae: 3.181883, mean_q: 3.829832, mean_eps: 0.460716\n",
      " 1498975/3750000: episode: 2111, duration: 5.103s, episode steps: 678, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.018200, mae: 3.223531, mean_q: 3.884927, mean_eps: 0.460493\n",
      " 1499959/3750000: episode: 2112, duration: 7.353s, episode steps: 984, steps per second: 134, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014629, mae: 3.199396, mean_q: 3.854419, mean_eps: 0.460194\n",
      " 1500475/3750000: episode: 2113, duration: 4.027s, episode steps: 516, steps per second: 128, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.017324, mae: 3.183392, mean_q: 3.833300, mean_eps: 0.459924\n",
      " 1501010/3750000: episode: 2114, duration: 3.998s, episode steps: 535, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.017320, mae: 3.177828, mean_q: 3.823171, mean_eps: 0.459734\n",
      " 1501685/3750000: episode: 2115, duration: 5.065s, episode steps: 675, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.018129, mae: 3.182016, mean_q: 3.825765, mean_eps: 0.459514\n",
      " 1502602/3750000: episode: 2116, duration: 6.955s, episode steps: 917, steps per second: 132, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.013737, mae: 3.178861, mean_q: 3.827803, mean_eps: 0.459226\n",
      " 1503402/3750000: episode: 2117, duration: 5.974s, episode steps: 800, steps per second: 134, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.020723, mae: 3.118981, mean_q: 3.759876, mean_eps: 0.458916\n",
      " 1504102/3750000: episode: 2118, duration: 5.279s, episode steps: 700, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.015245, mae: 3.179003, mean_q: 3.821670, mean_eps: 0.458646\n",
      " 1504854/3750000: episode: 2119, duration: 5.509s, episode steps: 752, steps per second: 136, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.012124, mae: 3.190701, mean_q: 3.841917, mean_eps: 0.458387\n",
      " 1505613/3750000: episode: 2120, duration: 5.644s, episode steps: 759, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.019035, mae: 3.122701, mean_q: 3.755105, mean_eps: 0.458117\n",
      " 1506214/3750000: episode: 2121, duration: 4.544s, episode steps: 601, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.017553, mae: 3.113535, mean_q: 3.740970, mean_eps: 0.457872\n",
      " 1507190/3750000: episode: 2122, duration: 7.278s, episode steps: 976, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.018340, mae: 3.205906, mean_q: 3.857508, mean_eps: 0.457588\n",
      " 1508271/3750000: episode: 2123, duration: 8.046s, episode steps: 1081, steps per second: 134, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.014062, mae: 3.172646, mean_q: 3.814366, mean_eps: 0.457217\n",
      " 1509307/3750000: episode: 2124, duration: 7.719s, episode steps: 1036, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.015208, mae: 3.160324, mean_q: 3.806079, mean_eps: 0.456836\n",
      " 1509891/3750000: episode: 2125, duration: 4.348s, episode steps: 584, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.018555, mae: 3.110510, mean_q: 3.739573, mean_eps: 0.456544\n",
      " 1510381/3750000: episode: 2126, duration: 3.713s, episode steps: 490, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.759 [0.000, 5.000],  loss: 0.022092, mae: 3.192913, mean_q: 3.842025, mean_eps: 0.456350\n",
      " 1511128/3750000: episode: 2127, duration: 5.551s, episode steps: 747, steps per second: 135, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 0.017595, mae: 3.183572, mean_q: 3.833322, mean_eps: 0.456126\n",
      " 1512313/3750000: episode: 2128, duration: 8.752s, episode steps: 1185, steps per second: 135, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.023437, mae: 3.187496, mean_q: 3.843975, mean_eps: 0.455781\n",
      " 1512857/3750000: episode: 2129, duration: 4.106s, episode steps: 544, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.421 [0.000, 5.000],  loss: 0.012413, mae: 3.090519, mean_q: 3.725261, mean_eps: 0.455471\n",
      " 1513582/3750000: episode: 2130, duration: 5.437s, episode steps: 725, steps per second: 133, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.017479, mae: 3.160631, mean_q: 3.802311, mean_eps: 0.455241\n",
      " 1514245/3750000: episode: 2131, duration: 4.913s, episode steps: 663, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.014133, mae: 3.117833, mean_q: 3.751763, mean_eps: 0.454989\n",
      " 1515183/3750000: episode: 2132, duration: 7.017s, episode steps: 938, steps per second: 134, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.014892, mae: 3.138457, mean_q: 3.775425, mean_eps: 0.454701\n",
      " 1516755/3750000: episode: 2133, duration: 11.597s, episode steps: 1572, steps per second: 136, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.015803, mae: 3.127765, mean_q: 3.762419, mean_eps: 0.454251\n",
      " 1517598/3750000: episode: 2134, duration: 6.277s, episode steps: 843, steps per second: 134, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.107 [0.000, 5.000],  loss: 0.014624, mae: 3.126889, mean_q: 3.762377, mean_eps: 0.453819\n",
      " 1518504/3750000: episode: 2135, duration: 6.796s, episode steps: 906, steps per second: 133, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.012119, mae: 3.167569, mean_q: 3.809475, mean_eps: 0.453502\n",
      " 1519031/3750000: episode: 2136, duration: 3.989s, episode steps: 527, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.011690, mae: 3.160575, mean_q: 3.802218, mean_eps: 0.453243\n",
      " 1519529/3750000: episode: 2137, duration: 3.754s, episode steps: 498, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.013626, mae: 3.150517, mean_q: 3.784972, mean_eps: 0.453059\n",
      " 1520278/3750000: episode: 2138, duration: 5.626s, episode steps: 749, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.021202, mae: 3.152524, mean_q: 3.797845, mean_eps: 0.452836\n",
      " 1521295/3750000: episode: 2139, duration: 7.660s, episode steps: 1017, steps per second: 133, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.015481, mae: 3.139381, mean_q: 3.783525, mean_eps: 0.452519\n",
      " 1522084/3750000: episode: 2140, duration: 5.931s, episode steps: 789, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.014806, mae: 3.151794, mean_q: 3.790726, mean_eps: 0.452192\n",
      " 1522435/3750000: episode: 2141, duration: 2.613s, episode steps: 351, steps per second: 134, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.013968, mae: 3.155419, mean_q: 3.795299, mean_eps: 0.451986\n",
      " 1522882/3750000: episode: 2142, duration: 3.382s, episode steps: 447, steps per second: 132, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.017919, mae: 3.184564, mean_q: 3.835436, mean_eps: 0.451842\n",
      " 1523631/3750000: episode: 2143, duration: 5.611s, episode steps: 749, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 0.015305, mae: 3.155618, mean_q: 3.793324, mean_eps: 0.451626\n",
      " 1524148/3750000: episode: 2144, duration: 3.876s, episode steps: 517, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.012455, mae: 3.121836, mean_q: 3.755067, mean_eps: 0.451400\n",
      " 1525208/3750000: episode: 2145, duration: 7.859s, episode steps: 1060, steps per second: 135, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.016493, mae: 3.106108, mean_q: 3.740085, mean_eps: 0.451115\n",
      " 1525705/3750000: episode: 2146, duration: 3.772s, episode steps: 497, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.022479, mae: 3.154289, mean_q: 3.800492, mean_eps: 0.450834\n",
      " 1526881/3750000: episode: 2147, duration: 8.718s, episode steps: 1176, steps per second: 135, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.017855, mae: 3.094676, mean_q: 3.727046, mean_eps: 0.450532\n",
      " 1527790/3750000: episode: 2148, duration: 6.834s, episode steps: 909, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.018979, mae: 3.109693, mean_q: 3.739719, mean_eps: 0.450158\n",
      " 1528756/3750000: episode: 2149, duration: 7.204s, episode steps: 966, steps per second: 134, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.016430, mae: 3.128223, mean_q: 3.765087, mean_eps: 0.449823\n",
      " 1529592/3750000: episode: 2150, duration: 6.250s, episode steps: 836, steps per second: 134, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.017210, mae: 3.107623, mean_q: 3.737257, mean_eps: 0.449499\n",
      " 1530243/3750000: episode: 2151, duration: 4.971s, episode steps: 651, steps per second: 131, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.018389, mae: 3.097997, mean_q: 3.732746, mean_eps: 0.449229\n",
      " 1530929/3750000: episode: 2152, duration: 5.069s, episode steps: 686, steps per second: 135, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.015233, mae: 3.098258, mean_q: 3.726117, mean_eps: 0.448988\n",
      " 1531742/3750000: episode: 2153, duration: 6.089s, episode steps: 813, steps per second: 134, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.014276, mae: 3.103253, mean_q: 3.735979, mean_eps: 0.448718\n",
      " 1532365/3750000: episode: 2154, duration: 4.702s, episode steps: 623, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.015093, mae: 3.083605, mean_q: 3.706922, mean_eps: 0.448458\n",
      " 1533000/3750000: episode: 2155, duration: 4.937s, episode steps: 635, steps per second: 129, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.022941, mae: 3.113944, mean_q: 3.746214, mean_eps: 0.448235\n",
      " 1533867/3750000: episode: 2156, duration: 6.531s, episode steps: 867, steps per second: 133, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.014409, mae: 3.073289, mean_q: 3.697713, mean_eps: 0.447965\n",
      " 1534748/3750000: episode: 2157, duration: 6.630s, episode steps: 881, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.755 [0.000, 5.000],  loss: 0.013941, mae: 3.083257, mean_q: 3.711444, mean_eps: 0.447648\n",
      " 1535770/3750000: episode: 2158, duration: 7.581s, episode steps: 1022, steps per second: 135, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.017485, mae: 3.072962, mean_q: 3.701889, mean_eps: 0.447306\n",
      " 1536464/3750000: episode: 2159, duration: 5.242s, episode steps: 694, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.019342, mae: 3.084312, mean_q: 3.706219, mean_eps: 0.446997\n",
      " 1537410/3750000: episode: 2160, duration: 6.976s, episode steps: 946, steps per second: 136, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.020264, mae: 3.087689, mean_q: 3.713919, mean_eps: 0.446702\n",
      " 1537779/3750000: episode: 2161, duration: 2.708s, episode steps: 369, steps per second: 136, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.268 [0.000, 5.000],  loss: 0.016208, mae: 3.093627, mean_q: 3.722613, mean_eps: 0.446468\n",
      " 1538546/3750000: episode: 2162, duration: 5.824s, episode steps: 767, steps per second: 132, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.015354, mae: 3.099530, mean_q: 3.729655, mean_eps: 0.446262\n",
      " 1539299/3750000: episode: 2163, duration: 5.545s, episode steps: 753, steps per second: 136, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.018479, mae: 3.106248, mean_q: 3.735263, mean_eps: 0.445989\n",
      " 1539803/3750000: episode: 2164, duration: 3.810s, episode steps: 504, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.016142, mae: 3.093377, mean_q: 3.719125, mean_eps: 0.445762\n",
      " 1540967/3750000: episode: 2165, duration: 8.703s, episode steps: 1164, steps per second: 134, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.016441, mae: 3.058784, mean_q: 3.687035, mean_eps: 0.445460\n",
      " 1541613/3750000: episode: 2166, duration: 4.783s, episode steps: 646, steps per second: 135, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.021816, mae: 3.063244, mean_q: 3.689209, mean_eps: 0.445136\n",
      " 1542259/3750000: episode: 2167, duration: 4.792s, episode steps: 646, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.016078, mae: 3.096610, mean_q: 3.731385, mean_eps: 0.444905\n",
      " 1542904/3750000: episode: 2168, duration: 4.936s, episode steps: 645, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.014966, mae: 3.068797, mean_q: 3.691692, mean_eps: 0.444671\n",
      " 1543403/3750000: episode: 2169, duration: 3.717s, episode steps: 499, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.021158, mae: 3.099705, mean_q: 3.728464, mean_eps: 0.444462\n",
      " 1543917/3750000: episode: 2170, duration: 3.761s, episode steps: 514, steps per second: 137, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.011929, mae: 3.137766, mean_q: 3.772664, mean_eps: 0.444282\n",
      " 1544308/3750000: episode: 2171, duration: 2.959s, episode steps: 391, steps per second: 132, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016271, mae: 3.114313, mean_q: 3.743477, mean_eps: 0.444120\n",
      " 1544945/3750000: episode: 2172, duration: 4.809s, episode steps: 637, steps per second: 132, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.016532, mae: 3.142229, mean_q: 3.782597, mean_eps: 0.443933\n",
      " 1545646/3750000: episode: 2173, duration: 5.223s, episode steps: 701, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.011350, mae: 3.133920, mean_q: 3.768779, mean_eps: 0.443692\n",
      " 1546347/3750000: episode: 2174, duration: 5.193s, episode steps: 701, steps per second: 135, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.015407, mae: 3.119702, mean_q: 3.750670, mean_eps: 0.443440\n",
      " 1547482/3750000: episode: 2175, duration: 8.511s, episode steps: 1135, steps per second: 133, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.017264, mae: 3.076800, mean_q: 3.700805, mean_eps: 0.443109\n",
      " 1548480/3750000: episode: 2176, duration: 7.395s, episode steps: 998, steps per second: 135, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.019230, mae: 3.067490, mean_q: 3.691633, mean_eps: 0.442727\n",
      " 1549289/3750000: episode: 2177, duration: 6.145s, episode steps: 809, steps per second: 132, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.852 [0.000, 5.000],  loss: 0.013111, mae: 3.051372, mean_q: 3.671731, mean_eps: 0.442403\n",
      " 1549823/3750000: episode: 2178, duration: 3.988s, episode steps: 534, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.018392, mae: 3.127101, mean_q: 3.774193, mean_eps: 0.442158\n",
      " 1550242/3750000: episode: 2179, duration: 3.128s, episode steps: 419, steps per second: 134, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.308 [0.000, 5.000],  loss: 0.013656, mae: 3.107594, mean_q: 3.740561, mean_eps: 0.441986\n",
      " 1550875/3750000: episode: 2180, duration: 4.754s, episode steps: 633, steps per second: 133, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.019972, mae: 3.125269, mean_q: 3.769014, mean_eps: 0.441798\n",
      " 1551242/3750000: episode: 2181, duration: 2.863s, episode steps: 367, steps per second: 128, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.018279, mae: 3.134430, mean_q: 3.787382, mean_eps: 0.441618\n",
      " 1551692/3750000: episode: 2182, duration: 3.290s, episode steps: 450, steps per second: 137, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.010674, mae: 3.101107, mean_q: 3.741093, mean_eps: 0.441471\n",
      " 1552184/3750000: episode: 2183, duration: 3.726s, episode steps: 492, steps per second: 132, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.012216, mae: 3.124180, mean_q: 3.761914, mean_eps: 0.441302\n",
      " 1552873/3750000: episode: 2184, duration: 5.093s, episode steps: 689, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.042 [0.000, 5.000],  loss: 0.013594, mae: 3.136044, mean_q: 3.777522, mean_eps: 0.441089\n",
      " 1554107/3750000: episode: 2185, duration: 9.292s, episode steps: 1234, steps per second: 133, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.012373, mae: 3.134153, mean_q: 3.771123, mean_eps: 0.440744\n",
      " 1554599/3750000: episode: 2186, duration: 3.597s, episode steps: 492, steps per second: 137, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.018102, mae: 3.109966, mean_q: 3.741444, mean_eps: 0.440434\n",
      " 1555596/3750000: episode: 2187, duration: 7.460s, episode steps: 997, steps per second: 134, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.010002, mae: 3.091411, mean_q: 3.721205, mean_eps: 0.440168\n",
      " 1556318/3750000: episode: 2188, duration: 5.361s, episode steps: 722, steps per second: 135, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.270 [0.000, 5.000],  loss: 0.013798, mae: 3.067593, mean_q: 3.689887, mean_eps: 0.439858\n",
      " 1556911/3750000: episode: 2189, duration: 4.429s, episode steps: 593, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.015464, mae: 3.031071, mean_q: 3.644749, mean_eps: 0.439620\n",
      " 1557594/3750000: episode: 2190, duration: 5.122s, episode steps: 683, steps per second: 133, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.013688, mae: 3.081916, mean_q: 3.703944, mean_eps: 0.439390\n",
      " 1557995/3750000: episode: 2191, duration: 2.969s, episode steps: 401, steps per second: 135, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.012377, mae: 3.067878, mean_q: 3.688360, mean_eps: 0.439196\n",
      " 1558885/3750000: episode: 2192, duration: 6.662s, episode steps: 890, steps per second: 134, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.014532, mae: 3.056846, mean_q: 3.677452, mean_eps: 0.438962\n",
      " 1559380/3750000: episode: 2193, duration: 3.686s, episode steps: 495, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.022580, mae: 3.055958, mean_q: 3.676651, mean_eps: 0.438713\n",
      " 1560197/3750000: episode: 2194, duration: 6.146s, episode steps: 817, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.016222, mae: 3.053677, mean_q: 3.679694, mean_eps: 0.438479\n",
      " 1560675/3750000: episode: 2195, duration: 3.550s, episode steps: 478, steps per second: 135, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.017510, mae: 3.108117, mean_q: 3.738783, mean_eps: 0.438245\n",
      " 1561169/3750000: episode: 2196, duration: 3.694s, episode steps: 494, steps per second: 134, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.012192, mae: 3.039640, mean_q: 3.665422, mean_eps: 0.438069\n",
      " 1561928/3750000: episode: 2197, duration: 5.664s, episode steps: 759, steps per second: 134, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.025589, mae: 3.062986, mean_q: 3.686186, mean_eps: 0.437842\n",
      " 1562794/3750000: episode: 2198, duration: 6.420s, episode steps: 866, steps per second: 135, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.014781, mae: 3.079473, mean_q: 3.710149, mean_eps: 0.437550\n",
      " 1563713/3750000: episode: 2199, duration: 6.827s, episode steps: 919, steps per second: 135, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.014572, mae: 3.105468, mean_q: 3.738263, mean_eps: 0.437230\n",
      " 1564249/3750000: episode: 2200, duration: 4.079s, episode steps: 536, steps per second: 131, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 0.018998, mae: 3.166431, mean_q: 3.814660, mean_eps: 0.436967\n",
      " 1564952/3750000: episode: 2201, duration: 5.223s, episode steps: 703, steps per second: 135, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.057 [0.000, 5.000],  loss: 0.022102, mae: 3.105613, mean_q: 3.746777, mean_eps: 0.436744\n",
      " 1565807/3750000: episode: 2202, duration: 6.418s, episode steps: 855, steps per second: 133, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.017831, mae: 3.160910, mean_q: 3.807702, mean_eps: 0.436463\n",
      " 1566621/3750000: episode: 2203, duration: 6.175s, episode steps: 814, steps per second: 132, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.025146, mae: 3.139765, mean_q: 3.777031, mean_eps: 0.436161\n",
      " 1567485/3750000: episode: 2204, duration: 6.476s, episode steps: 864, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.019287, mae: 3.151420, mean_q: 3.790695, mean_eps: 0.435858\n",
      " 1568028/3750000: episode: 2205, duration: 4.019s, episode steps: 543, steps per second: 135, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.018146, mae: 3.136150, mean_q: 3.771554, mean_eps: 0.435606\n",
      " 1568980/3750000: episode: 2206, duration: 7.151s, episode steps: 952, steps per second: 133, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.015309, mae: 3.174738, mean_q: 3.820532, mean_eps: 0.435340\n",
      " 1569933/3750000: episode: 2207, duration: 7.123s, episode steps: 953, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.014591, mae: 3.127035, mean_q: 3.758085, mean_eps: 0.434998\n",
      " 1571119/3750000: episode: 2208, duration: 8.862s, episode steps: 1186, steps per second: 134, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.821 [0.000, 5.000],  loss: 0.014212, mae: 3.194608, mean_q: 3.842525, mean_eps: 0.434613\n",
      " 1572581/3750000: episode: 2209, duration: 10.921s, episode steps: 1462, steps per second: 134, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.013587, mae: 3.181690, mean_q: 3.829819, mean_eps: 0.434134\n",
      " 1573962/3750000: episode: 2210, duration: 10.244s, episode steps: 1381, steps per second: 135, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.015886, mae: 3.161873, mean_q: 3.804657, mean_eps: 0.433619\n",
      " 1574723/3750000: episode: 2211, duration: 5.704s, episode steps: 761, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.018987, mae: 3.152432, mean_q: 3.792566, mean_eps: 0.433234\n",
      " 1575487/3750000: episode: 2212, duration: 5.634s, episode steps: 764, steps per second: 136, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.021449, mae: 3.191756, mean_q: 3.837851, mean_eps: 0.432960\n",
      " 1576247/3750000: episode: 2213, duration: 5.618s, episode steps: 760, steps per second: 135, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.013614, mae: 3.197230, mean_q: 3.847810, mean_eps: 0.432687\n",
      " 1577482/3750000: episode: 2214, duration: 9.224s, episode steps: 1235, steps per second: 134, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.012577, mae: 3.178085, mean_q: 3.825095, mean_eps: 0.432327\n",
      " 1578239/3750000: episode: 2215, duration: 5.556s, episode steps: 757, steps per second: 136, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.017504, mae: 3.173027, mean_q: 3.817990, mean_eps: 0.431970\n",
      " 1578785/3750000: episode: 2216, duration: 4.115s, episode steps: 546, steps per second: 133, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.014122, mae: 3.175503, mean_q: 3.823527, mean_eps: 0.431736\n",
      " 1579716/3750000: episode: 2217, duration: 6.900s, episode steps: 931, steps per second: 135, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.013970, mae: 3.211338, mean_q: 3.864354, mean_eps: 0.431470\n",
      " 1580615/3750000: episode: 2218, duration: 6.686s, episode steps: 899, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.015806, mae: 3.169623, mean_q: 3.807760, mean_eps: 0.431142\n",
      " 1581164/3750000: episode: 2219, duration: 4.269s, episode steps: 549, steps per second: 129, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.024 [0.000, 5.000],  loss: 0.013779, mae: 3.123516, mean_q: 3.757770, mean_eps: 0.430880\n",
      " 1582412/3750000: episode: 2220, duration: 9.204s, episode steps: 1248, steps per second: 136, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.021637, mae: 3.137226, mean_q: 3.773245, mean_eps: 0.430556\n",
      " 1582939/3750000: episode: 2221, duration: 3.951s, episode steps: 527, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.019086, mae: 3.198445, mean_q: 3.846762, mean_eps: 0.430239\n",
      " 1583828/3750000: episode: 2222, duration: 6.758s, episode steps: 889, steps per second: 132, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.992 [0.000, 5.000],  loss: 0.012561, mae: 3.185796, mean_q: 3.832563, mean_eps: 0.429983\n",
      " 1584691/3750000: episode: 2223, duration: 6.401s, episode steps: 863, steps per second: 135, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.023845, mae: 3.136285, mean_q: 3.771093, mean_eps: 0.429666\n",
      " 1585286/3750000: episode: 2224, duration: 4.504s, episode steps: 595, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.168 [0.000, 5.000],  loss: 0.017513, mae: 3.159551, mean_q: 3.817196, mean_eps: 0.429404\n",
      " 1586120/3750000: episode: 2225, duration: 6.287s, episode steps: 834, steps per second: 133, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.015761, mae: 3.164215, mean_q: 3.807856, mean_eps: 0.429148\n",
      " 1586672/3750000: episode: 2226, duration: 4.116s, episode steps: 552, steps per second: 134, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.021711, mae: 3.176261, mean_q: 3.826190, mean_eps: 0.428900\n",
      " 1587665/3750000: episode: 2227, duration: 7.418s, episode steps: 993, steps per second: 134, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.017148, mae: 3.237752, mean_q: 3.895788, mean_eps: 0.428619\n",
      " 1588422/3750000: episode: 2228, duration: 5.608s, episode steps: 757, steps per second: 135, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.020868, mae: 3.267679, mean_q: 3.929375, mean_eps: 0.428302\n",
      " 1588980/3750000: episode: 2229, duration: 4.151s, episode steps: 558, steps per second: 134, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.030 [0.000, 5.000],  loss: 0.025583, mae: 3.155347, mean_q: 3.794321, mean_eps: 0.428068\n",
      " 1589471/3750000: episode: 2230, duration: 3.681s, episode steps: 491, steps per second: 133, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.438 [0.000, 5.000],  loss: 0.019598, mae: 3.225269, mean_q: 3.883801, mean_eps: 0.427881\n",
      " 1590461/3750000: episode: 2231, duration: 7.483s, episode steps: 990, steps per second: 132, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.013512, mae: 3.244324, mean_q: 3.910811, mean_eps: 0.427611\n",
      " 1591983/3750000: episode: 2232, duration: 11.252s, episode steps: 1522, steps per second: 135, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.021590, mae: 3.223495, mean_q: 3.874683, mean_eps: 0.427157\n",
      " 1592683/3750000: episode: 2233, duration: 5.194s, episode steps: 700, steps per second: 135, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.018038, mae: 3.207867, mean_q: 3.857696, mean_eps: 0.426758\n",
      " 1593366/3750000: episode: 2234, duration: 5.080s, episode steps: 683, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.318 [0.000, 5.000],  loss: 0.012986, mae: 3.207216, mean_q: 3.866168, mean_eps: 0.426509\n",
      " 1594558/3750000: episode: 2235, duration: 8.831s, episode steps: 1192, steps per second: 135, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.019802, mae: 3.188818, mean_q: 3.836343, mean_eps: 0.426174\n",
      " 1595178/3750000: episode: 2236, duration: 4.613s, episode steps: 620, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.017727, mae: 3.156943, mean_q: 3.798631, mean_eps: 0.425850\n",
      " 1595959/3750000: episode: 2237, duration: 5.763s, episode steps: 781, steps per second: 136, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.013195, mae: 3.191401, mean_q: 3.840184, mean_eps: 0.425598\n",
      " 1596537/3750000: episode: 2238, duration: 4.376s, episode steps: 578, steps per second: 132, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.018543, mae: 3.121730, mean_q: 3.758938, mean_eps: 0.425354\n",
      " 1596906/3750000: episode: 2239, duration: 2.803s, episode steps: 369, steps per second: 132, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.012992, mae: 3.201985, mean_q: 3.862738, mean_eps: 0.425181\n",
      " 1597766/3750000: episode: 2240, duration: 6.378s, episode steps: 860, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.016642, mae: 3.180730, mean_q: 3.828592, mean_eps: 0.424958\n",
      " 1598516/3750000: episode: 2241, duration: 5.578s, episode steps: 750, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.868 [0.000, 5.000],  loss: 0.016551, mae: 3.195247, mean_q: 3.853364, mean_eps: 0.424670\n",
      " 1599701/3750000: episode: 2242, duration: 8.912s, episode steps: 1185, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.016161, mae: 3.242175, mean_q: 3.903423, mean_eps: 0.424320\n",
      " 1600716/3750000: episode: 2243, duration: 7.541s, episode steps: 1015, steps per second: 135, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.012058, mae: 3.247538, mean_q: 3.909433, mean_eps: 0.423924\n",
      " 1601582/3750000: episode: 2244, duration: 6.517s, episode steps: 866, steps per second: 133, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.022220, mae: 3.196498, mean_q: 3.841373, mean_eps: 0.423586\n",
      " 1602680/3750000: episode: 2245, duration: 8.210s, episode steps: 1098, steps per second: 134, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.015172, mae: 3.193141, mean_q: 3.842084, mean_eps: 0.423233\n",
      " 1603133/3750000: episode: 2246, duration: 3.438s, episode steps: 453, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.020182, mae: 3.228774, mean_q: 3.885094, mean_eps: 0.422956\n",
      " 1604055/3750000: episode: 2247, duration: 6.824s, episode steps: 922, steps per second: 135, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.018282, mae: 3.204706, mean_q: 3.855154, mean_eps: 0.422708\n",
      " 1604821/3750000: episode: 2248, duration: 5.805s, episode steps: 766, steps per second: 132, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.018665, mae: 3.210500, mean_q: 3.863863, mean_eps: 0.422402\n",
      " 1605693/3750000: episode: 2249, duration: 6.417s, episode steps: 872, steps per second: 136, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.010283, mae: 3.227240, mean_q: 3.888057, mean_eps: 0.422106\n",
      " 1606674/3750000: episode: 2250, duration: 7.314s, episode steps: 981, steps per second: 134, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.012529, mae: 3.232220, mean_q: 3.886260, mean_eps: 0.421775\n",
      " 1607225/3750000: episode: 2251, duration: 4.243s, episode steps: 551, steps per second: 130, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.021260, mae: 3.217526, mean_q: 3.867063, mean_eps: 0.421498\n",
      " 1607632/3750000: episode: 2252, duration: 3.010s, episode steps: 407, steps per second: 135, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.813 [0.000, 5.000],  loss: 0.018195, mae: 3.292377, mean_q: 3.961946, mean_eps: 0.421325\n",
      " 1608274/3750000: episode: 2253, duration: 4.782s, episode steps: 642, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.015231, mae: 3.268135, mean_q: 3.937526, mean_eps: 0.421138\n",
      " 1608806/3750000: episode: 2254, duration: 4.001s, episode steps: 532, steps per second: 133, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.152 [0.000, 5.000],  loss: 0.021936, mae: 3.275834, mean_q: 3.946880, mean_eps: 0.420926\n",
      " 1609195/3750000: episode: 2255, duration: 2.917s, episode steps: 389, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.473 [0.000, 5.000],  loss: 0.022069, mae: 3.288928, mean_q: 3.956456, mean_eps: 0.420760\n",
      " 1609865/3750000: episode: 2256, duration: 4.994s, episode steps: 670, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.330 [0.000, 5.000],  loss: 0.016168, mae: 3.298319, mean_q: 3.972802, mean_eps: 0.420569\n",
      " 1610445/3750000: episode: 2257, duration: 4.431s, episode steps: 580, steps per second: 131, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.014661, mae: 3.324968, mean_q: 4.005894, mean_eps: 0.420342\n",
      " 1610922/3750000: episode: 2258, duration: 3.560s, episode steps: 477, steps per second: 134, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.012927, mae: 3.355279, mean_q: 4.038428, mean_eps: 0.420152\n",
      " 1611562/3750000: episode: 2259, duration: 4.834s, episode steps: 640, steps per second: 132, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.172 [0.000, 5.000],  loss: 0.017270, mae: 3.300819, mean_q: 3.976625, mean_eps: 0.419950\n",
      " 1612338/3750000: episode: 2260, duration: 5.692s, episode steps: 776, steps per second: 136, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.735 [0.000, 5.000],  loss: 0.015955, mae: 3.303591, mean_q: 3.973670, mean_eps: 0.419698\n",
      " 1613217/3750000: episode: 2261, duration: 6.683s, episode steps: 879, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.020750, mae: 3.298018, mean_q: 3.965674, mean_eps: 0.419403\n",
      " 1613725/3750000: episode: 2262, duration: 3.897s, episode steps: 508, steps per second: 130, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.114 [0.000, 5.000],  loss: 0.016151, mae: 3.323198, mean_q: 3.997513, mean_eps: 0.419151\n",
      " 1614396/3750000: episode: 2263, duration: 4.935s, episode steps: 671, steps per second: 136, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.015240, mae: 3.316220, mean_q: 3.989576, mean_eps: 0.418938\n",
      " 1615268/3750000: episode: 2264, duration: 6.568s, episode steps: 872, steps per second: 133, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.014169, mae: 3.324211, mean_q: 3.995213, mean_eps: 0.418661\n",
      " 1615842/3750000: episode: 2265, duration: 4.334s, episode steps: 574, steps per second: 132, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016818, mae: 3.334312, mean_q: 4.011225, mean_eps: 0.418398\n",
      " 1617037/3750000: episode: 2266, duration: 8.774s, episode steps: 1195, steps per second: 136, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.017812, mae: 3.323886, mean_q: 4.000079, mean_eps: 0.418082\n",
      " 1618130/3750000: episode: 2267, duration: 8.180s, episode steps: 1093, steps per second: 134, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.019371, mae: 3.327466, mean_q: 4.001501, mean_eps: 0.417671\n",
      " 1618774/3750000: episode: 2268, duration: 4.807s, episode steps: 644, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.017910, mae: 3.301141, mean_q: 3.974104, mean_eps: 0.417358\n",
      " 1619454/3750000: episode: 2269, duration: 5.128s, episode steps: 680, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.021732, mae: 3.328856, mean_q: 4.000603, mean_eps: 0.417120\n",
      " 1619838/3750000: episode: 2270, duration: 2.887s, episode steps: 384, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.014469, mae: 3.389148, mean_q: 4.076573, mean_eps: 0.416930\n",
      " 1620306/3750000: episode: 2271, duration: 3.516s, episode steps: 468, steps per second: 133, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.010525, mae: 3.340266, mean_q: 4.026908, mean_eps: 0.416775\n",
      " 1620914/3750000: episode: 2272, duration: 4.446s, episode steps: 608, steps per second: 137, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.019100, mae: 3.364656, mean_q: 4.049738, mean_eps: 0.416580\n",
      " 1621788/3750000: episode: 2273, duration: 6.536s, episode steps: 874, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.013141, mae: 3.334061, mean_q: 4.006734, mean_eps: 0.416314\n",
      " 1622825/3750000: episode: 2274, duration: 7.707s, episode steps: 1037, steps per second: 135, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.861 [0.000, 5.000],  loss: 0.019183, mae: 3.361080, mean_q: 4.046382, mean_eps: 0.415968\n",
      " 1623335/3750000: episode: 2275, duration: 3.748s, episode steps: 510, steps per second: 136, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013293, mae: 3.353765, mean_q: 4.036106, mean_eps: 0.415691\n",
      " 1624318/3750000: episode: 2276, duration: 7.301s, episode steps: 983, steps per second: 135, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.012297, mae: 3.269719, mean_q: 3.931143, mean_eps: 0.415425\n",
      " 1625027/3750000: episode: 2277, duration: 5.321s, episode steps: 709, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.014723, mae: 3.320876, mean_q: 3.997306, mean_eps: 0.415119\n",
      " 1625546/3750000: episode: 2278, duration: 3.901s, episode steps: 519, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.018075, mae: 3.250588, mean_q: 3.908257, mean_eps: 0.414896\n",
      " 1626304/3750000: episode: 2279, duration: 5.726s, episode steps: 758, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017941, mae: 3.267390, mean_q: 3.935083, mean_eps: 0.414665\n",
      " 1627113/3750000: episode: 2280, duration: 5.969s, episode steps: 809, steps per second: 136, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.016413, mae: 3.223809, mean_q: 3.877320, mean_eps: 0.414384\n",
      " 1627754/3750000: episode: 2281, duration: 4.768s, episode steps: 641, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.021752, mae: 3.232734, mean_q: 3.890497, mean_eps: 0.414125\n",
      " 1628709/3750000: episode: 2282, duration: 7.149s, episode steps: 955, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.021192, mae: 3.212298, mean_q: 3.866418, mean_eps: 0.413837\n",
      " 1629198/3750000: episode: 2283, duration: 3.582s, episode steps: 489, steps per second: 137, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.599 [0.000, 5.000],  loss: 0.020789, mae: 3.269218, mean_q: 3.935579, mean_eps: 0.413578\n",
      " 1629788/3750000: episode: 2284, duration: 4.427s, episode steps: 590, steps per second: 133, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.017626, mae: 3.271341, mean_q: 3.933646, mean_eps: 0.413384\n",
      " 1630628/3750000: episode: 2285, duration: 6.337s, episode steps: 840, steps per second: 133, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.018304, mae: 3.228083, mean_q: 3.883012, mean_eps: 0.413124\n",
      " 1631123/3750000: episode: 2286, duration: 3.763s, episode steps: 495, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.521 [0.000, 5.000],  loss: 0.013245, mae: 3.238329, mean_q: 3.901731, mean_eps: 0.412883\n",
      " 1631766/3750000: episode: 2287, duration: 4.778s, episode steps: 643, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.011509, mae: 3.196248, mean_q: 3.853446, mean_eps: 0.412678\n",
      " 1632516/3750000: episode: 2288, duration: 5.570s, episode steps: 750, steps per second: 135, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.014174, mae: 3.194586, mean_q: 3.844893, mean_eps: 0.412430\n",
      " 1633341/3750000: episode: 2289, duration: 6.204s, episode steps: 825, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.019540, mae: 3.161673, mean_q: 3.805605, mean_eps: 0.412145\n",
      " 1634339/3750000: episode: 2290, duration: 7.371s, episode steps: 998, steps per second: 135, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.016615, mae: 3.172700, mean_q: 3.823375, mean_eps: 0.411818\n",
      " 1634875/3750000: episode: 2291, duration: 4.059s, episode steps: 536, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.013805, mae: 3.248042, mean_q: 3.908658, mean_eps: 0.411544\n",
      " 1635826/3750000: episode: 2292, duration: 7.074s, episode steps: 951, steps per second: 134, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.019787, mae: 3.205303, mean_q: 3.854975, mean_eps: 0.411274\n",
      " 1636367/3750000: episode: 2293, duration: 4.018s, episode steps: 541, steps per second: 135, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.023025, mae: 3.216883, mean_q: 3.864144, mean_eps: 0.411004\n",
      " 1637053/3750000: episode: 2294, duration: 5.172s, episode steps: 686, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.991 [0.000, 5.000],  loss: 0.022179, mae: 3.275298, mean_q: 3.943742, mean_eps: 0.410784\n",
      " 1637669/3750000: episode: 2295, duration: 4.691s, episode steps: 616, steps per second: 131, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.932 [0.000, 5.000],  loss: 0.024286, mae: 3.194293, mean_q: 3.849729, mean_eps: 0.410550\n",
      " 1638302/3750000: episode: 2296, duration: 4.766s, episode steps: 633, steps per second: 133, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.015393, mae: 3.206080, mean_q: 3.856846, mean_eps: 0.410324\n",
      " 1639006/3750000: episode: 2297, duration: 5.394s, episode steps: 704, steps per second: 131, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.013084, mae: 3.239802, mean_q: 3.896824, mean_eps: 0.410082\n",
      " 1639443/3750000: episode: 2298, duration: 3.269s, episode steps: 437, steps per second: 134, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.015617, mae: 3.246286, mean_q: 3.907657, mean_eps: 0.409877\n",
      " 1640585/3750000: episode: 2299, duration: 8.465s, episode steps: 1142, steps per second: 135, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.024163, mae: 3.184369, mean_q: 3.829802, mean_eps: 0.409593\n",
      " 1641007/3750000: episode: 2300, duration: 3.173s, episode steps: 422, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.016793, mae: 3.173072, mean_q: 3.819813, mean_eps: 0.409312\n",
      " 1642145/3750000: episode: 2301, duration: 8.497s, episode steps: 1138, steps per second: 134, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.024219, mae: 3.122373, mean_q: 3.758113, mean_eps: 0.409031\n",
      " 1642749/3750000: episode: 2302, duration: 4.563s, episode steps: 604, steps per second: 132, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.396 [0.000, 5.000],  loss: 0.016708, mae: 3.165273, mean_q: 3.814799, mean_eps: 0.408718\n",
      " 1643242/3750000: episode: 2303, duration: 3.747s, episode steps: 493, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.019965, mae: 3.088472, mean_q: 3.716478, mean_eps: 0.408520\n",
      " 1643785/3750000: episode: 2304, duration: 4.057s, episode steps: 543, steps per second: 134, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.018599, mae: 3.146931, mean_q: 3.787121, mean_eps: 0.408333\n",
      " 1644484/3750000: episode: 2305, duration: 5.225s, episode steps: 699, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.810 [0.000, 5.000],  loss: 0.011420, mae: 3.098865, mean_q: 3.729215, mean_eps: 0.408110\n",
      " 1645037/3750000: episode: 2306, duration: 4.066s, episode steps: 553, steps per second: 136, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.021750, mae: 3.181078, mean_q: 3.826652, mean_eps: 0.407886\n",
      " 1646122/3750000: episode: 2307, duration: 8.159s, episode steps: 1085, steps per second: 133, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.068 [0.000, 5.000],  loss: 0.018614, mae: 3.083596, mean_q: 3.707709, mean_eps: 0.407591\n",
      " 1646737/3750000: episode: 2308, duration: 4.501s, episode steps: 615, steps per second: 137, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.021328, mae: 3.038609, mean_q: 3.654906, mean_eps: 0.407285\n",
      " 1647426/3750000: episode: 2309, duration: 5.360s, episode steps: 689, steps per second: 129, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.015836, mae: 3.043461, mean_q: 3.660511, mean_eps: 0.407051\n",
      " 1648111/3750000: episode: 2310, duration: 5.110s, episode steps: 685, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.021110, mae: 3.063582, mean_q: 3.683979, mean_eps: 0.406803\n",
      " 1648804/3750000: episode: 2311, duration: 5.181s, episode steps: 693, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.015934, mae: 3.050118, mean_q: 3.668054, mean_eps: 0.406554\n",
      " 1649582/3750000: episode: 2312, duration: 5.815s, episode steps: 778, steps per second: 134, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.855 [0.000, 5.000],  loss: 0.015385, mae: 3.056230, mean_q: 3.679072, mean_eps: 0.406288\n",
      " 1650519/3750000: episode: 2313, duration: 6.928s, episode steps: 937, steps per second: 135, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.016283, mae: 3.046624, mean_q: 3.665043, mean_eps: 0.405982\n",
      " 1651054/3750000: episode: 2314, duration: 3.989s, episode steps: 535, steps per second: 134, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.017532, mae: 3.042013, mean_q: 3.654144, mean_eps: 0.405719\n",
      " 1652041/3750000: episode: 2315, duration: 7.475s, episode steps: 987, steps per second: 132, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.016309, mae: 3.041017, mean_q: 3.663464, mean_eps: 0.405442\n",
      " 1652758/3750000: episode: 2316, duration: 5.271s, episode steps: 717, steps per second: 136, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016915, mae: 3.071848, mean_q: 3.695276, mean_eps: 0.405136\n",
      " 1653491/3750000: episode: 2317, duration: 5.568s, episode steps: 733, steps per second: 132, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.019322, mae: 3.069997, mean_q: 3.688940, mean_eps: 0.404877\n",
      " 1653821/3750000: episode: 2318, duration: 2.509s, episode steps: 330, steps per second: 132, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.013961, mae: 3.072659, mean_q: 3.695224, mean_eps: 0.404682\n",
      " 1654445/3750000: episode: 2319, duration: 4.675s, episode steps: 624, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.016377, mae: 3.056623, mean_q: 3.676250, mean_eps: 0.404510\n",
      " 1654949/3750000: episode: 2320, duration: 3.748s, episode steps: 504, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.015791, mae: 3.055961, mean_q: 3.677681, mean_eps: 0.404308\n",
      " 1655305/3750000: episode: 2321, duration: 2.680s, episode steps: 356, steps per second: 133, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.020875, mae: 3.076299, mean_q: 3.696705, mean_eps: 0.404153\n",
      " 1655918/3750000: episode: 2322, duration: 4.521s, episode steps: 613, steps per second: 136, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.019629, mae: 3.051335, mean_q: 3.670693, mean_eps: 0.403980\n",
      " 1656923/3750000: episode: 2323, duration: 7.593s, episode steps: 1005, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.016887, mae: 3.071466, mean_q: 3.693753, mean_eps: 0.403689\n",
      " 1657784/3750000: episode: 2324, duration: 6.368s, episode steps: 861, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.018925, mae: 3.118846, mean_q: 3.753365, mean_eps: 0.403350\n",
      " 1658611/3750000: episode: 2325, duration: 6.157s, episode steps: 827, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.132 [0.000, 5.000],  loss: 0.014516, mae: 3.118879, mean_q: 3.762867, mean_eps: 0.403048\n",
      " 1659307/3750000: episode: 2326, duration: 5.185s, episode steps: 696, steps per second: 134, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.013128, mae: 3.093728, mean_q: 3.728863, mean_eps: 0.402774\n",
      " 1659677/3750000: episode: 2327, duration: 2.718s, episode steps: 370, steps per second: 136, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.943 [0.000, 5.000],  loss: 0.014274, mae: 3.087905, mean_q: 3.723330, mean_eps: 0.402584\n",
      " 1660421/3750000: episode: 2328, duration: 5.648s, episode steps: 744, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.014703, mae: 3.088196, mean_q: 3.714437, mean_eps: 0.402382\n",
      " 1661198/3750000: episode: 2329, duration: 5.712s, episode steps: 777, steps per second: 136, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.012334, mae: 3.095748, mean_q: 3.727133, mean_eps: 0.402108\n",
      " 1661718/3750000: episode: 2330, duration: 3.873s, episode steps: 520, steps per second: 134, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.011322, mae: 3.091498, mean_q: 3.719426, mean_eps: 0.401878\n",
      " 1662542/3750000: episode: 2331, duration: 6.216s, episode steps: 824, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.013782, mae: 3.088989, mean_q: 3.718283, mean_eps: 0.401633\n",
      " 1663550/3750000: episode: 2332, duration: 7.509s, episode steps: 1008, steps per second: 134, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.664 [0.000, 5.000],  loss: 0.024437, mae: 3.069640, mean_q: 3.695199, mean_eps: 0.401302\n",
      " 1663995/3750000: episode: 2333, duration: 3.286s, episode steps: 445, steps per second: 135, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.708 [0.000, 5.000],  loss: 0.014198, mae: 3.082338, mean_q: 3.706298, mean_eps: 0.401043\n",
      " 1664393/3750000: episode: 2334, duration: 2.962s, episode steps: 398, steps per second: 134, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.060 [0.000, 5.000],  loss: 0.014161, mae: 3.117950, mean_q: 3.761600, mean_eps: 0.400892\n",
      " 1664897/3750000: episode: 2335, duration: 3.781s, episode steps: 504, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.486 [0.000, 5.000],  loss: 0.014573, mae: 3.116247, mean_q: 3.753277, mean_eps: 0.400730\n",
      " 1665345/3750000: episode: 2336, duration: 3.380s, episode steps: 448, steps per second: 133, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.654 [0.000, 5.000],  loss: 0.017605, mae: 3.048004, mean_q: 3.665997, mean_eps: 0.400557\n",
      " 1665791/3750000: episode: 2337, duration: 3.288s, episode steps: 446, steps per second: 136, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.021162, mae: 3.121219, mean_q: 3.759884, mean_eps: 0.400395\n",
      " 1666508/3750000: episode: 2338, duration: 5.382s, episode steps: 717, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.022 [0.000, 5.000],  loss: 0.017804, mae: 3.073721, mean_q: 3.705197, mean_eps: 0.400186\n",
      " 1667378/3750000: episode: 2339, duration: 6.533s, episode steps: 870, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.013785, mae: 3.121966, mean_q: 3.769518, mean_eps: 0.399902\n",
      " 1668301/3750000: episode: 2340, duration: 7.007s, episode steps: 923, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.014521, mae: 3.073753, mean_q: 3.706415, mean_eps: 0.399578\n",
      " 1669337/3750000: episode: 2341, duration: 7.694s, episode steps: 1036, steps per second: 135, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.017032, mae: 3.129927, mean_q: 3.765526, mean_eps: 0.399225\n",
      " 1669954/3750000: episode: 2342, duration: 4.593s, episode steps: 617, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.012189, mae: 3.133958, mean_q: 3.773166, mean_eps: 0.398930\n",
      " 1671141/3750000: episode: 2343, duration: 8.912s, episode steps: 1187, steps per second: 133, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.013012, mae: 3.142257, mean_q: 3.781568, mean_eps: 0.398602\n",
      " 1671643/3750000: episode: 2344, duration: 3.731s, episode steps: 502, steps per second: 135, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.013557, mae: 3.098107, mean_q: 3.732029, mean_eps: 0.398296\n",
      " 1672654/3750000: episode: 2345, duration: 7.448s, episode steps: 1011, steps per second: 136, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011941, mae: 3.071418, mean_q: 3.695491, mean_eps: 0.398026\n",
      " 1673630/3750000: episode: 2346, duration: 7.372s, episode steps: 976, steps per second: 132, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.033 [0.000, 5.000],  loss: 0.014476, mae: 3.099882, mean_q: 3.736990, mean_eps: 0.397670\n",
      " 1674518/3750000: episode: 2347, duration: 6.605s, episode steps: 888, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.015535, mae: 3.111584, mean_q: 3.745516, mean_eps: 0.397335\n",
      " 1676046/3750000: episode: 2348, duration: 11.418s, episode steps: 1528, steps per second: 134, episode reward: 35.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.018766, mae: 3.098258, mean_q: 3.731676, mean_eps: 0.396899\n",
      " 1676836/3750000: episode: 2349, duration: 5.825s, episode steps: 790, steps per second: 136, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.016205, mae: 3.087135, mean_q: 3.718565, mean_eps: 0.396482\n",
      " 1677574/3750000: episode: 2350, duration: 5.560s, episode steps: 738, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.752 [0.000, 5.000],  loss: 0.020323, mae: 3.067634, mean_q: 3.691169, mean_eps: 0.396208\n",
      " 1678377/3750000: episode: 2351, duration: 5.959s, episode steps: 803, steps per second: 135, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.016421, mae: 3.107717, mean_q: 3.751749, mean_eps: 0.395931\n",
      " 1679005/3750000: episode: 2352, duration: 4.715s, episode steps: 628, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.015159, mae: 3.085573, mean_q: 3.719350, mean_eps: 0.395672\n",
      " 1679508/3750000: episode: 2353, duration: 3.755s, episode steps: 503, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.012429, mae: 3.140592, mean_q: 3.783908, mean_eps: 0.395466\n",
      " 1679944/3750000: episode: 2354, duration: 3.374s, episode steps: 436, steps per second: 129, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.016910, mae: 3.121223, mean_q: 3.757455, mean_eps: 0.395297\n",
      " 1681143/3750000: episode: 2355, duration: 8.874s, episode steps: 1199, steps per second: 135, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.016987, mae: 3.081912, mean_q: 3.708532, mean_eps: 0.395002\n",
      " 1682097/3750000: episode: 2356, duration: 7.111s, episode steps: 954, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.018288, mae: 3.135216, mean_q: 3.772154, mean_eps: 0.394617\n",
      " 1682802/3750000: episode: 2357, duration: 5.344s, episode steps: 705, steps per second: 132, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.014315, mae: 3.091317, mean_q: 3.726388, mean_eps: 0.394318\n",
      " 1683512/3750000: episode: 2358, duration: 5.249s, episode steps: 710, steps per second: 135, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.023431, mae: 3.074224, mean_q: 3.703642, mean_eps: 0.394062\n",
      " 1684528/3750000: episode: 2359, duration: 7.646s, episode steps: 1016, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013084, mae: 3.133535, mean_q: 3.771554, mean_eps: 0.393753\n",
      " 1685236/3750000: episode: 2360, duration: 5.262s, episode steps: 708, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.048 [0.000, 5.000],  loss: 0.017177, mae: 3.151890, mean_q: 3.791331, mean_eps: 0.393443\n",
      " 1685984/3750000: episode: 2361, duration: 5.634s, episode steps: 748, steps per second: 133, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.009 [0.000, 5.000],  loss: 0.017615, mae: 3.131212, mean_q: 3.772015, mean_eps: 0.393180\n",
      " 1686351/3750000: episode: 2362, duration: 2.746s, episode steps: 367, steps per second: 134, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.018077, mae: 3.133855, mean_q: 3.770093, mean_eps: 0.392979\n",
      " 1687087/3750000: episode: 2363, duration: 5.493s, episode steps: 736, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.016233, mae: 3.096180, mean_q: 3.732321, mean_eps: 0.392781\n",
      " 1688799/3750000: episode: 2364, duration: 12.715s, episode steps: 1712, steps per second: 135, episode reward: 41.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.018627, mae: 3.128874, mean_q: 3.766853, mean_eps: 0.392342\n",
      " 1689420/3750000: episode: 2365, duration: 4.719s, episode steps: 621, steps per second: 132, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.016784, mae: 3.158909, mean_q: 3.797498, mean_eps: 0.391924\n",
      " 1690045/3750000: episode: 2366, duration: 4.726s, episode steps: 625, steps per second: 132, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.074 [0.000, 5.000],  loss: 0.015831, mae: 3.133315, mean_q: 3.780052, mean_eps: 0.391697\n",
      " 1691117/3750000: episode: 2367, duration: 7.944s, episode steps: 1072, steps per second: 135, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.013452, mae: 3.129760, mean_q: 3.778324, mean_eps: 0.391391\n",
      " 1692200/3750000: episode: 2368, duration: 8.064s, episode steps: 1083, steps per second: 134, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.018827, mae: 3.144936, mean_q: 3.785048, mean_eps: 0.391006\n",
      " 1692932/3750000: episode: 2369, duration: 5.588s, episode steps: 732, steps per second: 131, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.762 [0.000, 5.000],  loss: 0.013859, mae: 3.134419, mean_q: 3.782918, mean_eps: 0.390678\n",
      " 1693815/3750000: episode: 2370, duration: 6.742s, episode steps: 883, steps per second: 131, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.966 [0.000, 5.000],  loss: 0.016642, mae: 3.158984, mean_q: 3.806923, mean_eps: 0.390387\n",
      " 1694851/3750000: episode: 2371, duration: 7.838s, episode steps: 1036, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.017957, mae: 3.148018, mean_q: 3.784698, mean_eps: 0.390041\n",
      " 1695536/3750000: episode: 2372, duration: 5.142s, episode steps: 685, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.495 [0.000, 5.000],  loss: 0.016968, mae: 3.232734, mean_q: 3.898771, mean_eps: 0.389732\n",
      " 1696439/3750000: episode: 2373, duration: 6.705s, episode steps: 903, steps per second: 135, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.805 [0.000, 5.000],  loss: 0.017129, mae: 3.203771, mean_q: 3.858261, mean_eps: 0.389447\n",
      " 1697281/3750000: episode: 2374, duration: 6.421s, episode steps: 842, steps per second: 131, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.090 [0.000, 5.000],  loss: 0.019582, mae: 3.199534, mean_q: 3.855342, mean_eps: 0.389130\n",
      " 1698068/3750000: episode: 2375, duration: 5.844s, episode steps: 787, steps per second: 135, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.022567, mae: 3.105046, mean_q: 3.736849, mean_eps: 0.388835\n",
      " 1699032/3750000: episode: 2376, duration: 7.215s, episode steps: 964, steps per second: 134, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.017332, mae: 3.152919, mean_q: 3.792672, mean_eps: 0.388522\n",
      " 1699827/3750000: episode: 2377, duration: 5.910s, episode steps: 795, steps per second: 135, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.014251, mae: 3.142092, mean_q: 3.779424, mean_eps: 0.388205\n",
      " 1700994/3750000: episode: 2378, duration: 8.719s, episode steps: 1167, steps per second: 134, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.017288, mae: 3.142539, mean_q: 3.784011, mean_eps: 0.387852\n",
      " 1701624/3750000: episode: 2379, duration: 4.774s, episode steps: 630, steps per second: 132, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.022844, mae: 3.124860, mean_q: 3.762984, mean_eps: 0.387528\n",
      " 1702816/3750000: episode: 2380, duration: 8.819s, episode steps: 1192, steps per second: 135, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.017672, mae: 3.095395, mean_q: 3.723262, mean_eps: 0.387201\n",
      " 1703657/3750000: episode: 2381, duration: 6.336s, episode steps: 841, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.012500, mae: 3.119305, mean_q: 3.752952, mean_eps: 0.386837\n",
      " 1705028/3750000: episode: 2382, duration: 10.207s, episode steps: 1371, steps per second: 134, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.716 [0.000, 5.000],  loss: 0.012712, mae: 3.087850, mean_q: 3.717076, mean_eps: 0.386438\n",
      " 1705908/3750000: episode: 2383, duration: 6.589s, episode steps: 880, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.015327, mae: 3.102295, mean_q: 3.733196, mean_eps: 0.386031\n",
      " 1706418/3750000: episode: 2384, duration: 3.736s, episode steps: 510, steps per second: 137, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015000, mae: 3.144977, mean_q: 3.783201, mean_eps: 0.385782\n",
      " 1707081/3750000: episode: 2385, duration: 5.004s, episode steps: 663, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.119 [0.000, 5.000],  loss: 0.017735, mae: 3.167296, mean_q: 3.807920, mean_eps: 0.385570\n",
      " 1708099/3750000: episode: 2386, duration: 7.561s, episode steps: 1018, steps per second: 135, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.015006, mae: 3.098145, mean_q: 3.724371, mean_eps: 0.385268\n",
      " 1708960/3750000: episode: 2387, duration: 6.488s, episode steps: 861, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011942, mae: 3.131489, mean_q: 3.767854, mean_eps: 0.384933\n",
      " 1709657/3750000: episode: 2388, duration: 5.269s, episode steps: 697, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.017931, mae: 3.114622, mean_q: 3.748019, mean_eps: 0.384652\n",
      " 1710606/3750000: episode: 2389, duration: 7.082s, episode steps: 949, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.017707, mae: 3.073426, mean_q: 3.697696, mean_eps: 0.384353\n",
      " 1711652/3750000: episode: 2390, duration: 7.798s, episode steps: 1046, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.145 [0.000, 5.000],  loss: 0.021535, mae: 3.073282, mean_q: 3.711811, mean_eps: 0.383993\n",
      " 1712545/3750000: episode: 2391, duration: 6.724s, episode steps: 893, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.014401, mae: 3.130782, mean_q: 3.770493, mean_eps: 0.383644\n",
      " 1713420/3750000: episode: 2392, duration: 6.513s, episode steps: 875, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.018230, mae: 3.094179, mean_q: 3.727394, mean_eps: 0.383327\n",
      " 1714338/3750000: episode: 2393, duration: 6.971s, episode steps: 918, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.017208, mae: 3.087226, mean_q: 3.715083, mean_eps: 0.383007\n",
      " 1715562/3750000: episode: 2394, duration: 9.162s, episode steps: 1224, steps per second: 134, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.018188, mae: 3.071043, mean_q: 3.697569, mean_eps: 0.382618\n",
      " 1716709/3750000: episode: 2395, duration: 8.525s, episode steps: 1147, steps per second: 135, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.118 [0.000, 5.000],  loss: 0.016370, mae: 3.112797, mean_q: 3.749940, mean_eps: 0.382190\n",
      " 1717494/3750000: episode: 2396, duration: 5.822s, episode steps: 785, steps per second: 135, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.016694, mae: 3.120366, mean_q: 3.756378, mean_eps: 0.381844\n",
      " 1718191/3750000: episode: 2397, duration: 5.230s, episode steps: 697, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013183, mae: 3.175170, mean_q: 3.820721, mean_eps: 0.381578\n",
      " 1718689/3750000: episode: 2398, duration: 3.734s, episode steps: 498, steps per second: 133, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.253 [0.000, 5.000],  loss: 0.015282, mae: 3.048488, mean_q: 3.667953, mean_eps: 0.381362\n",
      " 1719400/3750000: episode: 2399, duration: 5.311s, episode steps: 711, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.019070, mae: 3.117645, mean_q: 3.754608, mean_eps: 0.381146\n",
      " 1719796/3750000: episode: 2400, duration: 2.980s, episode steps: 396, steps per second: 133, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.017154, mae: 3.044018, mean_q: 3.682173, mean_eps: 0.380948\n",
      " 1720532/3750000: episode: 2401, duration: 5.595s, episode steps: 736, steps per second: 132, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.023488, mae: 3.059655, mean_q: 3.678271, mean_eps: 0.380742\n",
      " 1721043/3750000: episode: 2402, duration: 3.912s, episode steps: 511, steps per second: 131, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.975 [0.000, 5.000],  loss: 0.018707, mae: 3.107912, mean_q: 3.735972, mean_eps: 0.380516\n",
      " 1721707/3750000: episode: 2403, duration: 4.904s, episode steps: 664, steps per second: 135, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.015434, mae: 3.023853, mean_q: 3.637347, mean_eps: 0.380303\n",
      " 1722836/3750000: episode: 2404, duration: 8.439s, episode steps: 1129, steps per second: 134, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.606 [0.000, 5.000],  loss: 0.014495, mae: 3.072496, mean_q: 3.707967, mean_eps: 0.379983\n",
      " 1723233/3750000: episode: 2405, duration: 2.993s, episode steps: 397, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.012206, mae: 3.126429, mean_q: 3.764699, mean_eps: 0.379709\n",
      " 1723777/3750000: episode: 2406, duration: 4.062s, episode steps: 544, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.016380, mae: 3.059261, mean_q: 3.681553, mean_eps: 0.379540\n",
      " 1724178/3750000: episode: 2407, duration: 3.007s, episode steps: 401, steps per second: 133, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.017599, mae: 3.103251, mean_q: 3.734754, mean_eps: 0.379371\n",
      " 1724829/3750000: episode: 2408, duration: 4.946s, episode steps: 651, steps per second: 132, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015914, mae: 3.083242, mean_q: 3.709956, mean_eps: 0.379180\n",
      " 1725531/3750000: episode: 2409, duration: 5.227s, episode steps: 702, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.014497, mae: 3.091245, mean_q: 3.720504, mean_eps: 0.378935\n",
      " 1725878/3750000: episode: 2410, duration: 2.547s, episode steps: 347, steps per second: 136, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 3.493 [0.000, 5.000],  loss: 0.013312, mae: 3.097297, mean_q: 3.727174, mean_eps: 0.378748\n",
      " 1726478/3750000: episode: 2411, duration: 4.508s, episode steps: 600, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.016161, mae: 3.092026, mean_q: 3.722359, mean_eps: 0.378579\n",
      " 1727696/3750000: episode: 2412, duration: 9.219s, episode steps: 1218, steps per second: 132, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.014313, mae: 3.096387, mean_q: 3.727842, mean_eps: 0.378251\n",
      " 1728507/3750000: episode: 2413, duration: 6.084s, episode steps: 811, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.014743, mae: 3.127832, mean_q: 3.761946, mean_eps: 0.377884\n",
      " 1729793/3750000: episode: 2414, duration: 9.609s, episode steps: 1286, steps per second: 134, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.012265, mae: 3.066237, mean_q: 3.687869, mean_eps: 0.377506\n",
      " 1730965/3750000: episode: 2415, duration: 8.827s, episode steps: 1172, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.017646, mae: 3.067001, mean_q: 3.687085, mean_eps: 0.377063\n",
      " 1731901/3750000: episode: 2416, duration: 6.982s, episode steps: 936, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.014442, mae: 3.060272, mean_q: 3.686103, mean_eps: 0.376682\n",
      " 1732570/3750000: episode: 2417, duration: 4.952s, episode steps: 669, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.016142, mae: 3.030996, mean_q: 3.653126, mean_eps: 0.376394\n",
      " 1733332/3750000: episode: 2418, duration: 5.708s, episode steps: 762, steps per second: 134, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.010774, mae: 3.065519, mean_q: 3.688401, mean_eps: 0.376138\n",
      " 1733833/3750000: episode: 2419, duration: 3.720s, episode steps: 501, steps per second: 135, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.013272, mae: 3.043170, mean_q: 3.660367, mean_eps: 0.375911\n",
      " 1734691/3750000: episode: 2420, duration: 6.431s, episode steps: 858, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.016843, mae: 3.005357, mean_q: 3.612785, mean_eps: 0.375666\n",
      " 1735468/3750000: episode: 2421, duration: 5.845s, episode steps: 777, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.604 [0.000, 5.000],  loss: 0.016640, mae: 3.039705, mean_q: 3.658585, mean_eps: 0.375371\n",
      " 1735962/3750000: episode: 2422, duration: 3.695s, episode steps: 494, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.017185, mae: 3.051571, mean_q: 3.667075, mean_eps: 0.375141\n",
      " 1736887/3750000: episode: 2423, duration: 6.899s, episode steps: 925, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.107 [0.000, 5.000],  loss: 0.018795, mae: 3.041077, mean_q: 3.660745, mean_eps: 0.374885\n",
      " 1737387/3750000: episode: 2424, duration: 3.770s, episode steps: 500, steps per second: 133, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.364 [0.000, 5.000],  loss: 0.013316, mae: 2.998832, mean_q: 3.606929, mean_eps: 0.374630\n",
      " 1738704/3750000: episode: 2425, duration: 9.772s, episode steps: 1317, steps per second: 135, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.015534, mae: 3.033117, mean_q: 3.646578, mean_eps: 0.374302\n",
      " 1739828/3750000: episode: 2426, duration: 8.522s, episode steps: 1124, steps per second: 132, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.017999, mae: 3.097893, mean_q: 3.723014, mean_eps: 0.373863\n",
      " 1741209/3750000: episode: 2427, duration: 10.231s, episode steps: 1381, steps per second: 135, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.016807, mae: 3.024196, mean_q: 3.641765, mean_eps: 0.373413\n",
      " 1742466/3750000: episode: 2428, duration: 9.492s, episode steps: 1257, steps per second: 132, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.019503, mae: 3.071394, mean_q: 3.697993, mean_eps: 0.372938\n",
      " 1743144/3750000: episode: 2429, duration: 5.113s, episode steps: 678, steps per second: 133, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.985 [0.000, 5.000],  loss: 0.017141, mae: 3.092053, mean_q: 3.718561, mean_eps: 0.372588\n",
      " 1743697/3750000: episode: 2430, duration: 4.088s, episode steps: 553, steps per second: 135, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.015487, mae: 3.084676, mean_q: 3.710987, mean_eps: 0.372369\n",
      " 1744432/3750000: episode: 2431, duration: 5.515s, episode steps: 735, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.013678, mae: 3.107411, mean_q: 3.743181, mean_eps: 0.372138\n",
      " 1745376/3750000: episode: 2432, duration: 7.025s, episode steps: 944, steps per second: 134, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.015922, mae: 3.128004, mean_q: 3.774641, mean_eps: 0.371836\n",
      " 1745924/3750000: episode: 2433, duration: 4.196s, episode steps: 548, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.018381, mae: 3.163352, mean_q: 3.821446, mean_eps: 0.371566\n",
      " 1747188/3750000: episode: 2434, duration: 9.382s, episode steps: 1264, steps per second: 135, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.017854, mae: 3.132793, mean_q: 3.774055, mean_eps: 0.371238\n",
      " 1747793/3750000: episode: 2435, duration: 4.482s, episode steps: 605, steps per second: 135, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.015235, mae: 3.160580, mean_q: 3.798943, mean_eps: 0.370904\n",
      " 1748636/3750000: episode: 2436, duration: 6.314s, episode steps: 843, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.023765, mae: 3.107544, mean_q: 3.738862, mean_eps: 0.370644\n",
      " 1749119/3750000: episode: 2437, duration: 3.588s, episode steps: 483, steps per second: 135, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.011965, mae: 3.106331, mean_q: 3.748103, mean_eps: 0.370407\n",
      " 1750066/3750000: episode: 2438, duration: 7.383s, episode steps: 947, steps per second: 128, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.016737, mae: 3.106060, mean_q: 3.742866, mean_eps: 0.370148\n",
      " 1750686/3750000: episode: 2439, duration: 4.819s, episode steps: 620, steps per second: 129, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.013481, mae: 3.124262, mean_q: 3.761780, mean_eps: 0.369863\n",
      " 1751019/3750000: episode: 2440, duration: 2.430s, episode steps: 333, steps per second: 137, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.013771, mae: 3.179297, mean_q: 3.834806, mean_eps: 0.369694\n",
      " 1752177/3750000: episode: 2441, duration: 8.585s, episode steps: 1158, steps per second: 135, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.014955, mae: 3.137675, mean_q: 3.778082, mean_eps: 0.369428\n",
      " 1753215/3750000: episode: 2442, duration: 7.765s, episode steps: 1038, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.018110, mae: 3.132886, mean_q: 3.781297, mean_eps: 0.369032\n",
      " 1753732/3750000: episode: 2443, duration: 3.873s, episode steps: 517, steps per second: 133, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.016588, mae: 3.167869, mean_q: 3.816428, mean_eps: 0.368751\n",
      " 1754546/3750000: episode: 2444, duration: 6.150s, episode steps: 814, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.016773, mae: 3.107042, mean_q: 3.742492, mean_eps: 0.368510\n",
      " 1755114/3750000: episode: 2445, duration: 4.325s, episode steps: 568, steps per second: 131, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.017748, mae: 3.116334, mean_q: 3.763617, mean_eps: 0.368261\n",
      " 1755695/3750000: episode: 2446, duration: 4.333s, episode steps: 581, steps per second: 134, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.013812, mae: 3.154504, mean_q: 3.798453, mean_eps: 0.368056\n",
      " 1756503/3750000: episode: 2447, duration: 6.135s, episode steps: 808, steps per second: 132, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.015155, mae: 3.148133, mean_q: 3.788674, mean_eps: 0.367804\n",
      " 1757362/3750000: episode: 2448, duration: 6.415s, episode steps: 859, steps per second: 134, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.026 [0.000, 5.000],  loss: 0.013492, mae: 3.158161, mean_q: 3.807861, mean_eps: 0.367502\n",
      " 1757901/3750000: episode: 2449, duration: 4.018s, episode steps: 539, steps per second: 134, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.774 [0.000, 5.000],  loss: 0.012225, mae: 3.112310, mean_q: 3.750837, mean_eps: 0.367250\n",
      " 1758418/3750000: episode: 2450, duration: 3.777s, episode steps: 517, steps per second: 137, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.133 [0.000, 5.000],  loss: 0.022424, mae: 3.164439, mean_q: 3.809889, mean_eps: 0.367062\n",
      " 1759070/3750000: episode: 2451, duration: 4.987s, episode steps: 652, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.013358, mae: 3.115813, mean_q: 3.754015, mean_eps: 0.366854\n",
      " 1759574/3750000: episode: 2452, duration: 3.741s, episode steps: 504, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.018529, mae: 3.132248, mean_q: 3.772073, mean_eps: 0.366645\n",
      " 1760453/3750000: episode: 2453, duration: 6.544s, episode steps: 879, steps per second: 134, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.015509, mae: 3.154751, mean_q: 3.800203, mean_eps: 0.366396\n",
      " 1761084/3750000: episode: 2454, duration: 4.811s, episode steps: 631, steps per second: 131, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.494 [0.000, 5.000],  loss: 0.014779, mae: 3.176100, mean_q: 3.831905, mean_eps: 0.366123\n",
      " 1762045/3750000: episode: 2455, duration: 7.139s, episode steps: 961, steps per second: 135, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.048 [0.000, 5.000],  loss: 0.020929, mae: 3.183005, mean_q: 3.831831, mean_eps: 0.365835\n",
      " 1762724/3750000: episode: 2456, duration: 5.072s, episode steps: 679, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.013962, mae: 3.269411, mean_q: 3.941927, mean_eps: 0.365540\n",
      " 1763516/3750000: episode: 2457, duration: 5.890s, episode steps: 792, steps per second: 134, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.015989, mae: 3.233726, mean_q: 3.892762, mean_eps: 0.365277\n",
      " 1764330/3750000: episode: 2458, duration: 6.094s, episode steps: 814, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.019392, mae: 3.200434, mean_q: 3.855715, mean_eps: 0.364989\n",
      " 1764827/3750000: episode: 2459, duration: 3.724s, episode steps: 497, steps per second: 133, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.013396, mae: 3.229514, mean_q: 3.905340, mean_eps: 0.364751\n",
      " 1765769/3750000: episode: 2460, duration: 7.037s, episode steps: 942, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.016260, mae: 3.184024, mean_q: 3.833643, mean_eps: 0.364492\n",
      " 1766510/3750000: episode: 2461, duration: 5.528s, episode steps: 741, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.015892, mae: 3.265143, mean_q: 3.929636, mean_eps: 0.364190\n",
      " 1767169/3750000: episode: 2462, duration: 4.941s, episode steps: 659, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.015190, mae: 3.201470, mean_q: 3.853834, mean_eps: 0.363938\n",
      " 1767885/3750000: episode: 2463, duration: 5.454s, episode steps: 716, steps per second: 131, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.012740, mae: 3.153010, mean_q: 3.795308, mean_eps: 0.363689\n",
      " 1768703/3750000: episode: 2464, duration: 6.083s, episode steps: 818, steps per second: 134, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.131 [0.000, 5.000],  loss: 0.016252, mae: 3.142468, mean_q: 3.779416, mean_eps: 0.363412\n",
      " 1769352/3750000: episode: 2465, duration: 4.804s, episode steps: 649, steps per second: 135, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.013529, mae: 3.146202, mean_q: 3.785030, mean_eps: 0.363149\n",
      " 1769716/3750000: episode: 2466, duration: 2.730s, episode steps: 364, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.514 [0.000, 5.000],  loss: 0.014153, mae: 3.084363, mean_q: 3.710635, mean_eps: 0.362969\n",
      " 1770888/3750000: episode: 2467, duration: 8.746s, episode steps: 1172, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.014081, mae: 3.162291, mean_q: 3.803270, mean_eps: 0.362692\n",
      " 1771756/3750000: episode: 2468, duration: 6.455s, episode steps: 868, steps per second: 134, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.018789, mae: 3.105320, mean_q: 3.737257, mean_eps: 0.362325\n",
      " 1772233/3750000: episode: 2469, duration: 3.570s, episode steps: 477, steps per second: 134, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.014045, mae: 3.116136, mean_q: 3.748856, mean_eps: 0.362084\n",
      " 1772752/3750000: episode: 2470, duration: 3.862s, episode steps: 519, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.268 [0.000, 5.000],  loss: 0.012031, mae: 3.093155, mean_q: 3.729366, mean_eps: 0.361904\n",
      " 1773795/3750000: episode: 2471, duration: 7.788s, episode steps: 1043, steps per second: 134, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.013753, mae: 3.127618, mean_q: 3.765433, mean_eps: 0.361623\n",
      " 1774401/3750000: episode: 2472, duration: 4.568s, episode steps: 606, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.024413, mae: 3.122910, mean_q: 3.758294, mean_eps: 0.361324\n",
      " 1775260/3750000: episode: 2473, duration: 6.503s, episode steps: 859, steps per second: 132, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.018253, mae: 3.129140, mean_q: 3.762597, mean_eps: 0.361061\n",
      " 1776260/3750000: episode: 2474, duration: 7.614s, episode steps: 1000, steps per second: 131, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.025243, mae: 3.123776, mean_q: 3.755950, mean_eps: 0.360730\n",
      " 1776860/3750000: episode: 2475, duration: 4.528s, episode steps: 600, steps per second: 132, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.023359, mae: 3.117216, mean_q: 3.751917, mean_eps: 0.360442\n",
      " 1777588/3750000: episode: 2476, duration: 5.474s, episode steps: 728, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.017097, mae: 3.146188, mean_q: 3.785097, mean_eps: 0.360201\n",
      " 1778939/3750000: episode: 2477, duration: 10.042s, episode steps: 1351, steps per second: 135, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.016518, mae: 3.172409, mean_q: 3.822615, mean_eps: 0.359826\n",
      " 1779923/3750000: episode: 2478, duration: 7.440s, episode steps: 984, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.013186, mae: 3.154954, mean_q: 3.798024, mean_eps: 0.359405\n",
      " 1780985/3750000: episode: 2479, duration: 7.935s, episode steps: 1062, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.021291, mae: 3.136129, mean_q: 3.771500, mean_eps: 0.359034\n",
      " 1781709/3750000: episode: 2480, duration: 5.369s, episode steps: 724, steps per second: 135, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.013402, mae: 3.107488, mean_q: 3.735205, mean_eps: 0.358714\n",
      " 1782519/3750000: episode: 2481, duration: 6.017s, episode steps: 810, steps per second: 135, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014301, mae: 3.117974, mean_q: 3.748480, mean_eps: 0.358440\n",
      " 1783478/3750000: episode: 2482, duration: 7.109s, episode steps: 959, steps per second: 135, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.016408, mae: 3.095553, mean_q: 3.722831, mean_eps: 0.358124\n",
      " 1783939/3750000: episode: 2483, duration: 3.410s, episode steps: 461, steps per second: 135, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.965 [0.000, 5.000],  loss: 0.018010, mae: 3.126923, mean_q: 3.766220, mean_eps: 0.357868\n",
      " 1785447/3750000: episode: 2484, duration: 11.295s, episode steps: 1508, steps per second: 134, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.013353, mae: 3.086446, mean_q: 3.716294, mean_eps: 0.357512\n",
      " 1785864/3750000: episode: 2485, duration: 3.119s, episode steps: 417, steps per second: 134, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.015729, mae: 3.093417, mean_q: 3.719515, mean_eps: 0.357162\n",
      " 1786409/3750000: episode: 2486, duration: 4.074s, episode steps: 545, steps per second: 134, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.014888, mae: 3.041954, mean_q: 3.658539, mean_eps: 0.356990\n",
      " 1787284/3750000: episode: 2487, duration: 6.566s, episode steps: 875, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.015725, mae: 3.072991, mean_q: 3.698706, mean_eps: 0.356734\n",
      " 1788275/3750000: episode: 2488, duration: 7.307s, episode steps: 991, steps per second: 136, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.012993, mae: 3.110511, mean_q: 3.745983, mean_eps: 0.356399\n",
      " 1789109/3750000: episode: 2489, duration: 6.302s, episode steps: 834, steps per second: 132, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.013415, mae: 3.085135, mean_q: 3.714247, mean_eps: 0.356072\n",
      " 1789463/3750000: episode: 2490, duration: 2.668s, episode steps: 354, steps per second: 133, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.701 [0.000, 5.000],  loss: 0.015082, mae: 3.080284, mean_q: 3.716302, mean_eps: 0.355856\n",
      " 1790552/3750000: episode: 2491, duration: 8.052s, episode steps: 1089, steps per second: 135, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.013263, mae: 3.074796, mean_q: 3.697104, mean_eps: 0.355596\n",
      " 1791470/3750000: episode: 2492, duration: 6.985s, episode steps: 918, steps per second: 131, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013086, mae: 3.032109, mean_q: 3.648275, mean_eps: 0.355236\n",
      " 1792383/3750000: episode: 2493, duration: 6.816s, episode steps: 913, steps per second: 134, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.017767, mae: 3.062136, mean_q: 3.686151, mean_eps: 0.354905\n",
      " 1793285/3750000: episode: 2494, duration: 6.761s, episode steps: 902, steps per second: 133, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.504 [0.000, 5.000],  loss: 0.017256, mae: 3.075754, mean_q: 3.702742, mean_eps: 0.354578\n",
      " 1793918/3750000: episode: 2495, duration: 4.696s, episode steps: 633, steps per second: 135, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.015612, mae: 3.026096, mean_q: 3.633548, mean_eps: 0.354304\n",
      " 1794866/3750000: episode: 2496, duration: 7.112s, episode steps: 948, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.014117, mae: 3.089199, mean_q: 3.720208, mean_eps: 0.354020\n",
      " 1795851/3750000: episode: 2497, duration: 7.346s, episode steps: 985, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.015530, mae: 3.108540, mean_q: 3.741710, mean_eps: 0.353670\n",
      " 1796503/3750000: episode: 2498, duration: 4.894s, episode steps: 652, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.018198, mae: 3.131232, mean_q: 3.773795, mean_eps: 0.353375\n",
      " 1797530/3750000: episode: 2499, duration: 7.660s, episode steps: 1027, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.012444, mae: 3.117441, mean_q: 3.751560, mean_eps: 0.353073\n",
      " 1798839/3750000: episode: 2500, duration: 9.623s, episode steps: 1309, steps per second: 136, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.013432, mae: 3.063828, mean_q: 3.684235, mean_eps: 0.352655\n",
      " 1799769/3750000: episode: 2501, duration: 7.080s, episode steps: 930, steps per second: 131, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.016898, mae: 3.083571, mean_q: 3.707291, mean_eps: 0.352252\n",
      " 1800538/3750000: episode: 2502, duration: 5.752s, episode steps: 769, steps per second: 134, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.017984, mae: 3.068280, mean_q: 3.690881, mean_eps: 0.351946\n",
      " 1801027/3750000: episode: 2503, duration: 3.708s, episode steps: 489, steps per second: 132, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.104 [0.000, 5.000],  loss: 0.012488, mae: 3.058466, mean_q: 3.687987, mean_eps: 0.351719\n",
      " 1802234/3750000: episode: 2504, duration: 9.002s, episode steps: 1207, steps per second: 134, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.014624, mae: 3.097427, mean_q: 3.732685, mean_eps: 0.351413\n",
      " 1803667/3750000: episode: 2505, duration: 10.692s, episode steps: 1433, steps per second: 134, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.014957, mae: 3.065293, mean_q: 3.690076, mean_eps: 0.350938\n",
      " 1804633/3750000: episode: 2506, duration: 7.205s, episode steps: 966, steps per second: 134, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.012658, mae: 3.053278, mean_q: 3.677066, mean_eps: 0.350506\n",
      " 1805263/3750000: episode: 2507, duration: 4.743s, episode steps: 630, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.016473, mae: 3.096126, mean_q: 3.724523, mean_eps: 0.350218\n",
      " 1806163/3750000: episode: 2508, duration: 6.760s, episode steps: 900, steps per second: 133, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.012575, mae: 3.051378, mean_q: 3.672162, mean_eps: 0.349941\n",
      " 1807469/3750000: episode: 2509, duration: 9.757s, episode steps: 1306, steps per second: 134, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.015027, mae: 3.084369, mean_q: 3.712113, mean_eps: 0.349545\n",
      " 1808192/3750000: episode: 2510, duration: 5.517s, episode steps: 723, steps per second: 131, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.012298, mae: 3.055580, mean_q: 3.686952, mean_eps: 0.349181\n",
      " 1808957/3750000: episode: 2511, duration: 5.670s, episode steps: 765, steps per second: 135, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.013065, mae: 3.065486, mean_q: 3.687962, mean_eps: 0.348915\n",
      " 1809658/3750000: episode: 2512, duration: 5.245s, episode steps: 701, steps per second: 134, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.937 [0.000, 5.000],  loss: 0.015103, mae: 3.072690, mean_q: 3.697890, mean_eps: 0.348652\n",
      " 1810635/3750000: episode: 2513, duration: 7.368s, episode steps: 977, steps per second: 133, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.017975, mae: 3.084361, mean_q: 3.708253, mean_eps: 0.348350\n",
      " 1811742/3750000: episode: 2514, duration: 8.294s, episode steps: 1107, steps per second: 133, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.014981, mae: 3.048085, mean_q: 3.672614, mean_eps: 0.347972\n",
      " 1812407/3750000: episode: 2515, duration: 4.996s, episode steps: 665, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.016177, mae: 3.024670, mean_q: 3.643664, mean_eps: 0.347651\n",
      " 1813145/3750000: episode: 2516, duration: 5.472s, episode steps: 738, steps per second: 135, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.022025, mae: 3.038506, mean_q: 3.655990, mean_eps: 0.347399\n",
      " 1814009/3750000: episode: 2517, duration: 6.399s, episode steps: 864, steps per second: 135, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.013167, mae: 3.084948, mean_q: 3.714534, mean_eps: 0.347111\n",
      " 1814782/3750000: episode: 2518, duration: 5.841s, episode steps: 773, steps per second: 132, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.102 [0.000, 5.000],  loss: 0.017045, mae: 3.034943, mean_q: 3.652048, mean_eps: 0.346816\n",
      " 1815735/3750000: episode: 2519, duration: 7.015s, episode steps: 953, steps per second: 136, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.252 [0.000, 5.000],  loss: 0.016988, mae: 3.005410, mean_q: 3.618218, mean_eps: 0.346506\n",
      " 1816480/3750000: episode: 2520, duration: 5.678s, episode steps: 745, steps per second: 131, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.011782, mae: 3.022334, mean_q: 3.636823, mean_eps: 0.346204\n",
      " 1816997/3750000: episode: 2521, duration: 3.903s, episode steps: 517, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.996 [0.000, 5.000],  loss: 0.010909, mae: 3.022060, mean_q: 3.638524, mean_eps: 0.345977\n",
      " 1817487/3750000: episode: 2522, duration: 3.689s, episode steps: 490, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.014411, mae: 2.999652, mean_q: 3.613081, mean_eps: 0.345794\n",
      " 1818152/3750000: episode: 2523, duration: 4.914s, episode steps: 665, steps per second: 135, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.012072, mae: 3.038705, mean_q: 3.663037, mean_eps: 0.345585\n",
      " 1818831/3750000: episode: 2524, duration: 5.166s, episode steps: 679, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.667 [0.000, 5.000],  loss: 0.015161, mae: 3.015823, mean_q: 3.632919, mean_eps: 0.345344\n",
      " 1819409/3750000: episode: 2525, duration: 4.313s, episode steps: 578, steps per second: 134, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.010701, mae: 3.016935, mean_q: 3.633101, mean_eps: 0.345117\n",
      " 1820163/3750000: episode: 2526, duration: 5.627s, episode steps: 754, steps per second: 134, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.014547, mae: 2.995045, mean_q: 3.605440, mean_eps: 0.344876\n",
      " 1820717/3750000: episode: 2527, duration: 4.108s, episode steps: 554, steps per second: 135, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.606 [0.000, 5.000],  loss: 0.018736, mae: 3.004595, mean_q: 3.616294, mean_eps: 0.344642\n",
      " 1821501/3750000: episode: 2528, duration: 5.915s, episode steps: 784, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.014927, mae: 2.976808, mean_q: 3.585683, mean_eps: 0.344400\n",
      " 1821975/3750000: episode: 2529, duration: 3.467s, episode steps: 474, steps per second: 137, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.816 [0.000, 5.000],  loss: 0.013745, mae: 2.995984, mean_q: 3.604156, mean_eps: 0.344174\n",
      " 1822657/3750000: episode: 2530, duration: 5.093s, episode steps: 682, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.238 [0.000, 5.000],  loss: 0.023586, mae: 3.023881, mean_q: 3.634077, mean_eps: 0.343968\n",
      " 1823319/3750000: episode: 2531, duration: 5.024s, episode steps: 662, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.016655, mae: 3.016398, mean_q: 3.629455, mean_eps: 0.343727\n",
      " 1824189/3750000: episode: 2532, duration: 6.510s, episode steps: 870, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.016746, mae: 2.982520, mean_q: 3.592340, mean_eps: 0.343450\n",
      " 1825351/3750000: episode: 2533, duration: 8.723s, episode steps: 1162, steps per second: 133, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.015088, mae: 3.053488, mean_q: 3.674606, mean_eps: 0.343083\n",
      " 1825941/3750000: episode: 2534, duration: 4.452s, episode steps: 590, steps per second: 133, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.012716, mae: 3.012586, mean_q: 3.622106, mean_eps: 0.342766\n",
      " 1826594/3750000: episode: 2535, duration: 4.804s, episode steps: 653, steps per second: 136, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.010433, mae: 3.087086, mean_q: 3.718448, mean_eps: 0.342543\n",
      " 1827598/3750000: episode: 2536, duration: 7.494s, episode steps: 1004, steps per second: 134, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.600 [0.000, 5.000],  loss: 0.016833, mae: 3.058725, mean_q: 3.689167, mean_eps: 0.342248\n",
      " 1828464/3750000: episode: 2537, duration: 6.503s, episode steps: 866, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.013924, mae: 3.082672, mean_q: 3.709913, mean_eps: 0.341909\n",
      " 1829625/3750000: episode: 2538, duration: 8.657s, episode steps: 1161, steps per second: 134, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.020920, mae: 3.062113, mean_q: 3.684196, mean_eps: 0.341542\n",
      " 1830650/3750000: episode: 2539, duration: 7.559s, episode steps: 1025, steps per second: 136, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.017863, mae: 3.027209, mean_q: 3.641892, mean_eps: 0.341150\n",
      " 1831027/3750000: episode: 2540, duration: 2.831s, episode steps: 377, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.130 [0.000, 5.000],  loss: 0.015969, mae: 3.075283, mean_q: 3.702368, mean_eps: 0.340898\n",
      " 1832067/3750000: episode: 2541, duration: 7.774s, episode steps: 1040, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.813 [0.000, 5.000],  loss: 0.012735, mae: 3.071364, mean_q: 3.697921, mean_eps: 0.340642\n",
      " 1832814/3750000: episode: 2542, duration: 5.502s, episode steps: 747, steps per second: 136, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.014215, mae: 3.042600, mean_q: 3.662305, mean_eps: 0.340322\n",
      " 1833659/3750000: episode: 2543, duration: 6.303s, episode steps: 845, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.013580, mae: 3.014184, mean_q: 3.626378, mean_eps: 0.340037\n",
      " 1834614/3750000: episode: 2544, duration: 7.138s, episode steps: 955, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.012901, mae: 3.055321, mean_q: 3.674769, mean_eps: 0.339713\n",
      " 1835174/3750000: episode: 2545, duration: 4.179s, episode steps: 560, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.886 [0.000, 5.000],  loss: 0.010739, mae: 3.021024, mean_q: 3.635939, mean_eps: 0.339440\n",
      " 1835704/3750000: episode: 2546, duration: 4.032s, episode steps: 530, steps per second: 131, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.847 [0.000, 5.000],  loss: 0.011334, mae: 3.050455, mean_q: 3.676905, mean_eps: 0.339242\n",
      " 1836374/3750000: episode: 2547, duration: 4.984s, episode steps: 670, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.017327, mae: 3.052815, mean_q: 3.678256, mean_eps: 0.339026\n",
      " 1836932/3750000: episode: 2548, duration: 4.150s, episode steps: 558, steps per second: 134, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.017435, mae: 3.021677, mean_q: 3.634980, mean_eps: 0.338806\n",
      " 1837457/3750000: episode: 2549, duration: 3.905s, episode steps: 525, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.015560, mae: 3.067189, mean_q: 3.692693, mean_eps: 0.338612\n",
      " 1838313/3750000: episode: 2550, duration: 6.432s, episode steps: 856, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.014706, mae: 3.041938, mean_q: 3.666048, mean_eps: 0.338363\n",
      " 1839534/3750000: episode: 2551, duration: 9.115s, episode steps: 1221, steps per second: 134, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.015704, mae: 3.040013, mean_q: 3.658428, mean_eps: 0.337989\n",
      " 1840698/3750000: episode: 2552, duration: 8.671s, episode steps: 1164, steps per second: 134, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.016805, mae: 3.069323, mean_q: 3.690565, mean_eps: 0.337560\n",
      " 1841175/3750000: episode: 2553, duration: 3.562s, episode steps: 477, steps per second: 134, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.013911, mae: 3.029359, mean_q: 3.642623, mean_eps: 0.337265\n",
      " 1841981/3750000: episode: 2554, duration: 6.068s, episode steps: 806, steps per second: 133, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.020048, mae: 3.042487, mean_q: 3.661553, mean_eps: 0.337031\n",
      " 1842496/3750000: episode: 2555, duration: 3.859s, episode steps: 515, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.280 [0.000, 5.000],  loss: 0.012962, mae: 3.092295, mean_q: 3.724217, mean_eps: 0.336794\n",
      " 1843208/3750000: episode: 2556, duration: 5.353s, episode steps: 712, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.015754, mae: 3.066324, mean_q: 3.689740, mean_eps: 0.336574\n",
      " 1843953/3750000: episode: 2557, duration: 5.509s, episode steps: 745, steps per second: 135, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.012915, mae: 3.133418, mean_q: 3.773467, mean_eps: 0.336311\n",
      " 1844752/3750000: episode: 2558, duration: 6.012s, episode steps: 799, steps per second: 133, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.015468, mae: 3.085313, mean_q: 3.710103, mean_eps: 0.336034\n",
      " 1845565/3750000: episode: 2559, duration: 6.055s, episode steps: 813, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.015606, mae: 3.105203, mean_q: 3.735616, mean_eps: 0.335742\n",
      " 1846490/3750000: episode: 2560, duration: 6.896s, episode steps: 925, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.878 [0.000, 5.000],  loss: 0.014006, mae: 3.111874, mean_q: 3.744603, mean_eps: 0.335429\n",
      " 1847715/3750000: episode: 2561, duration: 9.155s, episode steps: 1225, steps per second: 134, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.973 [0.000, 5.000],  loss: 0.014874, mae: 3.100898, mean_q: 3.732638, mean_eps: 0.335044\n",
      " 1848410/3750000: episode: 2562, duration: 5.214s, episode steps: 695, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.373 [0.000, 5.000],  loss: 0.014985, mae: 3.108500, mean_q: 3.743564, mean_eps: 0.334698\n",
      " 1849070/3750000: episode: 2563, duration: 5.029s, episode steps: 660, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.774 [0.000, 5.000],  loss: 0.015628, mae: 3.091900, mean_q: 3.724753, mean_eps: 0.334454\n",
      " 1849713/3750000: episode: 2564, duration: 4.789s, episode steps: 643, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.012625, mae: 3.099545, mean_q: 3.733088, mean_eps: 0.334220\n",
      " 1850654/3750000: episode: 2565, duration: 7.017s, episode steps: 941, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.019958, mae: 3.135991, mean_q: 3.776825, mean_eps: 0.333935\n",
      " 1851500/3750000: episode: 2566, duration: 6.393s, episode steps: 846, steps per second: 132, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.015215, mae: 3.175110, mean_q: 3.819081, mean_eps: 0.333615\n",
      " 1852138/3750000: episode: 2567, duration: 4.739s, episode steps: 638, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.550 [0.000, 5.000],  loss: 0.011986, mae: 3.119898, mean_q: 3.760701, mean_eps: 0.333348\n",
      " 1852890/3750000: episode: 2568, duration: 5.730s, episode steps: 752, steps per second: 131, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.016328, mae: 3.137397, mean_q: 3.772640, mean_eps: 0.333096\n",
      " 1853444/3750000: episode: 2569, duration: 4.299s, episode steps: 554, steps per second: 129, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.015800, mae: 3.087933, mean_q: 3.720270, mean_eps: 0.332859\n",
      " 1854554/3750000: episode: 2570, duration: 8.281s, episode steps: 1110, steps per second: 134, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.018010, mae: 3.125258, mean_q: 3.763677, mean_eps: 0.332560\n",
      " 1855209/3750000: episode: 2571, duration: 5.048s, episode steps: 655, steps per second: 130, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.019495, mae: 3.087307, mean_q: 3.716636, mean_eps: 0.332243\n",
      " 1855889/3750000: episode: 2572, duration: 5.082s, episode steps: 680, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.014751, mae: 3.110604, mean_q: 3.745689, mean_eps: 0.332002\n",
      " 1856405/3750000: episode: 2573, duration: 3.858s, episode steps: 516, steps per second: 134, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.017327, mae: 3.059517, mean_q: 3.681801, mean_eps: 0.331786\n",
      " 1856831/3750000: episode: 2574, duration: 3.142s, episode steps: 426, steps per second: 136, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.013569, mae: 3.109554, mean_q: 3.740863, mean_eps: 0.331617\n",
      " 1857633/3750000: episode: 2575, duration: 6.024s, episode steps: 802, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.167 [0.000, 5.000],  loss: 0.011669, mae: 3.083166, mean_q: 3.710668, mean_eps: 0.331397\n",
      " 1858037/3750000: episode: 2576, duration: 3.001s, episode steps: 404, steps per second: 135, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.933 [0.000, 5.000],  loss: 0.014632, mae: 3.065240, mean_q: 3.700351, mean_eps: 0.331181\n",
      " 1858418/3750000: episode: 2577, duration: 2.832s, episode steps: 381, steps per second: 135, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.038006, mae: 3.111052, mean_q: 3.757190, mean_eps: 0.331041\n",
      " 1859536/3750000: episode: 2578, duration: 8.368s, episode steps: 1118, steps per second: 134, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.102 [0.000, 5.000],  loss: 0.016131, mae: 3.128357, mean_q: 3.769650, mean_eps: 0.330771\n",
      " 1860339/3750000: episode: 2579, duration: 6.031s, episode steps: 803, steps per second: 133, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.013321, mae: 3.155490, mean_q: 3.799676, mean_eps: 0.330425\n",
      " 1861055/3750000: episode: 2580, duration: 5.334s, episode steps: 716, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.012278, mae: 3.117461, mean_q: 3.755724, mean_eps: 0.330152\n",
      " 1861854/3750000: episode: 2581, duration: 6.016s, episode steps: 799, steps per second: 133, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.014081, mae: 3.134997, mean_q: 3.773535, mean_eps: 0.329878\n",
      " 1863085/3750000: episode: 2582, duration: 9.191s, episode steps: 1231, steps per second: 134, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.020982, mae: 3.168412, mean_q: 3.812027, mean_eps: 0.329511\n",
      " 1863890/3750000: episode: 2583, duration: 5.980s, episode steps: 805, steps per second: 135, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.935 [0.000, 5.000],  loss: 0.019283, mae: 3.171533, mean_q: 3.823097, mean_eps: 0.329144\n",
      " 1865075/3750000: episode: 2584, duration: 8.732s, episode steps: 1185, steps per second: 136, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.017737, mae: 3.150107, mean_q: 3.792692, mean_eps: 0.328787\n",
      " 1865831/3750000: episode: 2585, duration: 5.697s, episode steps: 756, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.018199, mae: 3.136577, mean_q: 3.789413, mean_eps: 0.328438\n",
      " 1866634/3750000: episode: 2586, duration: 5.927s, episode steps: 803, steps per second: 135, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.016172, mae: 3.168486, mean_q: 3.816215, mean_eps: 0.328157\n",
      " 1867304/3750000: episode: 2587, duration: 5.019s, episode steps: 670, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.016112, mae: 3.197359, mean_q: 3.850839, mean_eps: 0.327891\n",
      " 1868121/3750000: episode: 2588, duration: 6.143s, episode steps: 817, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.013416, mae: 3.202552, mean_q: 3.852713, mean_eps: 0.327621\n",
      " 1868729/3750000: episode: 2589, duration: 4.460s, episode steps: 608, steps per second: 136, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.015040, mae: 3.137525, mean_q: 3.772431, mean_eps: 0.327365\n",
      " 1869794/3750000: episode: 2590, duration: 7.891s, episode steps: 1065, steps per second: 135, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.010944, mae: 3.174676, mean_q: 3.818954, mean_eps: 0.327066\n",
      " 1870301/3750000: episode: 2591, duration: 3.941s, episode steps: 507, steps per second: 129, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.846 [0.000, 5.000],  loss: 0.014400, mae: 3.159717, mean_q: 3.800115, mean_eps: 0.326782\n",
      " 1870951/3750000: episode: 2592, duration: 4.804s, episode steps: 650, steps per second: 135, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.016158, mae: 3.161860, mean_q: 3.805837, mean_eps: 0.326573\n",
      " 1871445/3750000: episode: 2593, duration: 3.776s, episode steps: 494, steps per second: 131, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.015049, mae: 3.126434, mean_q: 3.762035, mean_eps: 0.326368\n",
      " 1872989/3750000: episode: 2594, duration: 11.568s, episode steps: 1544, steps per second: 133, episode reward: 33.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.887 [0.000, 5.000],  loss: 0.014739, mae: 3.134228, mean_q: 3.772359, mean_eps: 0.326001\n",
      " 1874128/3750000: episode: 2595, duration: 8.556s, episode steps: 1139, steps per second: 133, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.606 [0.000, 5.000],  loss: 0.016020, mae: 3.139495, mean_q: 3.780791, mean_eps: 0.325518\n",
      " 1874804/3750000: episode: 2596, duration: 5.123s, episode steps: 676, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.015816, mae: 3.110191, mean_q: 3.741315, mean_eps: 0.325191\n",
      " 1875437/3750000: episode: 2597, duration: 4.677s, episode steps: 633, steps per second: 135, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.016671, mae: 3.161472, mean_q: 3.805719, mean_eps: 0.324957\n",
      " 1876742/3750000: episode: 2598, duration: 9.832s, episode steps: 1305, steps per second: 133, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.897 [0.000, 5.000],  loss: 0.015767, mae: 3.140861, mean_q: 3.779713, mean_eps: 0.324608\n",
      " 1877499/3750000: episode: 2599, duration: 5.558s, episode steps: 757, steps per second: 136, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.020248, mae: 3.112813, mean_q: 3.756000, mean_eps: 0.324237\n",
      " 1878233/3750000: episode: 2600, duration: 5.516s, episode steps: 734, steps per second: 133, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.244 [0.000, 5.000],  loss: 0.014846, mae: 3.096176, mean_q: 3.728887, mean_eps: 0.323970\n",
      " 1878904/3750000: episode: 2601, duration: 5.106s, episode steps: 671, steps per second: 131, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.016356, mae: 3.113186, mean_q: 3.742716, mean_eps: 0.323715\n",
      " 1879595/3750000: episode: 2602, duration: 5.150s, episode steps: 691, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.016117, mae: 3.115505, mean_q: 3.748796, mean_eps: 0.323470\n",
      " 1880518/3750000: episode: 2603, duration: 6.828s, episode steps: 923, steps per second: 135, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.014874, mae: 3.145164, mean_q: 3.787108, mean_eps: 0.323182\n",
      " 1881280/3750000: episode: 2604, duration: 5.810s, episode steps: 762, steps per second: 131, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.014757, mae: 3.132836, mean_q: 3.771728, mean_eps: 0.322880\n",
      " 1882114/3750000: episode: 2605, duration: 6.223s, episode steps: 834, steps per second: 134, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.739 [0.000, 5.000],  loss: 0.011627, mae: 3.168442, mean_q: 3.814249, mean_eps: 0.322592\n",
      " 1883033/3750000: episode: 2606, duration: 6.942s, episode steps: 919, steps per second: 132, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.011109, mae: 3.147442, mean_q: 3.787666, mean_eps: 0.322275\n",
      " 1884053/3750000: episode: 2607, duration: 7.586s, episode steps: 1020, steps per second: 134, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.012573, mae: 3.162377, mean_q: 3.801377, mean_eps: 0.321926\n",
      " 1884723/3750000: episode: 2608, duration: 5.067s, episode steps: 670, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.025286, mae: 3.201098, mean_q: 3.855058, mean_eps: 0.321620\n",
      " 1885642/3750000: episode: 2609, duration: 6.877s, episode steps: 919, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.721 [0.000, 5.000],  loss: 0.013838, mae: 3.176437, mean_q: 3.825950, mean_eps: 0.321332\n",
      " 1886171/3750000: episode: 2610, duration: 3.895s, episode steps: 529, steps per second: 136, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.011846, mae: 3.218896, mean_q: 3.877010, mean_eps: 0.321072\n",
      " 1887034/3750000: episode: 2611, duration: 6.437s, episode steps: 863, steps per second: 134, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.017585, mae: 3.144004, mean_q: 3.784318, mean_eps: 0.320824\n",
      " 1887671/3750000: episode: 2612, duration: 4.858s, episode steps: 637, steps per second: 131, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.190 [0.000, 5.000],  loss: 0.017467, mae: 3.255702, mean_q: 3.917968, mean_eps: 0.320554\n",
      " 1888438/3750000: episode: 2613, duration: 5.759s, episode steps: 767, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.087 [0.000, 5.000],  loss: 0.018465, mae: 3.226671, mean_q: 3.881849, mean_eps: 0.320302\n",
      " 1889450/3750000: episode: 2614, duration: 7.606s, episode steps: 1012, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.017161, mae: 3.239303, mean_q: 3.897220, mean_eps: 0.319982\n",
      " 1890079/3750000: episode: 2615, duration: 4.660s, episode steps: 629, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.016192, mae: 3.213369, mean_q: 3.861575, mean_eps: 0.319686\n",
      " 1890827/3750000: episode: 2616, duration: 5.618s, episode steps: 748, steps per second: 133, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.016735, mae: 3.232459, mean_q: 3.890214, mean_eps: 0.319438\n",
      " 1891497/3750000: episode: 2617, duration: 5.009s, episode steps: 670, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.019583, mae: 3.262722, mean_q: 3.926595, mean_eps: 0.319182\n",
      " 1892213/3750000: episode: 2618, duration: 5.349s, episode steps: 716, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.014305, mae: 3.185005, mean_q: 3.839622, mean_eps: 0.318934\n",
      " 1893064/3750000: episode: 2619, duration: 6.376s, episode steps: 851, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.840 [0.000, 5.000],  loss: 0.015850, mae: 3.208680, mean_q: 3.863332, mean_eps: 0.318650\n",
      " 1893963/3750000: episode: 2620, duration: 6.738s, episode steps: 899, steps per second: 133, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.016707, mae: 3.162988, mean_q: 3.808948, mean_eps: 0.318333\n",
      " 1895122/3750000: episode: 2621, duration: 8.689s, episode steps: 1159, steps per second: 133, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.718 [0.000, 5.000],  loss: 0.015267, mae: 3.169684, mean_q: 3.813546, mean_eps: 0.317962\n",
      " 1895776/3750000: episode: 2622, duration: 4.849s, episode steps: 654, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.717 [0.000, 5.000],  loss: 0.020652, mae: 3.209555, mean_q: 3.859909, mean_eps: 0.317638\n",
      " 1896159/3750000: episode: 2623, duration: 2.845s, episode steps: 383, steps per second: 135, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.507 [0.000, 5.000],  loss: 0.013635, mae: 3.199288, mean_q: 3.855998, mean_eps: 0.317454\n",
      " 1896690/3750000: episode: 2624, duration: 3.986s, episode steps: 531, steps per second: 133, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.149 [0.000, 5.000],  loss: 0.021092, mae: 3.178793, mean_q: 3.832124, mean_eps: 0.317289\n",
      " 1897440/3750000: episode: 2625, duration: 5.673s, episode steps: 750, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.020139, mae: 3.201938, mean_q: 3.858203, mean_eps: 0.317058\n",
      " 1898023/3750000: episode: 2626, duration: 4.480s, episode steps: 583, steps per second: 130, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.016268, mae: 3.172820, mean_q: 3.827969, mean_eps: 0.316817\n",
      " 1898561/3750000: episode: 2627, duration: 4.011s, episode steps: 538, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.019494, mae: 3.196069, mean_q: 3.850274, mean_eps: 0.316612\n",
      " 1899389/3750000: episode: 2628, duration: 6.168s, episode steps: 828, steps per second: 134, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.023558, mae: 3.175409, mean_q: 3.821993, mean_eps: 0.316367\n",
      " 1900432/3750000: episode: 2629, duration: 7.797s, episode steps: 1043, steps per second: 134, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.018569, mae: 3.202703, mean_q: 3.861991, mean_eps: 0.316032\n",
      " 1901365/3750000: episode: 2630, duration: 6.945s, episode steps: 933, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.017392, mae: 3.219710, mean_q: 3.870724, mean_eps: 0.315676\n",
      " 1901962/3750000: episode: 2631, duration: 4.461s, episode steps: 597, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.018764, mae: 3.250744, mean_q: 3.908221, mean_eps: 0.315399\n",
      " 1902627/3750000: episode: 2632, duration: 4.968s, episode steps: 665, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.016858, mae: 3.211976, mean_q: 3.862495, mean_eps: 0.315172\n",
      " 1903553/3750000: episode: 2633, duration: 6.874s, episode steps: 926, steps per second: 135, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.018078, mae: 3.256006, mean_q: 3.917423, mean_eps: 0.314888\n",
      " 1904153/3750000: episode: 2634, duration: 4.528s, episode steps: 600, steps per second: 133, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.022404, mae: 3.209552, mean_q: 3.859436, mean_eps: 0.314614\n",
      " 1905001/3750000: episode: 2635, duration: 6.409s, episode steps: 848, steps per second: 132, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.920 [0.000, 5.000],  loss: 0.015118, mae: 3.242401, mean_q: 3.898314, mean_eps: 0.314351\n",
      " 1905816/3750000: episode: 2636, duration: 5.979s, episode steps: 815, steps per second: 136, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.033 [0.000, 5.000],  loss: 0.017457, mae: 3.238592, mean_q: 3.895753, mean_eps: 0.314052\n",
      " 1906783/3750000: episode: 2637, duration: 7.293s, episode steps: 967, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.015005, mae: 3.219183, mean_q: 3.875061, mean_eps: 0.313732\n",
      " 1907543/3750000: episode: 2638, duration: 5.635s, episode steps: 760, steps per second: 135, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.013840, mae: 3.205317, mean_q: 3.856025, mean_eps: 0.313419\n",
      " 1908075/3750000: episode: 2639, duration: 3.901s, episode steps: 532, steps per second: 136, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.013351, mae: 3.271450, mean_q: 3.945047, mean_eps: 0.313188\n",
      " 1908991/3750000: episode: 2640, duration: 6.905s, episode steps: 916, steps per second: 133, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.016733, mae: 3.275496, mean_q: 3.938135, mean_eps: 0.312929\n",
      " 1909640/3750000: episode: 2641, duration: 4.859s, episode steps: 649, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.014175, mae: 3.245776, mean_q: 3.902491, mean_eps: 0.312648\n",
      " 1910407/3750000: episode: 2642, duration: 5.740s, episode steps: 767, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.010846, mae: 3.221574, mean_q: 3.871223, mean_eps: 0.312393\n",
      " 1910938/3750000: episode: 2643, duration: 3.975s, episode steps: 531, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.011206, mae: 3.203710, mean_q: 3.851332, mean_eps: 0.312159\n",
      " 1912361/3750000: episode: 2644, duration: 10.606s, episode steps: 1423, steps per second: 134, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.013150, mae: 3.250751, mean_q: 3.912292, mean_eps: 0.311806\n",
      " 1913618/3750000: episode: 2645, duration: 9.284s, episode steps: 1257, steps per second: 135, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.014978, mae: 3.255923, mean_q: 3.917451, mean_eps: 0.311324\n",
      " 1914448/3750000: episode: 2646, duration: 6.157s, episode steps: 830, steps per second: 135, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.017933, mae: 3.264729, mean_q: 3.935860, mean_eps: 0.310949\n",
      " 1915500/3750000: episode: 2647, duration: 7.935s, episode steps: 1052, steps per second: 133, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.012882, mae: 3.305462, mean_q: 3.979368, mean_eps: 0.310611\n",
      " 1916674/3750000: episode: 2648, duration: 8.747s, episode steps: 1174, steps per second: 134, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.016778, mae: 3.314448, mean_q: 3.988004, mean_eps: 0.310211\n",
      " 1917785/3750000: episode: 2649, duration: 8.357s, episode steps: 1111, steps per second: 133, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.021397, mae: 3.263015, mean_q: 3.924217, mean_eps: 0.309797\n",
      " 1918649/3750000: episode: 2650, duration: 6.386s, episode steps: 864, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.014919, mae: 3.301064, mean_q: 3.975318, mean_eps: 0.309441\n",
      " 1919287/3750000: episode: 2651, duration: 4.807s, episode steps: 638, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.016380, mae: 3.276097, mean_q: 3.942251, mean_eps: 0.309171\n",
      " 1920066/3750000: episode: 2652, duration: 5.837s, episode steps: 779, steps per second: 133, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.584 [0.000, 5.000],  loss: 0.015018, mae: 3.299255, mean_q: 3.971947, mean_eps: 0.308915\n",
      " 1920865/3750000: episode: 2653, duration: 5.956s, episode steps: 799, steps per second: 134, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.875 [0.000, 5.000],  loss: 0.011983, mae: 3.241789, mean_q: 3.898866, mean_eps: 0.308631\n",
      " 1921564/3750000: episode: 2654, duration: 5.296s, episode steps: 699, steps per second: 132, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.017622, mae: 3.275732, mean_q: 3.942908, mean_eps: 0.308361\n",
      " 1922643/3750000: episode: 2655, duration: 7.994s, episode steps: 1079, steps per second: 135, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.017884, mae: 3.249227, mean_q: 3.911191, mean_eps: 0.308040\n",
      " 1923706/3750000: episode: 2656, duration: 7.945s, episode steps: 1063, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.016649, mae: 3.278534, mean_q: 3.947480, mean_eps: 0.307655\n",
      " 1925044/3750000: episode: 2657, duration: 9.942s, episode steps: 1338, steps per second: 135, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.906 [0.000, 5.000],  loss: 0.014396, mae: 3.232474, mean_q: 3.890934, mean_eps: 0.307223\n",
      " 1925388/3750000: episode: 2658, duration: 2.543s, episode steps: 344, steps per second: 135, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.014379, mae: 3.180966, mean_q: 3.828089, mean_eps: 0.306921\n",
      " 1926229/3750000: episode: 2659, duration: 6.319s, episode steps: 841, steps per second: 133, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.017501, mae: 3.209826, mean_q: 3.864503, mean_eps: 0.306708\n",
      " 1927148/3750000: episode: 2660, duration: 6.838s, episode steps: 919, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.020472, mae: 3.179977, mean_q: 3.825829, mean_eps: 0.306392\n",
      " 1928259/3750000: episode: 2661, duration: 8.317s, episode steps: 1111, steps per second: 134, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.706 [0.000, 5.000],  loss: 0.016421, mae: 3.178829, mean_q: 3.825829, mean_eps: 0.306028\n",
      " 1928975/3750000: episode: 2662, duration: 5.366s, episode steps: 716, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.017500, mae: 3.146286, mean_q: 3.784064, mean_eps: 0.305700\n",
      " 1929617/3750000: episode: 2663, duration: 4.774s, episode steps: 642, steps per second: 134, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 0.022074, mae: 3.154965, mean_q: 3.793971, mean_eps: 0.305456\n",
      " 1930061/3750000: episode: 2664, duration: 3.431s, episode steps: 444, steps per second: 129, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.189 [0.000, 5.000],  loss: 0.017008, mae: 3.188502, mean_q: 3.841661, mean_eps: 0.305258\n",
      " 1930719/3750000: episode: 2665, duration: 4.833s, episode steps: 658, steps per second: 136, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.019380, mae: 3.189484, mean_q: 3.835414, mean_eps: 0.305060\n",
      " 1931117/3750000: episode: 2666, duration: 2.981s, episode steps: 398, steps per second: 134, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.016114, mae: 3.184140, mean_q: 3.834764, mean_eps: 0.304872\n",
      " 1931954/3750000: episode: 2667, duration: 6.233s, episode steps: 837, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.210 [0.000, 5.000],  loss: 0.017002, mae: 3.215173, mean_q: 3.870664, mean_eps: 0.304649\n",
      " 1933222/3750000: episode: 2668, duration: 9.488s, episode steps: 1268, steps per second: 134, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.021967, mae: 3.232696, mean_q: 3.885424, mean_eps: 0.304268\n",
      " 1934113/3750000: episode: 2669, duration: 6.611s, episode steps: 891, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.073 [0.000, 5.000],  loss: 0.012996, mae: 3.257396, mean_q: 3.916983, mean_eps: 0.303879\n",
      " 1934976/3750000: episode: 2670, duration: 6.435s, episode steps: 863, steps per second: 134, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.017179, mae: 3.267488, mean_q: 3.935464, mean_eps: 0.303566\n",
      " 1935594/3750000: episode: 2671, duration: 4.664s, episode steps: 618, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.015463, mae: 3.228688, mean_q: 3.884664, mean_eps: 0.303299\n",
      " 1936215/3750000: episode: 2672, duration: 4.660s, episode steps: 621, steps per second: 133, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.847 [0.000, 5.000],  loss: 0.014726, mae: 3.214352, mean_q: 3.871786, mean_eps: 0.303076\n",
      " 1937173/3750000: episode: 2673, duration: 7.245s, episode steps: 958, steps per second: 132, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.021508, mae: 3.223549, mean_q: 3.880080, mean_eps: 0.302792\n",
      " 1937899/3750000: episode: 2674, duration: 5.360s, episode steps: 726, steps per second: 135, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.012420, mae: 3.229234, mean_q: 3.885650, mean_eps: 0.302489\n",
      " 1938989/3750000: episode: 2675, duration: 8.209s, episode steps: 1090, steps per second: 133, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.013575, mae: 3.226120, mean_q: 3.884494, mean_eps: 0.302162\n",
      " 1939376/3750000: episode: 2676, duration: 2.860s, episode steps: 387, steps per second: 135, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.016714, mae: 3.211424, mean_q: 3.871533, mean_eps: 0.301895\n",
      " 1940964/3750000: episode: 2677, duration: 11.891s, episode steps: 1588, steps per second: 134, episode reward: 26.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.014986, mae: 3.240201, mean_q: 3.899816, mean_eps: 0.301539\n",
      " 1941631/3750000: episode: 2678, duration: 4.913s, episode steps: 667, steps per second: 136, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.022629, mae: 3.256722, mean_q: 3.920978, mean_eps: 0.301132\n",
      " 1942312/3750000: episode: 2679, duration: 5.038s, episode steps: 681, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.242 [0.000, 5.000],  loss: 0.015918, mae: 3.207616, mean_q: 3.868699, mean_eps: 0.300891\n",
      " 1943020/3750000: episode: 2680, duration: 5.375s, episode steps: 708, steps per second: 132, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.019632, mae: 3.273086, mean_q: 3.942801, mean_eps: 0.300642\n",
      " 1943660/3750000: episode: 2681, duration: 5.148s, episode steps: 640, steps per second: 124, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.018676, mae: 3.266309, mean_q: 3.928623, mean_eps: 0.300401\n",
      " 1944314/3750000: episode: 2682, duration: 4.951s, episode steps: 654, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.011497, mae: 3.190375, mean_q: 3.839590, mean_eps: 0.300167\n",
      " 1944789/3750000: episode: 2683, duration: 3.602s, episode steps: 475, steps per second: 132, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.326 [0.000, 5.000],  loss: 0.017123, mae: 3.171217, mean_q: 3.817361, mean_eps: 0.299962\n",
      " 1945179/3750000: episode: 2684, duration: 2.936s, episode steps: 390, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 0.772 [0.000, 5.000],  loss: 0.017860, mae: 3.229091, mean_q: 3.890535, mean_eps: 0.299807\n",
      " 1946189/3750000: episode: 2685, duration: 7.542s, episode steps: 1010, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.015893, mae: 3.243355, mean_q: 3.898914, mean_eps: 0.299555\n",
      " 1946576/3750000: episode: 2686, duration: 2.838s, episode steps: 387, steps per second: 136, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.563 [0.000, 5.000],  loss: 0.013527, mae: 3.192696, mean_q: 3.851908, mean_eps: 0.299303\n",
      " 1947438/3750000: episode: 2687, duration: 6.439s, episode steps: 862, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: 0.018703, mae: 3.211642, mean_q: 3.863080, mean_eps: 0.299080\n",
      " 1948859/3750000: episode: 2688, duration: 10.500s, episode steps: 1421, steps per second: 135, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.014929, mae: 3.218589, mean_q: 3.875521, mean_eps: 0.298670\n",
      " 1949668/3750000: episode: 2689, duration: 6.139s, episode steps: 809, steps per second: 132, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.884 [0.000, 5.000],  loss: 0.015252, mae: 3.231332, mean_q: 3.888367, mean_eps: 0.298266\n",
      " 1950485/3750000: episode: 2690, duration: 6.044s, episode steps: 817, steps per second: 135, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.013317, mae: 3.229896, mean_q: 3.890323, mean_eps: 0.297971\n",
      " 1951347/3750000: episode: 2691, duration: 6.489s, episode steps: 862, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.611 [0.000, 5.000],  loss: 0.017578, mae: 3.249845, mean_q: 3.914861, mean_eps: 0.297669\n",
      " 1951997/3750000: episode: 2692, duration: 4.811s, episode steps: 650, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.274 [0.000, 5.000],  loss: 0.014229, mae: 3.266475, mean_q: 3.933586, mean_eps: 0.297399\n",
      " 1953714/3750000: episode: 2693, duration: 12.797s, episode steps: 1717, steps per second: 134, episode reward: 35.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.016234, mae: 3.279289, mean_q: 3.944785, mean_eps: 0.296974\n",
      " 1954218/3750000: episode: 2694, duration: 3.737s, episode steps: 504, steps per second: 135, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.016999, mae: 3.257582, mean_q: 3.924195, mean_eps: 0.296574\n",
      " 1955294/3750000: episode: 2695, duration: 8.017s, episode steps: 1076, steps per second: 134, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.012491, mae: 3.271294, mean_q: 3.934571, mean_eps: 0.296290\n",
      " 1955954/3750000: episode: 2696, duration: 4.964s, episode steps: 660, steps per second: 133, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.019205, mae: 3.298967, mean_q: 3.972158, mean_eps: 0.295977\n",
      " 1956870/3750000: episode: 2697, duration: 6.836s, episode steps: 916, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.014891, mae: 3.274977, mean_q: 3.946043, mean_eps: 0.295692\n",
      " 1957473/3750000: episode: 2698, duration: 4.458s, episode steps: 603, steps per second: 135, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.209 [0.000, 5.000],  loss: 0.015463, mae: 3.268353, mean_q: 3.938346, mean_eps: 0.295419\n",
      " 1958479/3750000: episode: 2699, duration: 7.506s, episode steps: 1006, steps per second: 134, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.016738, mae: 3.265820, mean_q: 3.937444, mean_eps: 0.295131\n",
      " 1959429/3750000: episode: 2700, duration: 7.065s, episode steps: 950, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.014990, mae: 3.242763, mean_q: 3.901831, mean_eps: 0.294778\n",
      " 1959829/3750000: episode: 2701, duration: 3.062s, episode steps: 400, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.265 [0.000, 5.000],  loss: 0.013803, mae: 3.284877, mean_q: 3.958303, mean_eps: 0.294533\n",
      " 1960611/3750000: episode: 2702, duration: 5.874s, episode steps: 782, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.014549, mae: 3.247233, mean_q: 3.908246, mean_eps: 0.294321\n",
      " 1961407/3750000: episode: 2703, duration: 5.924s, episode steps: 796, steps per second: 134, episode reward: 27.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.014150, mae: 3.253544, mean_q: 3.913382, mean_eps: 0.294036\n",
      " 1962165/3750000: episode: 2704, duration: 5.684s, episode steps: 758, steps per second: 133, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.023638, mae: 3.270707, mean_q: 3.934766, mean_eps: 0.293756\n",
      " 1962643/3750000: episode: 2705, duration: 3.569s, episode steps: 478, steps per second: 134, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.015926, mae: 3.236385, mean_q: 3.901412, mean_eps: 0.293532\n",
      " 1963369/3750000: episode: 2706, duration: 5.367s, episode steps: 726, steps per second: 135, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.017109, mae: 3.252135, mean_q: 3.912500, mean_eps: 0.293316\n",
      " 1964326/3750000: episode: 2707, duration: 7.155s, episode steps: 957, steps per second: 134, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.017577, mae: 3.296296, mean_q: 3.969998, mean_eps: 0.293014\n",
      " 1964816/3750000: episode: 2708, duration: 3.646s, episode steps: 490, steps per second: 134, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.582 [0.000, 5.000],  loss: 0.017530, mae: 3.262922, mean_q: 3.924562, mean_eps: 0.292755\n",
      " 1965312/3750000: episode: 2709, duration: 3.705s, episode steps: 496, steps per second: 134, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.022004, mae: 3.245918, mean_q: 3.911102, mean_eps: 0.292578\n",
      " 1966047/3750000: episode: 2710, duration: 5.481s, episode steps: 735, steps per second: 134, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.038 [0.000, 5.000],  loss: 0.013912, mae: 3.245751, mean_q: 3.913798, mean_eps: 0.292355\n",
      " 1967116/3750000: episode: 2711, duration: 7.981s, episode steps: 1069, steps per second: 134, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.016487, mae: 3.214918, mean_q: 3.867087, mean_eps: 0.292031\n",
      " 1967771/3750000: episode: 2712, duration: 4.958s, episode steps: 655, steps per second: 132, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.012087, mae: 3.190172, mean_q: 3.836053, mean_eps: 0.291722\n",
      " 1968482/3750000: episode: 2713, duration: 5.483s, episode steps: 711, steps per second: 130, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.019191, mae: 3.210703, mean_q: 3.871337, mean_eps: 0.291473\n",
      " 1969304/3750000: episode: 2714, duration: 6.145s, episode steps: 822, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.017676, mae: 3.193658, mean_q: 3.847495, mean_eps: 0.291196\n",
      " 1969683/3750000: episode: 2715, duration: 2.841s, episode steps: 379, steps per second: 133, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014142, mae: 3.255529, mean_q: 3.914654, mean_eps: 0.290980\n",
      " 1970569/3750000: episode: 2716, duration: 6.631s, episode steps: 886, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.015264, mae: 3.228933, mean_q: 3.883626, mean_eps: 0.290753\n",
      " 1971531/3750000: episode: 2717, duration: 7.145s, episode steps: 962, steps per second: 135, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.756 [0.000, 5.000],  loss: 0.013818, mae: 3.211726, mean_q: 3.864983, mean_eps: 0.290422\n",
      " 1972660/3750000: episode: 2718, duration: 8.451s, episode steps: 1129, steps per second: 134, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.017783, mae: 3.197411, mean_q: 3.844564, mean_eps: 0.290048\n",
      " 1973465/3750000: episode: 2719, duration: 6.059s, episode steps: 805, steps per second: 133, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.078 [0.000, 5.000],  loss: 0.014141, mae: 3.193651, mean_q: 3.848405, mean_eps: 0.289698\n",
      " 1974320/3750000: episode: 2720, duration: 6.363s, episode steps: 855, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.017420, mae: 3.206812, mean_q: 3.854409, mean_eps: 0.289400\n",
      " 1975126/3750000: episode: 2721, duration: 6.132s, episode steps: 806, steps per second: 131, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.018811, mae: 3.202984, mean_q: 3.850547, mean_eps: 0.289101\n",
      " 1975781/3750000: episode: 2722, duration: 4.922s, episode steps: 655, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.015487, mae: 3.237711, mean_q: 3.892258, mean_eps: 0.288834\n",
      " 1976314/3750000: episode: 2723, duration: 3.908s, episode steps: 533, steps per second: 136, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.992 [0.000, 5.000],  loss: 0.017176, mae: 3.173974, mean_q: 3.815935, mean_eps: 0.288622\n",
      " 1977749/3750000: episode: 2724, duration: 10.700s, episode steps: 1435, steps per second: 134, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.015325, mae: 3.206382, mean_q: 3.856787, mean_eps: 0.288269\n",
      " 1978759/3750000: episode: 2725, duration: 7.446s, episode steps: 1010, steps per second: 136, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.014439, mae: 3.146231, mean_q: 3.786806, mean_eps: 0.287830\n",
      " 1979728/3750000: episode: 2726, duration: 7.305s, episode steps: 969, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.020327, mae: 3.136290, mean_q: 3.776701, mean_eps: 0.287474\n",
      " 1980422/3750000: episode: 2727, duration: 5.212s, episode steps: 694, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 4.079 [0.000, 5.000],  loss: 0.014610, mae: 3.151272, mean_q: 3.794442, mean_eps: 0.287171\n",
      " 1980925/3750000: episode: 2728, duration: 3.767s, episode steps: 503, steps per second: 134, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.386 [0.000, 5.000],  loss: 0.012696, mae: 3.115846, mean_q: 3.749593, mean_eps: 0.286955\n",
      " 1981737/3750000: episode: 2729, duration: 6.029s, episode steps: 812, steps per second: 135, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.012772, mae: 3.138380, mean_q: 3.776338, mean_eps: 0.286721\n",
      " 1982628/3750000: episode: 2730, duration: 6.745s, episode steps: 891, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.017069, mae: 3.142805, mean_q: 3.785675, mean_eps: 0.286415\n",
      " 1983671/3750000: episode: 2731, duration: 7.825s, episode steps: 1043, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.018700, mae: 3.162738, mean_q: 3.807429, mean_eps: 0.286066\n",
      " 1984403/3750000: episode: 2732, duration: 5.451s, episode steps: 732, steps per second: 134, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.230 [0.000, 5.000],  loss: 0.012579, mae: 3.106459, mean_q: 3.742220, mean_eps: 0.285746\n",
      " 1985074/3750000: episode: 2733, duration: 4.919s, episode steps: 671, steps per second: 136, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.009284, mae: 3.129456, mean_q: 3.764254, mean_eps: 0.285494\n",
      " 1985700/3750000: episode: 2734, duration: 4.789s, episode steps: 626, steps per second: 131, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.013499, mae: 3.108520, mean_q: 3.737986, mean_eps: 0.285263\n",
      " 1986727/3750000: episode: 2735, duration: 7.673s, episode steps: 1027, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.014689, mae: 3.089286, mean_q: 3.717318, mean_eps: 0.284964\n",
      " 1987403/3750000: episode: 2736, duration: 5.035s, episode steps: 676, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.012150, mae: 3.131384, mean_q: 3.772071, mean_eps: 0.284655\n",
      " 1988136/3750000: episode: 2737, duration: 5.480s, episode steps: 733, steps per second: 134, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.021115, mae: 3.111281, mean_q: 3.743336, mean_eps: 0.284403\n",
      " 1989097/3750000: episode: 2738, duration: 7.131s, episode steps: 961, steps per second: 135, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.014613, mae: 3.126938, mean_q: 3.763334, mean_eps: 0.284100\n",
      " 1989786/3750000: episode: 2739, duration: 5.200s, episode steps: 689, steps per second: 133, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.012784, mae: 3.061218, mean_q: 3.684134, mean_eps: 0.283802\n",
      " 1990299/3750000: episode: 2740, duration: 3.792s, episode steps: 513, steps per second: 135, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.025992, mae: 3.048854, mean_q: 3.666396, mean_eps: 0.283586\n",
      " 1991384/3750000: episode: 2741, duration: 8.242s, episode steps: 1085, steps per second: 132, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.492 [0.000, 5.000],  loss: 0.013846, mae: 3.089054, mean_q: 3.720194, mean_eps: 0.283298\n",
      " 1991872/3750000: episode: 2742, duration: 3.654s, episode steps: 488, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.013503, mae: 3.039422, mean_q: 3.655928, mean_eps: 0.283013\n",
      " 1992545/3750000: episode: 2743, duration: 5.111s, episode steps: 673, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.019135, mae: 3.050159, mean_q: 3.667566, mean_eps: 0.282804\n",
      " 1993268/3750000: episode: 2744, duration: 5.374s, episode steps: 723, steps per second: 135, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.023803, mae: 3.112892, mean_q: 3.750271, mean_eps: 0.282552\n",
      " 1993670/3750000: episode: 2745, duration: 2.992s, episode steps: 402, steps per second: 134, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.015922, mae: 3.064149, mean_q: 3.688836, mean_eps: 0.282351\n",
      " 1994449/3750000: episode: 2746, duration: 5.856s, episode steps: 779, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.164 [0.000, 5.000],  loss: 0.016867, mae: 3.098390, mean_q: 3.731997, mean_eps: 0.282138\n",
      " 1995257/3750000: episode: 2747, duration: 6.001s, episode steps: 808, steps per second: 135, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.020350, mae: 3.123370, mean_q: 3.761239, mean_eps: 0.281854\n",
      " 1996439/3750000: episode: 2748, duration: 8.844s, episode steps: 1182, steps per second: 134, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.138 [0.000, 5.000],  loss: 0.014398, mae: 3.114805, mean_q: 3.755952, mean_eps: 0.281498\n",
      " 1996937/3750000: episode: 2749, duration: 3.707s, episode steps: 498, steps per second: 134, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.020081, mae: 3.082902, mean_q: 3.706987, mean_eps: 0.281195\n",
      " 1997471/3750000: episode: 2750, duration: 4.028s, episode steps: 534, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.015087, mae: 3.095132, mean_q: 3.720657, mean_eps: 0.281008\n",
      " 1998904/3750000: episode: 2751, duration: 10.686s, episode steps: 1433, steps per second: 134, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.014781, mae: 3.098243, mean_q: 3.727434, mean_eps: 0.280652\n",
      " 1999959/3750000: episode: 2752, duration: 7.845s, episode steps: 1055, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.159 [0.000, 5.000],  loss: 0.016088, mae: 3.120971, mean_q: 3.755502, mean_eps: 0.280205\n",
      " 2001147/3750000: episode: 2753, duration: 9.107s, episode steps: 1188, steps per second: 130, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.016722, mae: 3.049761, mean_q: 3.671085, mean_eps: 0.279802\n",
      " 2001790/3750000: episode: 2754, duration: 4.764s, episode steps: 643, steps per second: 135, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.015135, mae: 3.057045, mean_q: 3.676574, mean_eps: 0.279471\n",
      " 2002555/3750000: episode: 2755, duration: 5.698s, episode steps: 765, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.013347, mae: 2.984462, mean_q: 3.589428, mean_eps: 0.279219\n",
      " 2003243/3750000: episode: 2756, duration: 5.219s, episode steps: 688, steps per second: 132, episode reward: 22.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.010929, mae: 3.006547, mean_q: 3.616195, mean_eps: 0.278956\n",
      " 2003652/3750000: episode: 2757, duration: 2.987s, episode steps: 409, steps per second: 137, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.853 [0.000, 5.000],  loss: 0.014664, mae: 3.064834, mean_q: 3.684757, mean_eps: 0.278758\n",
      " 2004038/3750000: episode: 2758, duration: 2.851s, episode steps: 386, steps per second: 135, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.009642, mae: 2.990280, mean_q: 3.596288, mean_eps: 0.278618\n",
      " 2004841/3750000: episode: 2759, duration: 6.110s, episode steps: 803, steps per second: 131, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.907 [0.000, 5.000],  loss: 0.011338, mae: 3.013142, mean_q: 3.634627, mean_eps: 0.278402\n",
      " 2005520/3750000: episode: 2760, duration: 5.056s, episode steps: 679, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.011560, mae: 3.046802, mean_q: 3.661655, mean_eps: 0.278135\n",
      " 2006457/3750000: episode: 2761, duration: 6.969s, episode steps: 937, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.015716, mae: 3.046307, mean_q: 3.667677, mean_eps: 0.277847\n",
      " 2006885/3750000: episode: 2762, duration: 3.324s, episode steps: 428, steps per second: 129, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.035 [0.000, 5.000],  loss: 0.010881, mae: 3.037953, mean_q: 3.657886, mean_eps: 0.277599\n",
      " 2007698/3750000: episode: 2763, duration: 5.993s, episode steps: 813, steps per second: 136, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.016684, mae: 3.062337, mean_q: 3.688683, mean_eps: 0.277376\n",
      " 2008873/3750000: episode: 2764, duration: 8.746s, episode steps: 1175, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.746 [0.000, 5.000],  loss: 0.014840, mae: 3.043237, mean_q: 3.664077, mean_eps: 0.277019\n",
      " 2010050/3750000: episode: 2765, duration: 8.815s, episode steps: 1177, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.277 [0.000, 5.000],  loss: 0.013763, mae: 3.092733, mean_q: 3.720775, mean_eps: 0.276594\n",
      " 2010706/3750000: episode: 2766, duration: 4.894s, episode steps: 656, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.877 [0.000, 5.000],  loss: 0.012005, mae: 3.105406, mean_q: 3.744114, mean_eps: 0.276263\n",
      " 2011192/3750000: episode: 2767, duration: 3.649s, episode steps: 486, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.714 [0.000, 5.000],  loss: 0.017924, mae: 3.111094, mean_q: 3.747050, mean_eps: 0.276058\n",
      " 2012109/3750000: episode: 2768, duration: 6.839s, episode steps: 917, steps per second: 134, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.638 [0.000, 5.000],  loss: 0.017779, mae: 3.063772, mean_q: 3.685051, mean_eps: 0.275806\n",
      " 2012996/3750000: episode: 2769, duration: 6.564s, episode steps: 887, steps per second: 135, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.015061, mae: 3.158412, mean_q: 3.801548, mean_eps: 0.275482\n",
      " 2014782/3750000: episode: 2770, duration: 13.516s, episode steps: 1786, steps per second: 132, episode reward: 38.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.016519, mae: 3.117855, mean_q: 3.751528, mean_eps: 0.275000\n",
      " 2015139/3750000: episode: 2771, duration: 2.599s, episode steps: 357, steps per second: 137, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.012434, mae: 3.044198, mean_q: 3.664199, mean_eps: 0.274614\n",
      " 2015807/3750000: episode: 2772, duration: 5.112s, episode steps: 668, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.015897, mae: 3.113094, mean_q: 3.746348, mean_eps: 0.274431\n",
      " 2016618/3750000: episode: 2773, duration: 6.053s, episode steps: 811, steps per second: 134, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.013558, mae: 3.068595, mean_q: 3.694045, mean_eps: 0.274164\n",
      " 2017506/3750000: episode: 2774, duration: 6.677s, episode steps: 888, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.012405, mae: 3.102583, mean_q: 3.730962, mean_eps: 0.273858\n",
      " 2018278/3750000: episode: 2775, duration: 5.719s, episode steps: 772, steps per second: 135, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.011462, mae: 3.064014, mean_q: 3.685866, mean_eps: 0.273560\n",
      " 2018910/3750000: episode: 2776, duration: 4.695s, episode steps: 632, steps per second: 135, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.018754, mae: 3.076945, mean_q: 3.697575, mean_eps: 0.273308\n",
      " 2019736/3750000: episode: 2777, duration: 6.166s, episode steps: 826, steps per second: 134, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.015086, mae: 3.075897, mean_q: 3.698766, mean_eps: 0.273045\n",
      " 2020191/3750000: episode: 2778, duration: 3.414s, episode steps: 455, steps per second: 133, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.868 [0.000, 5.000],  loss: 0.020353, mae: 3.067028, mean_q: 3.683699, mean_eps: 0.272814\n",
      " 2021291/3750000: episode: 2779, duration: 8.145s, episode steps: 1100, steps per second: 135, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.016981, mae: 3.012809, mean_q: 3.620672, mean_eps: 0.272534\n",
      " 2021923/3750000: episode: 2780, duration: 4.806s, episode steps: 632, steps per second: 131, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.010008, mae: 3.051416, mean_q: 3.669639, mean_eps: 0.272220\n",
      " 2022500/3750000: episode: 2781, duration: 4.289s, episode steps: 577, steps per second: 135, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.019609, mae: 3.044144, mean_q: 3.662694, mean_eps: 0.272004\n",
      " 2023211/3750000: episode: 2782, duration: 5.388s, episode steps: 711, steps per second: 132, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.527 [0.000, 5.000],  loss: 0.009998, mae: 3.104755, mean_q: 3.741244, mean_eps: 0.271774\n",
      " 2023718/3750000: episode: 2783, duration: 3.736s, episode steps: 507, steps per second: 136, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.011616, mae: 3.133783, mean_q: 3.776900, mean_eps: 0.271554\n",
      " 2024258/3750000: episode: 2784, duration: 4.039s, episode steps: 540, steps per second: 134, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.021007, mae: 3.088633, mean_q: 3.715715, mean_eps: 0.271367\n",
      " 2024799/3750000: episode: 2785, duration: 4.040s, episode steps: 541, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.021294, mae: 3.120841, mean_q: 3.762981, mean_eps: 0.271173\n",
      " 2025736/3750000: episode: 2786, duration: 6.958s, episode steps: 937, steps per second: 135, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.098 [0.000, 5.000],  loss: 0.016692, mae: 3.060344, mean_q: 3.686426, mean_eps: 0.270906\n",
      " 2026115/3750000: episode: 2787, duration: 2.885s, episode steps: 379, steps per second: 131, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.010283, mae: 3.090431, mean_q: 3.717297, mean_eps: 0.270669\n",
      " 2027055/3750000: episode: 2788, duration: 6.994s, episode steps: 940, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.015071, mae: 3.041026, mean_q: 3.662535, mean_eps: 0.270431\n",
      " 2027744/3750000: episode: 2789, duration: 5.279s, episode steps: 689, steps per second: 131, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.001 [0.000, 5.000],  loss: 0.016388, mae: 3.067248, mean_q: 3.691278, mean_eps: 0.270136\n",
      " 2028524/3750000: episode: 2790, duration: 5.833s, episode steps: 780, steps per second: 134, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014225, mae: 3.084270, mean_q: 3.715639, mean_eps: 0.269870\n",
      " 2029372/3750000: episode: 2791, duration: 6.252s, episode steps: 848, steps per second: 136, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.016267, mae: 3.055350, mean_q: 3.674935, mean_eps: 0.269578\n",
      " 2030165/3750000: episode: 2792, duration: 5.920s, episode steps: 793, steps per second: 134, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.016887, mae: 3.108954, mean_q: 3.740303, mean_eps: 0.269283\n",
      " 2031484/3750000: episode: 2793, duration: 9.893s, episode steps: 1319, steps per second: 133, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015492, mae: 3.047825, mean_q: 3.667144, mean_eps: 0.268901\n",
      " 2032202/3750000: episode: 2794, duration: 5.450s, episode steps: 718, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.015100, mae: 3.063634, mean_q: 3.682082, mean_eps: 0.268534\n",
      " 2032655/3750000: episode: 2795, duration: 3.377s, episode steps: 453, steps per second: 134, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 3.389 [0.000, 5.000],  loss: 0.013278, mae: 3.055025, mean_q: 3.683027, mean_eps: 0.268325\n",
      " 2033642/3750000: episode: 2796, duration: 7.352s, episode steps: 987, steps per second: 134, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.018297, mae: 3.005608, mean_q: 3.616937, mean_eps: 0.268066\n",
      " 2034379/3750000: episode: 2797, duration: 5.434s, episode steps: 737, steps per second: 136, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.017607, mae: 3.035118, mean_q: 3.649539, mean_eps: 0.267756\n",
      " 2035042/3750000: episode: 2798, duration: 5.128s, episode steps: 663, steps per second: 129, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.013465, mae: 3.029469, mean_q: 3.641616, mean_eps: 0.267504\n",
      " 2036121/3750000: episode: 2799, duration: 8.026s, episode steps: 1079, steps per second: 134, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.656 [0.000, 5.000],  loss: 0.018702, mae: 3.050395, mean_q: 3.667713, mean_eps: 0.267188\n",
      " 2037303/3750000: episode: 2800, duration: 8.820s, episode steps: 1182, steps per second: 134, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.013143, mae: 2.991240, mean_q: 3.598812, mean_eps: 0.266781\n",
      " 2038244/3750000: episode: 2801, duration: 6.994s, episode steps: 941, steps per second: 135, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.014238, mae: 2.961505, mean_q: 3.567751, mean_eps: 0.266399\n",
      " 2039304/3750000: episode: 2802, duration: 7.981s, episode steps: 1060, steps per second: 133, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.012902, mae: 2.952456, mean_q: 3.548925, mean_eps: 0.266039\n",
      " 2039968/3750000: episode: 2803, duration: 4.927s, episode steps: 664, steps per second: 135, episode reward: 21.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.560 [0.000, 5.000],  loss: 0.011851, mae: 2.891888, mean_q: 3.473734, mean_eps: 0.265730\n",
      " 2040822/3750000: episode: 2804, duration: 6.347s, episode steps: 854, steps per second: 135, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.700 [0.000, 5.000],  loss: 0.013023, mae: 2.920669, mean_q: 3.508549, mean_eps: 0.265456\n",
      " 2041455/3750000: episode: 2805, duration: 4.704s, episode steps: 633, steps per second: 135, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.014643, mae: 2.996858, mean_q: 3.603286, mean_eps: 0.265190\n",
      " 2042329/3750000: episode: 2806, duration: 6.529s, episode steps: 874, steps per second: 134, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013125, mae: 3.018699, mean_q: 3.634434, mean_eps: 0.264920\n",
      " 2043041/3750000: episode: 2807, duration: 5.315s, episode steps: 712, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013389, mae: 2.975288, mean_q: 3.575133, mean_eps: 0.264632\n",
      " 2043770/3750000: episode: 2808, duration: 5.422s, episode steps: 729, steps per second: 134, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.433 [0.000, 5.000],  loss: 0.016439, mae: 2.962363, mean_q: 3.562339, mean_eps: 0.264372\n",
      " 2044616/3750000: episode: 2809, duration: 6.247s, episode steps: 846, steps per second: 135, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.016960, mae: 2.910699, mean_q: 3.497992, mean_eps: 0.264092\n",
      " 2045668/3750000: episode: 2810, duration: 7.874s, episode steps: 1052, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.014249, mae: 2.935293, mean_q: 3.531947, mean_eps: 0.263750\n",
      " 2046253/3750000: episode: 2811, duration: 4.330s, episode steps: 585, steps per second: 135, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.014312, mae: 2.961566, mean_q: 3.566926, mean_eps: 0.263454\n",
      " 2047038/3750000: episode: 2812, duration: 5.786s, episode steps: 785, steps per second: 136, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.014965, mae: 2.879950, mean_q: 3.465502, mean_eps: 0.263210\n",
      " 2047751/3750000: episode: 2813, duration: 5.408s, episode steps: 713, steps per second: 132, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.019756, mae: 2.933836, mean_q: 3.529857, mean_eps: 0.262940\n",
      " 2048269/3750000: episode: 2814, duration: 3.970s, episode steps: 518, steps per second: 130, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.022800, mae: 2.969544, mean_q: 3.582020, mean_eps: 0.262716\n",
      " 2048991/3750000: episode: 2815, duration: 5.376s, episode steps: 722, steps per second: 134, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.013892, mae: 2.957791, mean_q: 3.556841, mean_eps: 0.262493\n",
      " 2049947/3750000: episode: 2816, duration: 7.207s, episode steps: 956, steps per second: 133, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.021159, mae: 2.918529, mean_q: 3.508506, mean_eps: 0.262191\n",
      " 2051034/3750000: episode: 2817, duration: 8.061s, episode steps: 1087, steps per second: 135, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.339 [0.000, 5.000],  loss: 0.015494, mae: 2.977631, mean_q: 3.585776, mean_eps: 0.261824\n",
      " 2051616/3750000: episode: 2818, duration: 4.344s, episode steps: 582, steps per second: 134, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.979 [0.000, 5.000],  loss: 0.016345, mae: 2.932629, mean_q: 3.530019, mean_eps: 0.261525\n",
      " 2052325/3750000: episode: 2819, duration: 5.355s, episode steps: 709, steps per second: 132, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014037, mae: 2.962346, mean_q: 3.562490, mean_eps: 0.261291\n",
      " 2053475/3750000: episode: 2820, duration: 8.451s, episode steps: 1150, steps per second: 136, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.015818, mae: 2.996536, mean_q: 3.607279, mean_eps: 0.260956\n",
      " 2053846/3750000: episode: 2821, duration: 2.801s, episode steps: 371, steps per second: 132, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.025095, mae: 2.988393, mean_q: 3.593992, mean_eps: 0.260682\n",
      " 2054451/3750000: episode: 2822, duration: 4.549s, episode steps: 605, steps per second: 133, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.011732, mae: 3.017752, mean_q: 3.639798, mean_eps: 0.260506\n",
      " 2055858/3750000: episode: 2823, duration: 10.394s, episode steps: 1407, steps per second: 135, episode reward: 23.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.015249, mae: 3.025365, mean_q: 3.649691, mean_eps: 0.260146\n",
      " 2056516/3750000: episode: 2824, duration: 4.982s, episode steps: 658, steps per second: 132, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.012567, mae: 3.045472, mean_q: 3.663541, mean_eps: 0.259775\n",
      " 2057562/3750000: episode: 2825, duration: 7.782s, episode steps: 1046, steps per second: 134, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.019769, mae: 3.047505, mean_q: 3.664222, mean_eps: 0.259466\n",
      " 2058229/3750000: episode: 2826, duration: 4.976s, episode steps: 667, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.019 [0.000, 5.000],  loss: 0.014827, mae: 3.038690, mean_q: 3.649918, mean_eps: 0.259156\n",
      " 2058600/3750000: episode: 2827, duration: 2.833s, episode steps: 371, steps per second: 131, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.020741, mae: 3.113219, mean_q: 3.745510, mean_eps: 0.258972\n",
      " 2059471/3750000: episode: 2828, duration: 6.804s, episode steps: 871, steps per second: 128, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.012224, mae: 3.039526, mean_q: 3.656085, mean_eps: 0.258749\n",
      " 2060374/3750000: episode: 2829, duration: 6.865s, episode steps: 903, steps per second: 132, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.017889, mae: 3.005445, mean_q: 3.611387, mean_eps: 0.258429\n",
      " 2060998/3750000: episode: 2830, duration: 4.664s, episode steps: 624, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.016160, mae: 3.082121, mean_q: 3.706987, mean_eps: 0.258155\n",
      " 2061665/3750000: episode: 2831, duration: 5.019s, episode steps: 667, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.016626, mae: 3.066764, mean_q: 3.689589, mean_eps: 0.257921\n",
      " 2063146/3750000: episode: 2832, duration: 11.057s, episode steps: 1481, steps per second: 134, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.016719, mae: 3.070504, mean_q: 3.692162, mean_eps: 0.257532\n",
      " 2064141/3750000: episode: 2833, duration: 7.462s, episode steps: 995, steps per second: 133, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.011812, mae: 3.038870, mean_q: 3.651665, mean_eps: 0.257086\n",
      " 2065663/3750000: episode: 2834, duration: 11.292s, episode steps: 1522, steps per second: 135, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.016210, mae: 3.032471, mean_q: 3.650506, mean_eps: 0.256632\n",
      " 2066206/3750000: episode: 2835, duration: 4.029s, episode steps: 543, steps per second: 135, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.010733, mae: 3.050693, mean_q: 3.667395, mean_eps: 0.256262\n",
      " 2066949/3750000: episode: 2836, duration: 5.563s, episode steps: 743, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.015535, mae: 3.049026, mean_q: 3.664841, mean_eps: 0.256031\n",
      " 2067780/3750000: episode: 2837, duration: 6.214s, episode steps: 831, steps per second: 134, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.015945, mae: 3.031332, mean_q: 3.642786, mean_eps: 0.255750\n",
      " 2068364/3750000: episode: 2838, duration: 4.419s, episode steps: 584, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.016729, mae: 3.021165, mean_q: 3.630388, mean_eps: 0.255495\n",
      " 2068933/3750000: episode: 2839, duration: 4.238s, episode steps: 569, steps per second: 134, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.023460, mae: 3.025073, mean_q: 3.639112, mean_eps: 0.255286\n",
      " 2069614/3750000: episode: 2840, duration: 5.073s, episode steps: 681, steps per second: 134, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.018408, mae: 3.054686, mean_q: 3.679183, mean_eps: 0.255063\n",
      " 2070189/3750000: episode: 2841, duration: 4.326s, episode steps: 575, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015458, mae: 3.057439, mean_q: 3.676444, mean_eps: 0.254836\n",
      " 2071040/3750000: episode: 2842, duration: 6.379s, episode steps: 851, steps per second: 133, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.016501, mae: 3.053563, mean_q: 3.671345, mean_eps: 0.254580\n",
      " 2071684/3750000: episode: 2843, duration: 4.891s, episode steps: 644, steps per second: 132, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.014055, mae: 3.067499, mean_q: 3.687429, mean_eps: 0.254310\n",
      " 2072632/3750000: episode: 2844, duration: 6.975s, episode steps: 948, steps per second: 136, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.012630, mae: 3.102137, mean_q: 3.733647, mean_eps: 0.254022\n",
      " 2073393/3750000: episode: 2845, duration: 5.712s, episode steps: 761, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.017879, mae: 3.068061, mean_q: 3.691989, mean_eps: 0.253716\n",
      " 2074098/3750000: episode: 2846, duration: 5.245s, episode steps: 705, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.018053, mae: 3.083095, mean_q: 3.708277, mean_eps: 0.253454\n",
      " 2075097/3750000: episode: 2847, duration: 7.408s, episode steps: 999, steps per second: 135, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.014292, mae: 3.038969, mean_q: 3.656466, mean_eps: 0.253148\n",
      " 2075735/3750000: episode: 2848, duration: 4.865s, episode steps: 638, steps per second: 131, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.566 [0.000, 5.000],  loss: 0.017234, mae: 3.098113, mean_q: 3.742866, mean_eps: 0.252852\n",
      " 2076958/3750000: episode: 2849, duration: 9.072s, episode steps: 1223, steps per second: 135, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 0.010829, mae: 3.091534, mean_q: 3.722605, mean_eps: 0.252518\n",
      " 2077343/3750000: episode: 2850, duration: 2.935s, episode steps: 385, steps per second: 131, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.016011, mae: 3.122394, mean_q: 3.762204, mean_eps: 0.252226\n",
      " 2078124/3750000: episode: 2851, duration: 5.840s, episode steps: 781, steps per second: 134, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.015005, mae: 3.149292, mean_q: 3.791468, mean_eps: 0.252014\n",
      " 2078874/3750000: episode: 2852, duration: 5.517s, episode steps: 750, steps per second: 136, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.203 [0.000, 5.000],  loss: 0.014510, mae: 3.117422, mean_q: 3.754155, mean_eps: 0.251740\n",
      " 2079563/3750000: episode: 2853, duration: 5.156s, episode steps: 689, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.016373, mae: 3.141927, mean_q: 3.785721, mean_eps: 0.251481\n",
      " 2080353/3750000: episode: 2854, duration: 5.945s, episode steps: 790, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.015591, mae: 3.098982, mean_q: 3.738542, mean_eps: 0.251214\n",
      " 2081512/3750000: episode: 2855, duration: 8.616s, episode steps: 1159, steps per second: 135, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.072 [0.000, 5.000],  loss: 0.019443, mae: 3.133802, mean_q: 3.773522, mean_eps: 0.250865\n",
      " 2081991/3750000: episode: 2856, duration: 3.628s, episode steps: 479, steps per second: 132, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.020733, mae: 3.139507, mean_q: 3.783917, mean_eps: 0.250570\n",
      " 2082615/3750000: episode: 2857, duration: 4.645s, episode steps: 624, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.240 [0.000, 5.000],  loss: 0.018455, mae: 3.116419, mean_q: 3.750566, mean_eps: 0.250372\n",
      " 2083237/3750000: episode: 2858, duration: 4.648s, episode steps: 622, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.018604, mae: 3.175347, mean_q: 3.819804, mean_eps: 0.250149\n",
      " 2083933/3750000: episode: 2859, duration: 5.248s, episode steps: 696, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.391 [0.000, 5.000],  loss: 0.017702, mae: 3.179283, mean_q: 3.830389, mean_eps: 0.249911\n",
      " 2084452/3750000: episode: 2860, duration: 3.942s, episode steps: 519, steps per second: 132, episode reward: 16.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.671 [0.000, 5.000],  loss: 0.014432, mae: 3.178428, mean_q: 3.840518, mean_eps: 0.249692\n",
      " 2085595/3750000: episode: 2861, duration: 8.487s, episode steps: 1143, steps per second: 135, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.013273, mae: 3.143422, mean_q: 3.785503, mean_eps: 0.249393\n",
      " 2086256/3750000: episode: 2862, duration: 4.973s, episode steps: 661, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.012959, mae: 3.142664, mean_q: 3.785164, mean_eps: 0.249069\n",
      " 2086951/3750000: episode: 2863, duration: 5.183s, episode steps: 695, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.016533, mae: 3.111635, mean_q: 3.743655, mean_eps: 0.248824\n",
      " 2087491/3750000: episode: 2864, duration: 4.050s, episode steps: 540, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.011132, mae: 3.153605, mean_q: 3.802285, mean_eps: 0.248601\n",
      " 2088401/3750000: episode: 2865, duration: 6.922s, episode steps: 910, steps per second: 131, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.018313, mae: 3.172384, mean_q: 3.819112, mean_eps: 0.248338\n",
      " 2089261/3750000: episode: 2866, duration: 6.394s, episode steps: 860, steps per second: 135, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.014211, mae: 3.143179, mean_q: 3.778071, mean_eps: 0.248018\n",
      " 2090397/3750000: episode: 2867, duration: 8.441s, episode steps: 1136, steps per second: 135, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.017748, mae: 3.130857, mean_q: 3.772171, mean_eps: 0.247661\n",
      " 2091319/3750000: episode: 2868, duration: 6.906s, episode steps: 922, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013297, mae: 3.131781, mean_q: 3.771726, mean_eps: 0.247294\n",
      " 2092232/3750000: episode: 2869, duration: 6.839s, episode steps: 913, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.016034, mae: 3.127952, mean_q: 3.765986, mean_eps: 0.246963\n",
      " 2092645/3750000: episode: 2870, duration: 3.227s, episode steps: 413, steps per second: 128, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.017310, mae: 3.123296, mean_q: 3.757917, mean_eps: 0.246722\n",
      " 2094006/3750000: episode: 2871, duration: 10.112s, episode steps: 1361, steps per second: 135, episode reward: 15.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 0.018315, mae: 3.145934, mean_q: 3.791339, mean_eps: 0.246401\n",
      " 2094762/3750000: episode: 2872, duration: 5.790s, episode steps: 756, steps per second: 131, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.019056, mae: 3.125321, mean_q: 3.765264, mean_eps: 0.246020\n",
      " 2095576/3750000: episode: 2873, duration: 6.000s, episode steps: 814, steps per second: 136, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.013416, mae: 3.141004, mean_q: 3.779536, mean_eps: 0.245739\n",
      " 2096281/3750000: episode: 2874, duration: 5.364s, episode steps: 705, steps per second: 131, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.014687, mae: 3.169535, mean_q: 3.810918, mean_eps: 0.245465\n",
      " 2097187/3750000: episode: 2875, duration: 6.764s, episode steps: 906, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.198 [0.000, 5.000],  loss: 0.015714, mae: 3.140750, mean_q: 3.781556, mean_eps: 0.245174\n",
      " 2097825/3750000: episode: 2876, duration: 4.756s, episode steps: 638, steps per second: 134, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.014372, mae: 3.152731, mean_q: 3.796531, mean_eps: 0.244896\n",
      " 2098979/3750000: episode: 2877, duration: 8.623s, episode steps: 1154, steps per second: 134, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.260 [0.000, 5.000],  loss: 0.016182, mae: 3.185954, mean_q: 3.836920, mean_eps: 0.244576\n",
      " 2099468/3750000: episode: 2878, duration: 3.729s, episode steps: 489, steps per second: 131, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.581 [0.000, 5.000],  loss: 0.024427, mae: 3.269342, mean_q: 3.932783, mean_eps: 0.244281\n",
      " 2100142/3750000: episode: 2879, duration: 5.043s, episode steps: 674, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013421, mae: 3.195579, mean_q: 3.846043, mean_eps: 0.244068\n",
      " 2100547/3750000: episode: 2880, duration: 3.083s, episode steps: 405, steps per second: 131, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.015881, mae: 3.184097, mean_q: 3.838457, mean_eps: 0.243874\n",
      " 2101162/3750000: episode: 2881, duration: 4.799s, episode steps: 615, steps per second: 128, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.023802, mae: 3.175923, mean_q: 3.826603, mean_eps: 0.243690\n",
      " 2102172/3750000: episode: 2882, duration: 7.465s, episode steps: 1010, steps per second: 135, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.015741, mae: 3.212800, mean_q: 3.868356, mean_eps: 0.243399\n",
      " 2102792/3750000: episode: 2883, duration: 4.602s, episode steps: 620, steps per second: 135, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.319 [0.000, 5.000],  loss: 0.015309, mae: 3.234491, mean_q: 3.890566, mean_eps: 0.243107\n",
      " 2103455/3750000: episode: 2884, duration: 5.016s, episode steps: 663, steps per second: 132, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.019112, mae: 3.188180, mean_q: 3.833912, mean_eps: 0.242877\n",
      " 2104538/3750000: episode: 2885, duration: 8.002s, episode steps: 1083, steps per second: 135, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.019559, mae: 3.186655, mean_q: 3.835624, mean_eps: 0.242564\n",
      " 2105154/3750000: episode: 2886, duration: 4.610s, episode steps: 616, steps per second: 134, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.017473, mae: 3.161591, mean_q: 3.805395, mean_eps: 0.242258\n",
      " 2106015/3750000: episode: 2887, duration: 6.451s, episode steps: 861, steps per second: 133, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.010133, mae: 3.195651, mean_q: 3.846469, mean_eps: 0.241991\n",
      " 2106550/3750000: episode: 2888, duration: 4.008s, episode steps: 535, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.415 [0.000, 5.000],  loss: 0.020360, mae: 3.231002, mean_q: 3.888006, mean_eps: 0.241739\n",
      " 2107298/3750000: episode: 2889, duration: 5.544s, episode steps: 748, steps per second: 135, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.019368, mae: 3.245433, mean_q: 3.902494, mean_eps: 0.241509\n",
      " 2107803/3750000: episode: 2890, duration: 3.941s, episode steps: 505, steps per second: 128, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.024583, mae: 3.201183, mean_q: 3.849937, mean_eps: 0.241282\n",
      " 2108692/3750000: episode: 2891, duration: 6.575s, episode steps: 889, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.022532, mae: 3.225859, mean_q: 3.890234, mean_eps: 0.241030\n",
      " 2109685/3750000: episode: 2892, duration: 7.576s, episode steps: 993, steps per second: 131, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012867, mae: 3.224944, mean_q: 3.890535, mean_eps: 0.240692\n",
      " 2110317/3750000: episode: 2893, duration: 4.658s, episode steps: 632, steps per second: 136, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.017055, mae: 3.279795, mean_q: 3.953662, mean_eps: 0.240400\n",
      " 2110837/3750000: episode: 2894, duration: 3.881s, episode steps: 520, steps per second: 134, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.020749, mae: 3.220513, mean_q: 3.879705, mean_eps: 0.240195\n",
      " 2111503/3750000: episode: 2895, duration: 5.014s, episode steps: 666, steps per second: 133, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.017581, mae: 3.307959, mean_q: 3.978897, mean_eps: 0.239979\n",
      " 2112915/3750000: episode: 2896, duration: 10.561s, episode steps: 1412, steps per second: 134, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.015019, mae: 3.301365, mean_q: 3.977289, mean_eps: 0.239604\n",
      " 2113575/3750000: episode: 2897, duration: 4.899s, episode steps: 660, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.019780, mae: 3.270288, mean_q: 3.928591, mean_eps: 0.239234\n",
      " 2114193/3750000: episode: 2898, duration: 4.663s, episode steps: 618, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.738 [0.000, 5.000],  loss: 0.015629, mae: 3.307424, mean_q: 3.981534, mean_eps: 0.239003\n",
      " 2114708/3750000: episode: 2899, duration: 3.865s, episode steps: 515, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.017558, mae: 3.245401, mean_q: 3.902552, mean_eps: 0.238798\n",
      " 2115293/3750000: episode: 2900, duration: 4.387s, episode steps: 585, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.253 [0.000, 5.000],  loss: 0.015634, mae: 3.186337, mean_q: 3.841207, mean_eps: 0.238600\n",
      " 2115755/3750000: episode: 2901, duration: 3.461s, episode steps: 462, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.016681, mae: 3.279094, mean_q: 3.946341, mean_eps: 0.238413\n",
      " 2116398/3750000: episode: 2902, duration: 4.932s, episode steps: 643, steps per second: 130, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.019337, mae: 3.242542, mean_q: 3.904936, mean_eps: 0.238215\n",
      " 2117260/3750000: episode: 2903, duration: 6.496s, episode steps: 862, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.017640, mae: 3.245758, mean_q: 3.905730, mean_eps: 0.237945\n",
      " 2118804/3750000: episode: 2904, duration: 11.580s, episode steps: 1544, steps per second: 133, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.021352, mae: 3.242177, mean_q: 3.900887, mean_eps: 0.237509\n",
      " 2119737/3750000: episode: 2905, duration: 6.875s, episode steps: 933, steps per second: 136, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.019700, mae: 3.248573, mean_q: 3.914869, mean_eps: 0.237063\n",
      " 2120531/3750000: episode: 2906, duration: 5.995s, episode steps: 794, steps per second: 132, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.016729, mae: 3.264001, mean_q: 3.925690, mean_eps: 0.236753\n",
      " 2121354/3750000: episode: 2907, duration: 6.093s, episode steps: 823, steps per second: 135, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.023410, mae: 3.246274, mean_q: 3.908133, mean_eps: 0.236462\n",
      " 2122205/3750000: episode: 2908, duration: 6.361s, episode steps: 851, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.015789, mae: 3.258744, mean_q: 3.921824, mean_eps: 0.236159\n",
      " 2123213/3750000: episode: 2909, duration: 7.543s, episode steps: 1008, steps per second: 134, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.015287, mae: 3.217278, mean_q: 3.873863, mean_eps: 0.235824\n",
      " 2123951/3750000: episode: 2910, duration: 5.486s, episode steps: 738, steps per second: 135, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.019051, mae: 3.201683, mean_q: 3.850452, mean_eps: 0.235511\n",
      " 2124482/3750000: episode: 2911, duration: 4.012s, episode steps: 531, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.522 [0.000, 5.000],  loss: 0.016187, mae: 3.296992, mean_q: 3.964809, mean_eps: 0.235281\n",
      " 2125010/3750000: episode: 2912, duration: 3.957s, episode steps: 528, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.295 [0.000, 5.000],  loss: 0.010451, mae: 3.226107, mean_q: 3.883731, mean_eps: 0.235090\n",
      " 2125518/3750000: episode: 2913, duration: 3.783s, episode steps: 508, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.020990, mae: 3.182674, mean_q: 3.823333, mean_eps: 0.234906\n",
      " 2126733/3750000: episode: 2914, duration: 9.130s, episode steps: 1215, steps per second: 133, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.012889, mae: 3.258428, mean_q: 3.918074, mean_eps: 0.234597\n",
      " 2128051/3750000: episode: 2915, duration: 9.830s, episode steps: 1318, steps per second: 134, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.018801, mae: 3.266390, mean_q: 3.936722, mean_eps: 0.234140\n",
      " 2128754/3750000: episode: 2916, duration: 5.392s, episode steps: 703, steps per second: 130, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.013377, mae: 3.328637, mean_q: 4.009254, mean_eps: 0.233776\n",
      " 2129237/3750000: episode: 2917, duration: 3.625s, episode steps: 483, steps per second: 133, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.841 [0.000, 5.000],  loss: 0.018547, mae: 3.341250, mean_q: 4.022017, mean_eps: 0.233564\n",
      " 2130003/3750000: episode: 2918, duration: 5.772s, episode steps: 766, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.219 [0.000, 5.000],  loss: 0.016277, mae: 3.268689, mean_q: 3.940359, mean_eps: 0.233337\n",
      " 2130480/3750000: episode: 2919, duration: 3.577s, episode steps: 477, steps per second: 133, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.038 [0.000, 5.000],  loss: 0.019239, mae: 3.350407, mean_q: 4.028271, mean_eps: 0.233114\n",
      " 2131360/3750000: episode: 2920, duration: 6.684s, episode steps: 880, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.762 [0.000, 5.000],  loss: 0.019505, mae: 3.281445, mean_q: 3.951124, mean_eps: 0.232872\n",
      " 2132297/3750000: episode: 2921, duration: 6.969s, episode steps: 937, steps per second: 134, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.016543, mae: 3.291009, mean_q: 3.956224, mean_eps: 0.232545\n",
      " 2133272/3750000: episode: 2922, duration: 7.322s, episode steps: 975, steps per second: 133, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.489 [0.000, 5.000],  loss: 0.016983, mae: 3.269164, mean_q: 3.931703, mean_eps: 0.232199\n",
      " 2133974/3750000: episode: 2923, duration: 5.216s, episode steps: 702, steps per second: 135, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.516 [0.000, 5.000],  loss: 0.012794, mae: 3.300840, mean_q: 3.978908, mean_eps: 0.231897\n",
      " 2134499/3750000: episode: 2924, duration: 3.894s, episode steps: 525, steps per second: 135, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.014627, mae: 3.303624, mean_q: 3.982583, mean_eps: 0.231677\n",
      " 2135232/3750000: episode: 2925, duration: 5.584s, episode steps: 733, steps per second: 131, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.012746, mae: 3.229671, mean_q: 3.890326, mean_eps: 0.231450\n",
      " 2136172/3750000: episode: 2926, duration: 6.971s, episode steps: 940, steps per second: 135, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.018745, mae: 3.260757, mean_q: 3.925256, mean_eps: 0.231148\n",
      " 2137533/3750000: episode: 2927, duration: 10.171s, episode steps: 1361, steps per second: 134, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.014312, mae: 3.303592, mean_q: 3.973687, mean_eps: 0.230734\n",
      " 2138714/3750000: episode: 2928, duration: 8.717s, episode steps: 1181, steps per second: 135, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.017710, mae: 3.326048, mean_q: 3.999576, mean_eps: 0.230277\n",
      " 2139045/3750000: episode: 2929, duration: 2.525s, episode steps: 331, steps per second: 131, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.025183, mae: 3.279384, mean_q: 3.944528, mean_eps: 0.230003\n",
      " 2139713/3750000: episode: 2930, duration: 4.971s, episode steps: 668, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.020507, mae: 3.281300, mean_q: 3.944494, mean_eps: 0.229823\n",
      " 2140432/3750000: episode: 2931, duration: 5.387s, episode steps: 719, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.661 [0.000, 5.000],  loss: 0.014881, mae: 3.253512, mean_q: 3.916398, mean_eps: 0.229575\n",
      " 2141294/3750000: episode: 2932, duration: 6.402s, episode steps: 862, steps per second: 135, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.012591, mae: 3.274534, mean_q: 3.943251, mean_eps: 0.229290\n",
      " 2141848/3750000: episode: 2933, duration: 4.204s, episode steps: 554, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.020442, mae: 3.232896, mean_q: 3.891685, mean_eps: 0.229035\n",
      " 2142897/3750000: episode: 2934, duration: 7.752s, episode steps: 1049, steps per second: 135, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.017339, mae: 3.272493, mean_q: 3.935247, mean_eps: 0.228747\n",
      " 2143802/3750000: episode: 2935, duration: 6.912s, episode steps: 905, steps per second: 131, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.910 [0.000, 5.000],  loss: 0.015760, mae: 3.225539, mean_q: 3.878079, mean_eps: 0.228394\n",
      " 2144310/3750000: episode: 2936, duration: 3.803s, episode steps: 508, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.728 [0.000, 5.000],  loss: 0.016167, mae: 3.291511, mean_q: 3.956079, mean_eps: 0.228138\n",
      " 2144863/3750000: episode: 2937, duration: 4.148s, episode steps: 553, steps per second: 133, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.622 [0.000, 5.000],  loss: 0.024668, mae: 3.260300, mean_q: 3.927082, mean_eps: 0.227948\n",
      " 2145234/3750000: episode: 2938, duration: 2.722s, episode steps: 371, steps per second: 136, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.539 [0.000, 5.000],  loss: 0.024286, mae: 3.318108, mean_q: 4.000562, mean_eps: 0.227782\n",
      " 2145746/3750000: episode: 2939, duration: 3.900s, episode steps: 512, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.727 [0.000, 5.000],  loss: 0.017190, mae: 3.260839, mean_q: 3.920206, mean_eps: 0.227624\n",
      " 2146803/3750000: episode: 2940, duration: 7.940s, episode steps: 1057, steps per second: 133, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015282, mae: 3.325133, mean_q: 4.000722, mean_eps: 0.227339\n",
      " 2147556/3750000: episode: 2941, duration: 5.645s, episode steps: 753, steps per second: 133, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.015677, mae: 3.250869, mean_q: 3.910615, mean_eps: 0.227015\n",
      " 2148043/3750000: episode: 2942, duration: 3.747s, episode steps: 487, steps per second: 130, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.489 [0.000, 5.000],  loss: 0.017930, mae: 3.270983, mean_q: 3.931577, mean_eps: 0.226792\n",
      " 2148895/3750000: episode: 2943, duration: 6.353s, episode steps: 852, steps per second: 134, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.016428, mae: 3.322368, mean_q: 4.003640, mean_eps: 0.226551\n",
      " 2149320/3750000: episode: 2944, duration: 3.230s, episode steps: 425, steps per second: 132, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.633 [0.000, 5.000],  loss: 0.021350, mae: 3.302262, mean_q: 3.972713, mean_eps: 0.226324\n",
      " 2150492/3750000: episode: 2945, duration: 8.812s, episode steps: 1172, steps per second: 133, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.021029, mae: 3.293039, mean_q: 3.959765, mean_eps: 0.226036\n",
      " 2151339/3750000: episode: 2946, duration: 6.316s, episode steps: 847, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.012476, mae: 3.272842, mean_q: 3.939440, mean_eps: 0.225672\n",
      " 2152273/3750000: episode: 2947, duration: 6.992s, episode steps: 934, steps per second: 134, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.018024, mae: 3.320350, mean_q: 3.995823, mean_eps: 0.225352\n",
      " 2153144/3750000: episode: 2948, duration: 6.584s, episode steps: 871, steps per second: 132, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.014834, mae: 3.354712, mean_q: 4.040042, mean_eps: 0.225024\n",
      " 2154482/3750000: episode: 2949, duration: 9.942s, episode steps: 1338, steps per second: 135, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.938 [0.000, 5.000],  loss: 0.015496, mae: 3.278107, mean_q: 3.944940, mean_eps: 0.224625\n",
      " 2155568/3750000: episode: 2950, duration: 8.000s, episode steps: 1086, steps per second: 136, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.655 [0.000, 5.000],  loss: 0.018281, mae: 3.286533, mean_q: 3.955337, mean_eps: 0.224189\n",
      " 2156120/3750000: episode: 2951, duration: 4.159s, episode steps: 552, steps per second: 133, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.018921, mae: 3.295640, mean_q: 3.967655, mean_eps: 0.223898\n",
      " 2156588/3750000: episode: 2952, duration: 3.573s, episode steps: 468, steps per second: 131, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.626 [0.000, 5.000],  loss: 0.012690, mae: 3.293514, mean_q: 3.979423, mean_eps: 0.223714\n",
      " 2157081/3750000: episode: 2953, duration: 3.712s, episode steps: 493, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.099 [0.000, 5.000],  loss: 0.019765, mae: 3.260699, mean_q: 3.924622, mean_eps: 0.223538\n",
      " 2157472/3750000: episode: 2954, duration: 2.887s, episode steps: 391, steps per second: 135, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.767 [0.000, 5.000],  loss: 0.016439, mae: 3.298407, mean_q: 3.972980, mean_eps: 0.223379\n",
      " 2158303/3750000: episode: 2955, duration: 6.248s, episode steps: 831, steps per second: 133, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.018479, mae: 3.283588, mean_q: 3.953091, mean_eps: 0.223160\n",
      " 2159074/3750000: episode: 2956, duration: 5.730s, episode steps: 771, steps per second: 135, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.015735, mae: 3.229321, mean_q: 3.883439, mean_eps: 0.222872\n",
      " 2159792/3750000: episode: 2957, duration: 5.388s, episode steps: 718, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.226 [0.000, 5.000],  loss: 0.019037, mae: 3.227056, mean_q: 3.881523, mean_eps: 0.222605\n",
      " 2160477/3750000: episode: 2958, duration: 5.046s, episode steps: 685, steps per second: 136, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.015998, mae: 3.224176, mean_q: 3.878735, mean_eps: 0.222353\n",
      " 2161865/3750000: episode: 2959, duration: 10.434s, episode steps: 1388, steps per second: 133, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.018110, mae: 3.252977, mean_q: 3.921686, mean_eps: 0.221979\n",
      " 2162933/3750000: episode: 2960, duration: 7.906s, episode steps: 1068, steps per second: 135, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.019107, mae: 3.251173, mean_q: 3.911428, mean_eps: 0.221536\n",
      " 2163941/3750000: episode: 2961, duration: 7.635s, episode steps: 1008, steps per second: 132, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.014434, mae: 3.289940, mean_q: 3.965247, mean_eps: 0.221162\n",
      " 2164539/3750000: episode: 2962, duration: 4.371s, episode steps: 598, steps per second: 137, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.013759, mae: 3.288800, mean_q: 3.953332, mean_eps: 0.220874\n",
      " 2165242/3750000: episode: 2963, duration: 5.401s, episode steps: 703, steps per second: 130, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.019913, mae: 3.303226, mean_q: 3.975340, mean_eps: 0.220640\n",
      " 2166035/3750000: episode: 2964, duration: 5.851s, episode steps: 793, steps per second: 136, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.015045, mae: 3.308900, mean_q: 3.976929, mean_eps: 0.220370\n",
      " 2166938/3750000: episode: 2965, duration: 6.752s, episode steps: 903, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.016653, mae: 3.266526, mean_q: 3.930432, mean_eps: 0.220067\n",
      " 2167872/3750000: episode: 2966, duration: 7.059s, episode steps: 934, steps per second: 132, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.015865, mae: 3.244521, mean_q: 3.902503, mean_eps: 0.219736\n",
      " 2168515/3750000: episode: 2967, duration: 4.807s, episode steps: 643, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.611 [0.000, 5.000],  loss: 0.015055, mae: 3.227297, mean_q: 3.880155, mean_eps: 0.219452\n",
      " 2168891/3750000: episode: 2968, duration: 2.835s, episode steps: 376, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.447 [0.000, 5.000],  loss: 0.018188, mae: 3.170765, mean_q: 3.811511, mean_eps: 0.219268\n",
      " 2169730/3750000: episode: 2969, duration: 6.328s, episode steps: 839, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.409 [0.000, 5.000],  loss: 0.021072, mae: 3.255690, mean_q: 3.920653, mean_eps: 0.219048\n",
      " 2170486/3750000: episode: 2970, duration: 5.652s, episode steps: 756, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.019987, mae: 3.254097, mean_q: 3.914526, mean_eps: 0.218760\n",
      " 2171587/3750000: episode: 2971, duration: 8.217s, episode steps: 1101, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.012549, mae: 3.287214, mean_q: 3.960661, mean_eps: 0.218426\n",
      " 2172409/3750000: episode: 2972, duration: 6.086s, episode steps: 822, steps per second: 135, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.018085, mae: 3.274158, mean_q: 3.947978, mean_eps: 0.218080\n",
      " 2173294/3750000: episode: 2973, duration: 6.551s, episode steps: 885, steps per second: 135, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.534 [0.000, 5.000],  loss: 0.018355, mae: 3.310517, mean_q: 3.991188, mean_eps: 0.217774\n",
      " 2174505/3750000: episode: 2974, duration: 9.293s, episode steps: 1211, steps per second: 130, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.318 [0.000, 5.000],  loss: 0.017317, mae: 3.345344, mean_q: 4.026066, mean_eps: 0.217396\n",
      " 2175390/3750000: episode: 2975, duration: 6.586s, episode steps: 885, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.015516, mae: 3.304226, mean_q: 3.973162, mean_eps: 0.217018\n",
      " 2176253/3750000: episode: 2976, duration: 6.547s, episode steps: 863, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.399 [0.000, 5.000],  loss: 0.013778, mae: 3.311762, mean_q: 3.984888, mean_eps: 0.216705\n",
      " 2176629/3750000: episode: 2977, duration: 2.835s, episode steps: 376, steps per second: 133, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.713 [0.000, 5.000],  loss: 0.017608, mae: 3.283471, mean_q: 3.945341, mean_eps: 0.216482\n",
      " 2177431/3750000: episode: 2978, duration: 5.959s, episode steps: 802, steps per second: 135, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 4.145 [0.000, 5.000],  loss: 0.018168, mae: 3.244803, mean_q: 3.915714, mean_eps: 0.216269\n",
      " 2178271/3750000: episode: 2979, duration: 6.332s, episode steps: 840, steps per second: 133, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.010236, mae: 3.243528, mean_q: 3.905628, mean_eps: 0.215974\n",
      " 2179068/3750000: episode: 2980, duration: 5.923s, episode steps: 797, steps per second: 135, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.075 [0.000, 5.000],  loss: 0.015451, mae: 3.298025, mean_q: 3.967263, mean_eps: 0.215679\n",
      " 2179950/3750000: episode: 2981, duration: 6.533s, episode steps: 882, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.013987, mae: 3.301791, mean_q: 3.981468, mean_eps: 0.215376\n",
      " 2180735/3750000: episode: 2982, duration: 5.864s, episode steps: 785, steps per second: 134, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.018534, mae: 3.318505, mean_q: 3.991398, mean_eps: 0.215078\n",
      " 2181650/3750000: episode: 2983, duration: 6.823s, episode steps: 915, steps per second: 134, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.422 [0.000, 5.000],  loss: 0.015163, mae: 3.316610, mean_q: 3.986376, mean_eps: 0.214772\n",
      " 2182248/3750000: episode: 2984, duration: 4.540s, episode steps: 598, steps per second: 132, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.021131, mae: 3.247626, mean_q: 3.907188, mean_eps: 0.214498\n",
      " 2182746/3750000: episode: 2985, duration: 3.726s, episode steps: 498, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.169 [0.000, 5.000],  loss: 0.016968, mae: 3.368101, mean_q: 4.047988, mean_eps: 0.214300\n",
      " 2183410/3750000: episode: 2986, duration: 4.930s, episode steps: 664, steps per second: 135, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.387 [0.000, 5.000],  loss: 0.020297, mae: 3.279521, mean_q: 3.940533, mean_eps: 0.214091\n",
      " 2183920/3750000: episode: 2987, duration: 3.837s, episode steps: 510, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.741 [0.000, 5.000],  loss: 0.014971, mae: 3.399489, mean_q: 4.090844, mean_eps: 0.213882\n",
      " 2184789/3750000: episode: 2988, duration: 6.570s, episode steps: 869, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.019737, mae: 3.353947, mean_q: 4.035446, mean_eps: 0.213634\n",
      " 2185889/3750000: episode: 2989, duration: 8.150s, episode steps: 1100, steps per second: 135, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.014666, mae: 3.334478, mean_q: 4.011468, mean_eps: 0.213278\n",
      " 2186612/3750000: episode: 2990, duration: 5.439s, episode steps: 723, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.599 [0.000, 5.000],  loss: 0.016701, mae: 3.307025, mean_q: 3.979895, mean_eps: 0.212950\n",
      " 2187346/3750000: episode: 2991, duration: 5.485s, episode steps: 734, steps per second: 134, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.902 [0.000, 5.000],  loss: 0.014907, mae: 3.300551, mean_q: 3.971831, mean_eps: 0.212687\n",
      " 2187965/3750000: episode: 2992, duration: 4.626s, episode steps: 619, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.012604, mae: 3.338971, mean_q: 4.018620, mean_eps: 0.212442\n",
      " 2189158/3750000: episode: 2993, duration: 8.840s, episode steps: 1193, steps per second: 135, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.015360, mae: 3.298752, mean_q: 3.967856, mean_eps: 0.212118\n",
      " 2189876/3750000: episode: 2994, duration: 5.376s, episode steps: 718, steps per second: 134, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.103 [0.000, 5.000],  loss: 0.013099, mae: 3.332637, mean_q: 4.010132, mean_eps: 0.211776\n",
      " 2190424/3750000: episode: 2995, duration: 4.166s, episode steps: 548, steps per second: 132, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.068 [0.000, 5.000],  loss: 0.016943, mae: 3.348402, mean_q: 4.039650, mean_eps: 0.211546\n",
      " 2191158/3750000: episode: 2996, duration: 5.489s, episode steps: 734, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.643 [0.000, 5.000],  loss: 0.019223, mae: 3.305628, mean_q: 3.988394, mean_eps: 0.211316\n",
      " 2191626/3750000: episode: 2997, duration: 3.544s, episode steps: 468, steps per second: 132, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.018840, mae: 3.294272, mean_q: 3.963134, mean_eps: 0.211100\n",
      " 2192157/3750000: episode: 2998, duration: 3.981s, episode steps: 531, steps per second: 133, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.501 [0.000, 5.000],  loss: 0.021166, mae: 3.292245, mean_q: 3.969828, mean_eps: 0.210920\n",
      " 2192975/3750000: episode: 2999, duration: 6.096s, episode steps: 818, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.014373, mae: 3.329361, mean_q: 4.004771, mean_eps: 0.210678\n",
      " 2193717/3750000: episode: 3000, duration: 5.545s, episode steps: 742, steps per second: 134, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.642 [0.000, 5.000],  loss: 0.023259, mae: 3.287188, mean_q: 3.949780, mean_eps: 0.210398\n",
      " 2194550/3750000: episode: 3001, duration: 6.256s, episode steps: 833, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.034 [0.000, 5.000],  loss: 0.016094, mae: 3.342052, mean_q: 4.024968, mean_eps: 0.210113\n",
      " 2195671/3750000: episode: 3002, duration: 8.444s, episode steps: 1121, steps per second: 133, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015045, mae: 3.363454, mean_q: 4.043475, mean_eps: 0.209760\n",
      " 2196297/3750000: episode: 3003, duration: 4.653s, episode steps: 626, steps per second: 135, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.016235, mae: 3.324918, mean_q: 3.999480, mean_eps: 0.209447\n",
      " 2196985/3750000: episode: 3004, duration: 5.181s, episode steps: 688, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.308 [0.000, 5.000],  loss: 0.018239, mae: 3.337436, mean_q: 4.019480, mean_eps: 0.209210\n",
      " 2197600/3750000: episode: 3005, duration: 4.687s, episode steps: 615, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.017566, mae: 3.307329, mean_q: 3.973844, mean_eps: 0.208976\n",
      " 2198347/3750000: episode: 3006, duration: 5.628s, episode steps: 747, steps per second: 133, episode reward: 23.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.023796, mae: 3.354454, mean_q: 4.031497, mean_eps: 0.208731\n",
      " 2198982/3750000: episode: 3007, duration: 4.730s, episode steps: 635, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.019729, mae: 3.291154, mean_q: 3.959372, mean_eps: 0.208479\n",
      " 2199851/3750000: episode: 3008, duration: 6.489s, episode steps: 869, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.016387, mae: 3.307424, mean_q: 3.981888, mean_eps: 0.208209\n",
      " 2200551/3750000: episode: 3009, duration: 5.229s, episode steps: 700, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.010 [0.000, 5.000],  loss: 0.018441, mae: 3.320509, mean_q: 3.994528, mean_eps: 0.207928\n",
      " 2201206/3750000: episode: 3010, duration: 4.874s, episode steps: 655, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.017392, mae: 3.326772, mean_q: 4.008485, mean_eps: 0.207683\n",
      " 2202140/3750000: episode: 3011, duration: 7.045s, episode steps: 934, steps per second: 133, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.015314, mae: 3.347856, mean_q: 4.026611, mean_eps: 0.207399\n",
      " 2202830/3750000: episode: 3012, duration: 5.156s, episode steps: 690, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.935 [0.000, 5.000],  loss: 0.019809, mae: 3.330018, mean_q: 4.004097, mean_eps: 0.207107\n",
      " 2203313/3750000: episode: 3013, duration: 3.597s, episode steps: 483, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.188 [0.000, 5.000],  loss: 0.021993, mae: 3.296970, mean_q: 3.969678, mean_eps: 0.206895\n",
      " 2204255/3750000: episode: 3014, duration: 7.064s, episode steps: 942, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.017913, mae: 3.376157, mean_q: 4.065630, mean_eps: 0.206639\n",
      " 2204586/3750000: episode: 3015, duration: 2.498s, episode steps: 331, steps per second: 133, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.010505, mae: 3.322798, mean_q: 4.007472, mean_eps: 0.206409\n",
      " 2205549/3750000: episode: 3016, duration: 7.164s, episode steps: 963, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.019764, mae: 3.325822, mean_q: 4.001384, mean_eps: 0.206175\n",
      " 2206822/3750000: episode: 3017, duration: 9.543s, episode steps: 1273, steps per second: 133, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.018825, mae: 3.357270, mean_q: 4.042385, mean_eps: 0.205772\n",
      " 2207451/3750000: episode: 3018, duration: 4.649s, episode steps: 629, steps per second: 135, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.015779, mae: 3.304705, mean_q: 3.980108, mean_eps: 0.205430\n",
      " 2208199/3750000: episode: 3019, duration: 5.765s, episode steps: 748, steps per second: 130, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.019417, mae: 3.301582, mean_q: 3.972079, mean_eps: 0.205185\n",
      " 2208741/3750000: episode: 3020, duration: 4.176s, episode steps: 542, steps per second: 130, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.515 [0.000, 5.000],  loss: 0.019478, mae: 3.266784, mean_q: 3.934257, mean_eps: 0.204951\n",
      " 2209636/3750000: episode: 3021, duration: 6.620s, episode steps: 895, steps per second: 135, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.431 [0.000, 5.000],  loss: 0.016269, mae: 3.257690, mean_q: 3.919660, mean_eps: 0.204692\n",
      " 2210186/3750000: episode: 3022, duration: 4.209s, episode steps: 550, steps per second: 131, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.016924, mae: 3.242374, mean_q: 3.900905, mean_eps: 0.204432\n",
      " 2211088/3750000: episode: 3023, duration: 6.703s, episode steps: 902, steps per second: 135, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.188 [0.000, 5.000],  loss: 0.015560, mae: 3.301622, mean_q: 3.972293, mean_eps: 0.204170\n",
      " 2212026/3750000: episode: 3024, duration: 6.977s, episode steps: 938, steps per second: 134, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.433 [0.000, 5.000],  loss: 0.019214, mae: 3.270387, mean_q: 3.939101, mean_eps: 0.203838\n",
      " 2212453/3750000: episode: 3025, duration: 3.246s, episode steps: 427, steps per second: 132, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.012624, mae: 3.260692, mean_q: 3.926159, mean_eps: 0.203594\n",
      " 2213188/3750000: episode: 3026, duration: 5.477s, episode steps: 735, steps per second: 134, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.335 [0.000, 5.000],  loss: 0.022400, mae: 3.327245, mean_q: 3.999805, mean_eps: 0.203385\n",
      " 2214531/3750000: episode: 3027, duration: 9.960s, episode steps: 1343, steps per second: 135, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.017943, mae: 3.271997, mean_q: 3.936908, mean_eps: 0.203010\n",
      " 2215100/3750000: episode: 3028, duration: 4.272s, episode steps: 569, steps per second: 133, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012193, mae: 3.270615, mean_q: 3.943918, mean_eps: 0.202668\n",
      " 2215968/3750000: episode: 3029, duration: 6.517s, episode steps: 868, steps per second: 133, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.013037, mae: 3.278663, mean_q: 3.943711, mean_eps: 0.202409\n",
      " 2216799/3750000: episode: 3030, duration: 6.191s, episode steps: 831, steps per second: 134, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.017528, mae: 3.224088, mean_q: 3.880532, mean_eps: 0.202103\n",
      " 2217289/3750000: episode: 3031, duration: 3.681s, episode steps: 490, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.014278, mae: 3.162239, mean_q: 3.805609, mean_eps: 0.201866\n",
      " 2218151/3750000: episode: 3032, duration: 6.440s, episode steps: 862, steps per second: 134, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.665 [0.000, 5.000],  loss: 0.020787, mae: 3.252122, mean_q: 3.915457, mean_eps: 0.201621\n",
      " 2218925/3750000: episode: 3033, duration: 5.828s, episode steps: 774, steps per second: 133, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.020923, mae: 3.278769, mean_q: 3.942540, mean_eps: 0.201326\n",
      " 2219588/3750000: episode: 3034, duration: 4.900s, episode steps: 663, steps per second: 135, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: 0.019440, mae: 3.337158, mean_q: 4.021079, mean_eps: 0.201066\n",
      " 2220363/3750000: episode: 3035, duration: 5.793s, episode steps: 775, steps per second: 134, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.016050, mae: 3.264408, mean_q: 3.934184, mean_eps: 0.200807\n",
      " 2221723/3750000: episode: 3036, duration: 10.229s, episode steps: 1360, steps per second: 133, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.018413, mae: 3.273512, mean_q: 3.941070, mean_eps: 0.200422\n",
      " 2222488/3750000: episode: 3037, duration: 5.672s, episode steps: 765, steps per second: 135, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.017129, mae: 3.214067, mean_q: 3.870929, mean_eps: 0.200040\n",
      " 2223000/3750000: episode: 3038, duration: 3.924s, episode steps: 512, steps per second: 130, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.023439, mae: 3.238439, mean_q: 3.893989, mean_eps: 0.199814\n",
      " 2224055/3750000: episode: 3039, duration: 7.997s, episode steps: 1055, steps per second: 132, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.332 [0.000, 5.000],  loss: 0.019456, mae: 3.181947, mean_q: 3.832856, mean_eps: 0.199533\n",
      " 2224583/3750000: episode: 3040, duration: 3.973s, episode steps: 528, steps per second: 133, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.019220, mae: 3.233040, mean_q: 3.892002, mean_eps: 0.199245\n",
      " 2225512/3750000: episode: 3041, duration: 6.932s, episode steps: 929, steps per second: 134, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.732 [0.000, 5.000],  loss: 0.018902, mae: 3.153243, mean_q: 3.792754, mean_eps: 0.198982\n",
      " 2226153/3750000: episode: 3042, duration: 4.769s, episode steps: 641, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.602 [0.000, 5.000],  loss: 0.017523, mae: 3.181881, mean_q: 3.834695, mean_eps: 0.198701\n",
      " 2226689/3750000: episode: 3043, duration: 4.025s, episode steps: 536, steps per second: 133, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.018281, mae: 3.158616, mean_q: 3.799243, mean_eps: 0.198489\n",
      " 2227172/3750000: episode: 3044, duration: 3.729s, episode steps: 483, steps per second: 130, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.021662, mae: 3.132400, mean_q: 3.764854, mean_eps: 0.198305\n",
      " 2228311/3750000: episode: 3045, duration: 8.486s, episode steps: 1139, steps per second: 134, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.014693, mae: 3.173036, mean_q: 3.821602, mean_eps: 0.198014\n",
      " 2229134/3750000: episode: 3046, duration: 6.117s, episode steps: 823, steps per second: 135, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.018814, mae: 3.196656, mean_q: 3.844675, mean_eps: 0.197661\n",
      " 2229601/3750000: episode: 3047, duration: 3.615s, episode steps: 467, steps per second: 129, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.019649, mae: 3.157336, mean_q: 3.800196, mean_eps: 0.197427\n",
      " 2230813/3750000: episode: 3048, duration: 8.928s, episode steps: 1212, steps per second: 136, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.017591, mae: 3.114956, mean_q: 3.745959, mean_eps: 0.197124\n",
      " 2231521/3750000: episode: 3049, duration: 5.371s, episode steps: 708, steps per second: 132, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.208 [0.000, 5.000],  loss: 0.014716, mae: 3.201409, mean_q: 3.850602, mean_eps: 0.196779\n",
      " 2232104/3750000: episode: 3050, duration: 4.316s, episode steps: 583, steps per second: 135, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.018481, mae: 3.162028, mean_q: 3.806482, mean_eps: 0.196545\n",
      " 2233283/3750000: episode: 3051, duration: 8.731s, episode steps: 1179, steps per second: 135, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.016434, mae: 3.158939, mean_q: 3.801806, mean_eps: 0.196228\n",
      " 2233899/3750000: episode: 3052, duration: 4.570s, episode steps: 616, steps per second: 135, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.015164, mae: 3.217166, mean_q: 3.873979, mean_eps: 0.195908\n",
      " 2234648/3750000: episode: 3053, duration: 5.599s, episode steps: 749, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.331 [0.000, 5.000],  loss: 0.011731, mae: 3.184899, mean_q: 3.837541, mean_eps: 0.195663\n",
      " 2235610/3750000: episode: 3054, duration: 7.163s, episode steps: 962, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.015376, mae: 3.174040, mean_q: 3.819868, mean_eps: 0.195353\n",
      " 2236175/3750000: episode: 3055, duration: 4.206s, episode steps: 565, steps per second: 134, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.015417, mae: 3.187147, mean_q: 3.836844, mean_eps: 0.195080\n",
      " 2236752/3750000: episode: 3056, duration: 4.290s, episode steps: 577, steps per second: 134, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.010893, mae: 3.197496, mean_q: 3.852284, mean_eps: 0.194874\n",
      " 2237407/3750000: episode: 3057, duration: 4.900s, episode steps: 655, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.016656, mae: 3.267853, mean_q: 3.933453, mean_eps: 0.194651\n",
      " 2238224/3750000: episode: 3058, duration: 6.131s, episode steps: 817, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.290 [0.000, 5.000],  loss: 0.017023, mae: 3.231674, mean_q: 3.891356, mean_eps: 0.194385\n",
      " 2239206/3750000: episode: 3059, duration: 7.237s, episode steps: 982, steps per second: 136, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.018101, mae: 3.234626, mean_q: 3.897168, mean_eps: 0.194061\n",
      " 2239906/3750000: episode: 3060, duration: 5.225s, episode steps: 700, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.007 [0.000, 5.000],  loss: 0.016326, mae: 3.200929, mean_q: 3.852814, mean_eps: 0.193758\n",
      " 2240350/3750000: episode: 3061, duration: 3.418s, episode steps: 444, steps per second: 130, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.716 [0.000, 5.000],  loss: 0.012896, mae: 3.197965, mean_q: 3.846605, mean_eps: 0.193553\n",
      " 2241500/3750000: episode: 3062, duration: 8.820s, episode steps: 1150, steps per second: 130, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.019425, mae: 3.228246, mean_q: 3.883827, mean_eps: 0.193269\n",
      " 2242179/3750000: episode: 3063, duration: 5.149s, episode steps: 679, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.016344, mae: 3.256076, mean_q: 3.917328, mean_eps: 0.192941\n",
      " 2243123/3750000: episode: 3064, duration: 7.124s, episode steps: 944, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016826, mae: 3.232079, mean_q: 3.884896, mean_eps: 0.192646\n",
      " 2244237/3750000: episode: 3065, duration: 8.226s, episode steps: 1114, steps per second: 135, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.895 [0.000, 5.000],  loss: 0.017410, mae: 3.223112, mean_q: 3.879165, mean_eps: 0.192275\n",
      " 2244995/3750000: episode: 3066, duration: 5.675s, episode steps: 758, steps per second: 134, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.005 [0.000, 5.000],  loss: 0.016882, mae: 3.210838, mean_q: 3.871947, mean_eps: 0.191940\n",
      " 2245876/3750000: episode: 3067, duration: 6.557s, episode steps: 881, steps per second: 134, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.014795, mae: 3.262543, mean_q: 3.923397, mean_eps: 0.191645\n",
      " 2247091/3750000: episode: 3068, duration: 9.143s, episode steps: 1215, steps per second: 133, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.926 [0.000, 5.000],  loss: 0.015775, mae: 3.261251, mean_q: 3.922233, mean_eps: 0.191267\n",
      " 2248379/3750000: episode: 3069, duration: 9.578s, episode steps: 1288, steps per second: 134, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.017174, mae: 3.192312, mean_q: 3.835215, mean_eps: 0.190817\n",
      " 2249273/3750000: episode: 3070, duration: 6.743s, episode steps: 894, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.021783, mae: 3.242326, mean_q: 3.893892, mean_eps: 0.190425\n",
      " 2250384/3750000: episode: 3071, duration: 8.443s, episode steps: 1111, steps per second: 132, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.017269, mae: 3.210200, mean_q: 3.865768, mean_eps: 0.190061\n",
      " 2251109/3750000: episode: 3072, duration: 5.451s, episode steps: 725, steps per second: 133, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.016525, mae: 3.217854, mean_q: 3.876616, mean_eps: 0.189730\n",
      " 2251836/3750000: episode: 3073, duration: 5.395s, episode steps: 727, steps per second: 135, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.437 [0.000, 5.000],  loss: 0.023986, mae: 3.224859, mean_q: 3.878015, mean_eps: 0.189471\n",
      " 2252556/3750000: episode: 3074, duration: 5.356s, episode steps: 720, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.017156, mae: 3.235161, mean_q: 3.897381, mean_eps: 0.189212\n",
      " 2253826/3750000: episode: 3075, duration: 9.556s, episode steps: 1270, steps per second: 133, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.018516, mae: 3.225408, mean_q: 3.889819, mean_eps: 0.188852\n",
      " 2254691/3750000: episode: 3076, duration: 6.393s, episode steps: 865, steps per second: 135, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.017682, mae: 3.205588, mean_q: 3.861354, mean_eps: 0.188466\n",
      " 2255346/3750000: episode: 3077, duration: 4.968s, episode steps: 655, steps per second: 132, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 4.376 [0.000, 5.000],  loss: 0.021300, mae: 3.287072, mean_q: 3.957955, mean_eps: 0.188193\n",
      " 2256266/3750000: episode: 3078, duration: 6.907s, episode steps: 920, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.016094, mae: 3.236523, mean_q: 3.895938, mean_eps: 0.187908\n",
      " 2256878/3750000: episode: 3079, duration: 4.502s, episode steps: 612, steps per second: 136, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.014357, mae: 3.294870, mean_q: 3.963907, mean_eps: 0.187635\n",
      " 2257802/3750000: episode: 3080, duration: 7.020s, episode steps: 924, steps per second: 132, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.449 [0.000, 5.000],  loss: 0.016357, mae: 3.317012, mean_q: 3.989095, mean_eps: 0.187358\n",
      " 2258314/3750000: episode: 3081, duration: 3.752s, episode steps: 512, steps per second: 136, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.686 [0.000, 5.000],  loss: 0.021547, mae: 3.245638, mean_q: 3.910561, mean_eps: 0.187098\n",
      " 2259293/3750000: episode: 3082, duration: 7.338s, episode steps: 979, steps per second: 133, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.021612, mae: 3.346241, mean_q: 4.022872, mean_eps: 0.186832\n",
      " 2259892/3750000: episode: 3083, duration: 4.459s, episode steps: 599, steps per second: 134, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.105 [0.000, 5.000],  loss: 0.016587, mae: 3.347236, mean_q: 4.034920, mean_eps: 0.186548\n",
      " 2260555/3750000: episode: 3084, duration: 4.964s, episode steps: 663, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.175 [0.000, 5.000],  loss: 0.012956, mae: 3.313135, mean_q: 3.988862, mean_eps: 0.186321\n",
      " 2261087/3750000: episode: 3085, duration: 3.983s, episode steps: 532, steps per second: 134, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.014771, mae: 3.255653, mean_q: 3.919450, mean_eps: 0.186105\n",
      " 2261934/3750000: episode: 3086, duration: 6.301s, episode steps: 847, steps per second: 134, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014446, mae: 3.287104, mean_q: 3.958895, mean_eps: 0.185856\n",
      " 2262756/3750000: episode: 3087, duration: 6.090s, episode steps: 822, steps per second: 135, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.019625, mae: 3.336399, mean_q: 4.013420, mean_eps: 0.185558\n",
      " 2263504/3750000: episode: 3088, duration: 5.652s, episode steps: 748, steps per second: 132, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.234 [0.000, 5.000],  loss: 0.015266, mae: 3.331801, mean_q: 4.015498, mean_eps: 0.185273\n",
      " 2264126/3750000: episode: 3089, duration: 4.626s, episode steps: 622, steps per second: 134, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.018188, mae: 3.278561, mean_q: 3.942980, mean_eps: 0.185025\n",
      " 2265207/3750000: episode: 3090, duration: 8.016s, episode steps: 1081, steps per second: 135, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.016004, mae: 3.232081, mean_q: 3.888795, mean_eps: 0.184719\n",
      " 2266110/3750000: episode: 3091, duration: 6.752s, episode steps: 903, steps per second: 134, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.012753, mae: 3.229952, mean_q: 3.886129, mean_eps: 0.184362\n",
      " 2267040/3750000: episode: 3092, duration: 6.924s, episode steps: 930, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.016114, mae: 3.240023, mean_q: 3.904156, mean_eps: 0.184035\n",
      " 2268324/3750000: episode: 3093, duration: 9.663s, episode steps: 1284, steps per second: 133, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.032 [0.000, 5.000],  loss: 0.017929, mae: 3.231386, mean_q: 3.889080, mean_eps: 0.183635\n",
      " 2269177/3750000: episode: 3094, duration: 6.304s, episode steps: 853, steps per second: 135, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.013421, mae: 3.211143, mean_q: 3.864118, mean_eps: 0.183250\n",
      " 2269999/3750000: episode: 3095, duration: 6.167s, episode steps: 822, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.013515, mae: 3.200685, mean_q: 3.851532, mean_eps: 0.182951\n",
      " 2270944/3750000: episode: 3096, duration: 7.061s, episode steps: 945, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.025969, mae: 3.180239, mean_q: 3.823540, mean_eps: 0.182631\n",
      " 2272077/3750000: episode: 3097, duration: 8.509s, episode steps: 1133, steps per second: 133, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.014425, mae: 3.137017, mean_q: 3.776354, mean_eps: 0.182256\n",
      " 2272884/3750000: episode: 3098, duration: 6.135s, episode steps: 807, steps per second: 132, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.013962, mae: 3.133981, mean_q: 3.775371, mean_eps: 0.181907\n",
      " 2273510/3750000: episode: 3099, duration: 4.661s, episode steps: 626, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.633 [0.000, 5.000],  loss: 0.016942, mae: 3.135298, mean_q: 3.776105, mean_eps: 0.181648\n",
      " 2274478/3750000: episode: 3100, duration: 7.222s, episode steps: 968, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.014754, mae: 3.133921, mean_q: 3.770340, mean_eps: 0.181364\n",
      " 2274990/3750000: episode: 3101, duration: 3.871s, episode steps: 512, steps per second: 132, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.013692, mae: 3.135752, mean_q: 3.777938, mean_eps: 0.181097\n",
      " 2275384/3750000: episode: 3102, duration: 3.057s, episode steps: 394, steps per second: 129, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.016290, mae: 3.253153, mean_q: 3.925201, mean_eps: 0.180932\n",
      " 2276295/3750000: episode: 3103, duration: 6.876s, episode steps: 911, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.018588, mae: 3.225944, mean_q: 3.884355, mean_eps: 0.180698\n",
      " 2276823/3750000: episode: 3104, duration: 4.006s, episode steps: 528, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.750 [0.000, 5.000],  loss: 0.019590, mae: 3.168699, mean_q: 3.820033, mean_eps: 0.180438\n",
      " 2277527/3750000: episode: 3105, duration: 5.231s, episode steps: 704, steps per second: 135, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.018239, mae: 3.182050, mean_q: 3.827464, mean_eps: 0.180215\n",
      " 2278189/3750000: episode: 3106, duration: 4.936s, episode steps: 662, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.013631, mae: 3.224565, mean_q: 3.882678, mean_eps: 0.179970\n",
      " 2279614/3750000: episode: 3107, duration: 10.601s, episode steps: 1425, steps per second: 134, episode reward: 26.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.301 [0.000, 5.000],  loss: 0.014652, mae: 3.220287, mean_q: 3.875266, mean_eps: 0.179596\n",
      " 2280304/3750000: episode: 3108, duration: 5.208s, episode steps: 690, steps per second: 132, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.018330, mae: 3.246353, mean_q: 3.907934, mean_eps: 0.179214\n",
      " 2280904/3750000: episode: 3109, duration: 4.520s, episode steps: 600, steps per second: 133, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.016832, mae: 3.232902, mean_q: 3.889681, mean_eps: 0.178980\n",
      " 2281528/3750000: episode: 3110, duration: 4.682s, episode steps: 624, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.013432, mae: 3.201143, mean_q: 3.860472, mean_eps: 0.178761\n",
      " 2282362/3750000: episode: 3111, duration: 6.257s, episode steps: 834, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.418 [0.000, 5.000],  loss: 0.015609, mae: 3.235769, mean_q: 3.894431, mean_eps: 0.178498\n",
      " 2283175/3750000: episode: 3112, duration: 6.024s, episode steps: 813, steps per second: 135, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.016956, mae: 3.229998, mean_q: 3.884990, mean_eps: 0.178203\n",
      " 2283572/3750000: episode: 3113, duration: 2.982s, episode steps: 397, steps per second: 133, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.014672, mae: 3.231090, mean_q: 3.886723, mean_eps: 0.177987\n",
      " 2284428/3750000: episode: 3114, duration: 6.346s, episode steps: 856, steps per second: 135, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 0.016392, mae: 3.215101, mean_q: 3.867630, mean_eps: 0.177760\n",
      " 2285149/3750000: episode: 3115, duration: 5.424s, episode steps: 721, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.018246, mae: 3.174353, mean_q: 3.824391, mean_eps: 0.177476\n",
      " 2286110/3750000: episode: 3116, duration: 7.172s, episode steps: 961, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.014220, mae: 3.216366, mean_q: 3.873443, mean_eps: 0.177173\n",
      " 2287268/3750000: episode: 3117, duration: 8.677s, episode steps: 1158, steps per second: 133, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.014377, mae: 3.193933, mean_q: 3.840867, mean_eps: 0.176792\n",
      " 2288054/3750000: episode: 3118, duration: 5.871s, episode steps: 786, steps per second: 134, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.013532, mae: 3.212065, mean_q: 3.859547, mean_eps: 0.176442\n",
      " 2288768/3750000: episode: 3119, duration: 5.438s, episode steps: 714, steps per second: 131, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.016895, mae: 3.225867, mean_q: 3.879233, mean_eps: 0.176172\n",
      " 2289978/3750000: episode: 3120, duration: 9.053s, episode steps: 1210, steps per second: 134, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.964 [0.000, 5.000],  loss: 0.018872, mae: 3.201585, mean_q: 3.854257, mean_eps: 0.175827\n",
      " 2290695/3750000: episode: 3121, duration: 5.391s, episode steps: 717, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.056 [0.000, 5.000],  loss: 0.019035, mae: 3.213741, mean_q: 3.862891, mean_eps: 0.175481\n",
      " 2291903/3750000: episode: 3122, duration: 9.070s, episode steps: 1208, steps per second: 133, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.016283, mae: 3.186305, mean_q: 3.830872, mean_eps: 0.175132\n",
      " 2293189/3750000: episode: 3123, duration: 9.496s, episode steps: 1286, steps per second: 135, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.013873, mae: 3.215445, mean_q: 3.871271, mean_eps: 0.174682\n",
      " 2293564/3750000: episode: 3124, duration: 2.912s, episode steps: 375, steps per second: 129, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.013936, mae: 3.226759, mean_q: 3.881429, mean_eps: 0.174383\n",
      " 2294198/3750000: episode: 3125, duration: 4.670s, episode steps: 634, steps per second: 136, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.298 [0.000, 5.000],  loss: 0.018393, mae: 3.196473, mean_q: 3.850993, mean_eps: 0.174203\n",
      " 2295189/3750000: episode: 3126, duration: 7.383s, episode steps: 991, steps per second: 134, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.106 [0.000, 5.000],  loss: 0.016150, mae: 3.244689, mean_q: 3.910798, mean_eps: 0.173912\n",
      " 2295690/3750000: episode: 3127, duration: 3.793s, episode steps: 501, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013755, mae: 3.235919, mean_q: 3.895042, mean_eps: 0.173642\n",
      " 2296294/3750000: episode: 3128, duration: 4.486s, episode steps: 604, steps per second: 135, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.017112, mae: 3.268086, mean_q: 3.939752, mean_eps: 0.173444\n",
      " 2297445/3750000: episode: 3129, duration: 8.595s, episode steps: 1151, steps per second: 134, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.015660, mae: 3.267330, mean_q: 3.931580, mean_eps: 0.173127\n",
      " 2298136/3750000: episode: 3130, duration: 5.212s, episode steps: 691, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.014256, mae: 3.269590, mean_q: 3.933927, mean_eps: 0.172796\n",
      " 2298637/3750000: episode: 3131, duration: 3.738s, episode steps: 501, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.010861, mae: 3.285402, mean_q: 3.954361, mean_eps: 0.172583\n",
      " 2299533/3750000: episode: 3132, duration: 6.690s, episode steps: 896, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.019903, mae: 3.258388, mean_q: 3.927188, mean_eps: 0.172331\n",
      " 2300046/3750000: episode: 3133, duration: 3.939s, episode steps: 513, steps per second: 130, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.018401, mae: 3.273726, mean_q: 3.939367, mean_eps: 0.172076\n",
      " 2300720/3750000: episode: 3134, duration: 5.014s, episode steps: 674, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.013916, mae: 3.297362, mean_q: 3.965952, mean_eps: 0.171863\n",
      " 2301122/3750000: episode: 3135, duration: 3.086s, episode steps: 402, steps per second: 130, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.341 [0.000, 5.000],  loss: 0.017655, mae: 3.283547, mean_q: 3.949562, mean_eps: 0.171669\n",
      " 2301755/3750000: episode: 3136, duration: 4.651s, episode steps: 633, steps per second: 136, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013490, mae: 3.304038, mean_q: 3.977798, mean_eps: 0.171482\n",
      " 2302127/3750000: episode: 3137, duration: 2.861s, episode steps: 372, steps per second: 130, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.255 [0.000, 5.000],  loss: 0.012698, mae: 3.360724, mean_q: 4.043695, mean_eps: 0.171302\n",
      " 2302970/3750000: episode: 3138, duration: 6.262s, episode steps: 843, steps per second: 135, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.011301, mae: 3.267006, mean_q: 3.929903, mean_eps: 0.171082\n",
      " 2303562/3750000: episode: 3139, duration: 4.450s, episode steps: 592, steps per second: 133, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.019438, mae: 3.281160, mean_q: 3.946632, mean_eps: 0.170823\n",
      " 2304358/3750000: episode: 3140, duration: 6.007s, episode steps: 796, steps per second: 133, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.180 [0.000, 5.000],  loss: 0.014030, mae: 3.291477, mean_q: 3.957701, mean_eps: 0.170574\n",
      " 2305299/3750000: episode: 3141, duration: 6.998s, episode steps: 941, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.015717, mae: 3.290120, mean_q: 3.962431, mean_eps: 0.170265\n",
      " 2306132/3750000: episode: 3142, duration: 6.234s, episode steps: 833, steps per second: 134, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.020829, mae: 3.279569, mean_q: 3.951326, mean_eps: 0.169944\n",
      " 2306929/3750000: episode: 3143, duration: 5.999s, episode steps: 797, steps per second: 133, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.012505, mae: 3.257033, mean_q: 3.919327, mean_eps: 0.169649\n",
      " 2307698/3750000: episode: 3144, duration: 5.731s, episode steps: 769, steps per second: 134, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.019240, mae: 3.257324, mean_q: 3.914005, mean_eps: 0.169368\n",
      " 2308624/3750000: episode: 3145, duration: 7.025s, episode steps: 926, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.014580, mae: 3.269366, mean_q: 3.935970, mean_eps: 0.169062\n",
      " 2309265/3750000: episode: 3146, duration: 4.765s, episode steps: 641, steps per second: 135, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.014429, mae: 3.289406, mean_q: 3.963834, mean_eps: 0.168778\n",
      " 2310028/3750000: episode: 3147, duration: 5.645s, episode steps: 763, steps per second: 135, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.152 [0.000, 5.000],  loss: 0.014171, mae: 3.204040, mean_q: 3.857139, mean_eps: 0.168526\n",
      " 2310899/3750000: episode: 3148, duration: 6.478s, episode steps: 871, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.015037, mae: 3.242614, mean_q: 3.896780, mean_eps: 0.168234\n",
      " 2311596/3750000: episode: 3149, duration: 5.210s, episode steps: 697, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.471 [0.000, 5.000],  loss: 0.022293, mae: 3.216430, mean_q: 3.873577, mean_eps: 0.167954\n",
      " 2312225/3750000: episode: 3150, duration: 4.721s, episode steps: 629, steps per second: 133, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.108 [0.000, 5.000],  loss: 0.014609, mae: 3.198672, mean_q: 3.845848, mean_eps: 0.167712\n",
      " 2312870/3750000: episode: 3151, duration: 4.849s, episode steps: 645, steps per second: 133, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.015278, mae: 3.227175, mean_q: 3.883542, mean_eps: 0.167482\n",
      " 2313743/3750000: episode: 3152, duration: 6.516s, episode steps: 873, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015447, mae: 3.142251, mean_q: 3.777757, mean_eps: 0.167208\n",
      " 2314230/3750000: episode: 3153, duration: 3.647s, episode steps: 487, steps per second: 134, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.013920, mae: 3.180388, mean_q: 3.823566, mean_eps: 0.166964\n",
      " 2315206/3750000: episode: 3154, duration: 7.385s, episode steps: 976, steps per second: 132, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.013349, mae: 3.165162, mean_q: 3.809037, mean_eps: 0.166701\n",
      " 2315691/3750000: episode: 3155, duration: 3.586s, episode steps: 485, steps per second: 135, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.016656, mae: 3.199478, mean_q: 3.841219, mean_eps: 0.166438\n",
      " 2316367/3750000: episode: 3156, duration: 5.044s, episode steps: 676, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.009905, mae: 3.193047, mean_q: 3.838704, mean_eps: 0.166229\n",
      " 2317501/3750000: episode: 3157, duration: 8.528s, episode steps: 1134, steps per second: 133, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.510 [0.000, 5.000],  loss: 0.012772, mae: 3.173200, mean_q: 3.821208, mean_eps: 0.165902\n",
      " 2318491/3750000: episode: 3158, duration: 7.323s, episode steps: 990, steps per second: 135, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.015847, mae: 3.111136, mean_q: 3.745716, mean_eps: 0.165520\n",
      " 2319451/3750000: episode: 3159, duration: 7.226s, episode steps: 960, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013356, mae: 3.203670, mean_q: 3.850615, mean_eps: 0.165171\n",
      " 2320389/3750000: episode: 3160, duration: 7.060s, episode steps: 938, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 0.013198, mae: 3.149397, mean_q: 3.788562, mean_eps: 0.164829\n",
      " 2320923/3750000: episode: 3161, duration: 3.994s, episode steps: 534, steps per second: 134, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 0.762 [0.000, 5.000],  loss: 0.012867, mae: 3.189992, mean_q: 3.843952, mean_eps: 0.164562\n",
      " 2321907/3750000: episode: 3162, duration: 7.374s, episode steps: 984, steps per second: 133, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.009698, mae: 3.165537, mean_q: 3.804362, mean_eps: 0.164289\n",
      " 2322531/3750000: episode: 3163, duration: 4.609s, episode steps: 624, steps per second: 135, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.017239, mae: 3.093008, mean_q: 3.724514, mean_eps: 0.164001\n",
      " 2323281/3750000: episode: 3164, duration: 5.651s, episode steps: 750, steps per second: 133, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.017043, mae: 3.119155, mean_q: 3.755132, mean_eps: 0.163752\n",
      " 2324621/3750000: episode: 3165, duration: 9.996s, episode steps: 1340, steps per second: 134, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.016496, mae: 3.121546, mean_q: 3.766219, mean_eps: 0.163374\n",
      " 2325165/3750000: episode: 3166, duration: 4.001s, episode steps: 544, steps per second: 136, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.774 [0.000, 5.000],  loss: 0.022199, mae: 3.131000, mean_q: 3.764781, mean_eps: 0.163036\n",
      " 2326164/3750000: episode: 3167, duration: 7.498s, episode steps: 999, steps per second: 133, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.012762, mae: 3.058142, mean_q: 3.682307, mean_eps: 0.162759\n",
      " 2327374/3750000: episode: 3168, duration: 8.958s, episode steps: 1210, steps per second: 135, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.012021, mae: 3.082407, mean_q: 3.710120, mean_eps: 0.162363\n",
      " 2327761/3750000: episode: 3169, duration: 3.041s, episode steps: 387, steps per second: 127, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.014066, mae: 3.105030, mean_q: 3.738973, mean_eps: 0.162075\n",
      " 2328471/3750000: episode: 3170, duration: 5.249s, episode steps: 710, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.232 [0.000, 5.000],  loss: 0.017626, mae: 3.123752, mean_q: 3.761924, mean_eps: 0.161877\n",
      " 2329345/3750000: episode: 3171, duration: 6.558s, episode steps: 874, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.018193, mae: 3.098867, mean_q: 3.730454, mean_eps: 0.161592\n",
      " 2329932/3750000: episode: 3172, duration: 4.406s, episode steps: 587, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.670 [0.000, 5.000],  loss: 0.012157, mae: 3.016354, mean_q: 3.631035, mean_eps: 0.161330\n",
      " 2330737/3750000: episode: 3173, duration: 5.968s, episode steps: 805, steps per second: 135, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.013837, mae: 3.122051, mean_q: 3.762195, mean_eps: 0.161081\n",
      " 2331458/3750000: episode: 3174, duration: 5.392s, episode steps: 721, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.046 [0.000, 5.000],  loss: 0.016933, mae: 3.073983, mean_q: 3.699988, mean_eps: 0.160808\n",
      " 2332411/3750000: episode: 3175, duration: 7.192s, episode steps: 953, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.014417, mae: 3.119525, mean_q: 3.750337, mean_eps: 0.160505\n",
      " 2332925/3750000: episode: 3176, duration: 3.849s, episode steps: 514, steps per second: 134, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.940 [0.000, 5.000],  loss: 0.011126, mae: 3.112829, mean_q: 3.743026, mean_eps: 0.160239\n",
      " 2333917/3750000: episode: 3177, duration: 7.296s, episode steps: 992, steps per second: 136, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.015244, mae: 3.061199, mean_q: 3.681692, mean_eps: 0.159969\n",
      " 2334736/3750000: episode: 3178, duration: 6.386s, episode steps: 819, steps per second: 128, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.016135, mae: 3.055121, mean_q: 3.678064, mean_eps: 0.159645\n",
      " 2335357/3750000: episode: 3179, duration: 4.623s, episode steps: 621, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.010781, mae: 3.064892, mean_q: 3.687939, mean_eps: 0.159386\n",
      " 2335853/3750000: episode: 3180, duration: 3.731s, episode steps: 496, steps per second: 133, episode reward: 14.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.013786, mae: 3.086427, mean_q: 3.709207, mean_eps: 0.159184\n",
      " 2336871/3750000: episode: 3181, duration: 7.675s, episode steps: 1018, steps per second: 133, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.548 [0.000, 5.000],  loss: 0.014231, mae: 3.078411, mean_q: 3.700451, mean_eps: 0.158910\n",
      " 2337979/3750000: episode: 3182, duration: 8.203s, episode steps: 1108, steps per second: 135, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.012728, mae: 3.114742, mean_q: 3.748867, mean_eps: 0.158529\n",
      " 2338862/3750000: episode: 3183, duration: 6.712s, episode steps: 883, steps per second: 132, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.602 [0.000, 5.000],  loss: 0.013614, mae: 3.104061, mean_q: 3.737552, mean_eps: 0.158169\n",
      " 2339474/3750000: episode: 3184, duration: 4.501s, episode steps: 612, steps per second: 136, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.016376, mae: 3.148296, mean_q: 3.783969, mean_eps: 0.157899\n",
      " 2340236/3750000: episode: 3185, duration: 5.652s, episode steps: 762, steps per second: 135, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.064 [0.000, 5.000],  loss: 0.018223, mae: 3.111869, mean_q: 3.747850, mean_eps: 0.157654\n",
      " 2341288/3750000: episode: 3186, duration: 7.905s, episode steps: 1052, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.017076, mae: 3.133416, mean_q: 3.770774, mean_eps: 0.157326\n",
      " 2342095/3750000: episode: 3187, duration: 6.058s, episode steps: 807, steps per second: 133, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.015730, mae: 3.110159, mean_q: 3.745783, mean_eps: 0.156992\n",
      " 2342827/3750000: episode: 3188, duration: 5.511s, episode steps: 732, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.014795, mae: 3.099571, mean_q: 3.728247, mean_eps: 0.156714\n",
      " 2343458/3750000: episode: 3189, duration: 4.671s, episode steps: 631, steps per second: 135, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.013417, mae: 3.190767, mean_q: 3.840860, mean_eps: 0.156470\n",
      " 2344117/3750000: episode: 3190, duration: 4.902s, episode steps: 659, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.010389, mae: 3.079515, mean_q: 3.702791, mean_eps: 0.156239\n",
      " 2344652/3750000: episode: 3191, duration: 4.102s, episode steps: 535, steps per second: 130, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.013100, mae: 3.049873, mean_q: 3.675024, mean_eps: 0.156023\n",
      " 2345339/3750000: episode: 3192, duration: 5.234s, episode steps: 687, steps per second: 131, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.011015, mae: 3.140309, mean_q: 3.781948, mean_eps: 0.155804\n",
      " 2345820/3750000: episode: 3193, duration: 3.688s, episode steps: 481, steps per second: 130, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.012237, mae: 3.087732, mean_q: 3.715060, mean_eps: 0.155595\n",
      " 2346588/3750000: episode: 3194, duration: 5.785s, episode steps: 768, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 0.014787, mae: 3.165262, mean_q: 3.806817, mean_eps: 0.155368\n",
      " 2347492/3750000: episode: 3195, duration: 6.742s, episode steps: 904, steps per second: 134, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.020492, mae: 3.124174, mean_q: 3.756578, mean_eps: 0.155066\n",
      " 2348385/3750000: episode: 3196, duration: 6.693s, episode steps: 893, steps per second: 133, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.015200, mae: 3.118531, mean_q: 3.758186, mean_eps: 0.154742\n",
      " 2349115/3750000: episode: 3197, duration: 5.472s, episode steps: 730, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.581 [0.000, 5.000],  loss: 0.017723, mae: 3.151684, mean_q: 3.789926, mean_eps: 0.154450\n",
      " 2349859/3750000: episode: 3198, duration: 5.498s, episode steps: 744, steps per second: 135, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.015899, mae: 3.154495, mean_q: 3.791148, mean_eps: 0.154187\n",
      " 2350634/3750000: episode: 3199, duration: 5.815s, episode steps: 775, steps per second: 133, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.297 [0.000, 5.000],  loss: 0.019175, mae: 3.149947, mean_q: 3.786583, mean_eps: 0.153914\n",
      " 2351534/3750000: episode: 3200, duration: 6.794s, episode steps: 900, steps per second: 132, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.161 [0.000, 5.000],  loss: 0.014829, mae: 3.181413, mean_q: 3.833912, mean_eps: 0.153611\n",
      " 2352297/3750000: episode: 3201, duration: 5.766s, episode steps: 763, steps per second: 132, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.039 [0.000, 5.000],  loss: 0.014222, mae: 3.221574, mean_q: 3.872957, mean_eps: 0.153312\n",
      " 2353509/3750000: episode: 3202, duration: 9.129s, episode steps: 1212, steps per second: 133, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014969, mae: 3.162337, mean_q: 3.799379, mean_eps: 0.152956\n",
      " 2354240/3750000: episode: 3203, duration: 5.495s, episode steps: 731, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.501 [0.000, 5.000],  loss: 0.015229, mae: 3.203377, mean_q: 3.854489, mean_eps: 0.152607\n",
      " 2354959/3750000: episode: 3204, duration: 5.350s, episode steps: 719, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.469 [0.000, 5.000],  loss: 0.019500, mae: 3.190859, mean_q: 3.841008, mean_eps: 0.152348\n",
      " 2355469/3750000: episode: 3205, duration: 3.938s, episode steps: 510, steps per second: 129, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.980 [0.000, 5.000],  loss: 0.015465, mae: 3.250387, mean_q: 3.919674, mean_eps: 0.152124\n",
      " 2356330/3750000: episode: 3206, duration: 6.456s, episode steps: 861, steps per second: 133, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.383 [0.000, 5.000],  loss: 0.014768, mae: 3.223784, mean_q: 3.878538, mean_eps: 0.151876\n",
      " 2356773/3750000: episode: 3207, duration: 3.312s, episode steps: 443, steps per second: 134, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.018746, mae: 3.189402, mean_q: 3.837986, mean_eps: 0.151642\n",
      " 2357393/3750000: episode: 3208, duration: 4.621s, episode steps: 620, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.685 [0.000, 5.000],  loss: 0.012184, mae: 3.165490, mean_q: 3.807539, mean_eps: 0.151451\n",
      " 2357882/3750000: episode: 3209, duration: 3.750s, episode steps: 489, steps per second: 130, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.933 [0.000, 5.000],  loss: 0.018375, mae: 3.189001, mean_q: 3.835559, mean_eps: 0.151250\n",
      " 2358588/3750000: episode: 3210, duration: 5.245s, episode steps: 706, steps per second: 135, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.012297, mae: 3.126911, mean_q: 3.760808, mean_eps: 0.151034\n",
      " 2359130/3750000: episode: 3211, duration: 4.006s, episode steps: 542, steps per second: 135, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.014368, mae: 3.191110, mean_q: 3.838164, mean_eps: 0.150810\n",
      " 2359948/3750000: episode: 3212, duration: 6.148s, episode steps: 818, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.351 [0.000, 5.000],  loss: 0.017298, mae: 3.201185, mean_q: 3.848607, mean_eps: 0.150566\n",
      " 2360982/3750000: episode: 3213, duration: 7.722s, episode steps: 1034, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.016313, mae: 3.145223, mean_q: 3.782428, mean_eps: 0.150231\n",
      " 2361874/3750000: episode: 3214, duration: 6.636s, episode steps: 892, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.053 [0.000, 5.000],  loss: 0.014607, mae: 3.192554, mean_q: 3.843883, mean_eps: 0.149885\n",
      " 2362578/3750000: episode: 3215, duration: 5.234s, episode steps: 704, steps per second: 134, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.017409, mae: 3.181174, mean_q: 3.822592, mean_eps: 0.149601\n",
      " 2363488/3750000: episode: 3216, duration: 6.808s, episode steps: 910, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.019916, mae: 3.186784, mean_q: 3.836993, mean_eps: 0.149309\n",
      " 2364537/3750000: episode: 3217, duration: 7.800s, episode steps: 1049, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.085 [0.000, 5.000],  loss: 0.017998, mae: 3.208191, mean_q: 3.853535, mean_eps: 0.148956\n",
      " 2365875/3750000: episode: 3218, duration: 9.934s, episode steps: 1338, steps per second: 135, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.019295, mae: 3.206280, mean_q: 3.854123, mean_eps: 0.148528\n",
      " 2366513/3750000: episode: 3219, duration: 4.865s, episode steps: 638, steps per second: 131, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.012222, mae: 3.149544, mean_q: 3.789586, mean_eps: 0.148172\n",
      " 2367755/3750000: episode: 3220, duration: 9.213s, episode steps: 1242, steps per second: 135, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.606 [0.000, 5.000],  loss: 0.014800, mae: 3.183557, mean_q: 3.827346, mean_eps: 0.147833\n",
      " 2368691/3750000: episode: 3221, duration: 7.110s, episode steps: 936, steps per second: 132, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.893 [0.000, 5.000],  loss: 0.020025, mae: 3.219433, mean_q: 3.872842, mean_eps: 0.147441\n",
      " 2369620/3750000: episode: 3222, duration: 6.984s, episode steps: 929, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.718 [0.000, 5.000],  loss: 0.014892, mae: 3.235998, mean_q: 3.897697, mean_eps: 0.147106\n",
      " 2369993/3750000: episode: 3223, duration: 2.840s, episode steps: 373, steps per second: 131, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.893 [0.000, 5.000],  loss: 0.016010, mae: 3.228967, mean_q: 3.882182, mean_eps: 0.146872\n",
      " 2370690/3750000: episode: 3224, duration: 5.266s, episode steps: 697, steps per second: 132, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.014815, mae: 3.242426, mean_q: 3.900987, mean_eps: 0.146678\n",
      " 2371680/3750000: episode: 3225, duration: 7.382s, episode steps: 990, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.018371, mae: 3.219166, mean_q: 3.868653, mean_eps: 0.146375\n",
      " 2372743/3750000: episode: 3226, duration: 8.070s, episode steps: 1063, steps per second: 132, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.541 [0.000, 5.000],  loss: 0.014336, mae: 3.182250, mean_q: 3.826675, mean_eps: 0.146004\n",
      " 2373137/3750000: episode: 3227, duration: 2.890s, episode steps: 394, steps per second: 136, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.014174, mae: 3.225256, mean_q: 3.888697, mean_eps: 0.145742\n",
      " 2373997/3750000: episode: 3228, duration: 6.361s, episode steps: 860, steps per second: 135, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.773 [0.000, 5.000],  loss: 0.015498, mae: 3.174089, mean_q: 3.820643, mean_eps: 0.145518\n",
      " 2374537/3750000: episode: 3229, duration: 4.048s, episode steps: 540, steps per second: 133, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013500, mae: 3.197185, mean_q: 3.846168, mean_eps: 0.145266\n",
      " 2375084/3750000: episode: 3230, duration: 4.145s, episode steps: 547, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.012878, mae: 3.237615, mean_q: 3.902069, mean_eps: 0.145068\n",
      " 2376410/3750000: episode: 3231, duration: 9.806s, episode steps: 1326, steps per second: 135, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.015953, mae: 3.177425, mean_q: 3.822343, mean_eps: 0.144730\n",
      " 2376876/3750000: episode: 3232, duration: 3.576s, episode steps: 466, steps per second: 130, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.356 [0.000, 5.000],  loss: 0.012041, mae: 3.167694, mean_q: 3.821048, mean_eps: 0.144410\n",
      " 2377790/3750000: episode: 3233, duration: 6.821s, episode steps: 914, steps per second: 134, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.410 [0.000, 5.000],  loss: 0.022455, mae: 3.201255, mean_q: 3.856296, mean_eps: 0.144161\n",
      " 2378271/3750000: episode: 3234, duration: 3.588s, episode steps: 481, steps per second: 134, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.014993, mae: 3.148013, mean_q: 3.801086, mean_eps: 0.143909\n",
      " 2378961/3750000: episode: 3235, duration: 5.227s, episode steps: 690, steps per second: 132, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.013694, mae: 3.176451, mean_q: 3.825621, mean_eps: 0.143697\n",
      " 2380293/3750000: episode: 3236, duration: 9.831s, episode steps: 1332, steps per second: 135, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.842 [0.000, 5.000],  loss: 0.013737, mae: 3.192164, mean_q: 3.840408, mean_eps: 0.143333\n",
      " 2380950/3750000: episode: 3237, duration: 4.966s, episode steps: 657, steps per second: 132, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.393 [0.000, 5.000],  loss: 0.012962, mae: 3.168646, mean_q: 3.818214, mean_eps: 0.142977\n",
      " 2381428/3750000: episode: 3238, duration: 3.604s, episode steps: 478, steps per second: 133, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.015787, mae: 3.135256, mean_q: 3.770161, mean_eps: 0.142772\n",
      " 2382540/3750000: episode: 3239, duration: 8.240s, episode steps: 1112, steps per second: 135, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.015287, mae: 3.124391, mean_q: 3.767338, mean_eps: 0.142487\n",
      " 2383221/3750000: episode: 3240, duration: 5.204s, episode steps: 681, steps per second: 131, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.016961, mae: 3.110521, mean_q: 3.741184, mean_eps: 0.142163\n",
      " 2384063/3750000: episode: 3241, duration: 6.305s, episode steps: 842, steps per second: 134, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.690 [0.000, 5.000],  loss: 0.015875, mae: 3.117573, mean_q: 3.750418, mean_eps: 0.141886\n",
      " 2384444/3750000: episode: 3242, duration: 2.873s, episode steps: 381, steps per second: 133, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.017108, mae: 3.120358, mean_q: 3.750478, mean_eps: 0.141666\n",
      " 2385201/3750000: episode: 3243, duration: 5.651s, episode steps: 757, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.284 [0.000, 5.000],  loss: 0.013459, mae: 3.211452, mean_q: 3.862972, mean_eps: 0.141461\n",
      " 2385867/3750000: episode: 3244, duration: 4.986s, episode steps: 666, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.019604, mae: 3.169449, mean_q: 3.811261, mean_eps: 0.141206\n",
      " 2386766/3750000: episode: 3245, duration: 6.696s, episode steps: 899, steps per second: 134, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.014275, mae: 3.164592, mean_q: 3.811324, mean_eps: 0.140925\n",
      " 2387138/3750000: episode: 3246, duration: 2.713s, episode steps: 372, steps per second: 137, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 0.978 [0.000, 5.000],  loss: 0.011038, mae: 3.161511, mean_q: 3.808017, mean_eps: 0.140698\n",
      " 2387857/3750000: episode: 3247, duration: 5.392s, episode steps: 719, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.020028, mae: 3.135488, mean_q: 3.771571, mean_eps: 0.140504\n",
      " 2388199/3750000: episode: 3248, duration: 2.549s, episode steps: 342, steps per second: 134, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.120 [0.000, 5.000],  loss: 0.011965, mae: 3.252293, mean_q: 3.917397, mean_eps: 0.140313\n",
      " 2388879/3750000: episode: 3249, duration: 5.052s, episode steps: 680, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.603 [0.000, 5.000],  loss: 0.015215, mae: 3.220576, mean_q: 3.872376, mean_eps: 0.140129\n",
      " 2389231/3750000: episode: 3250, duration: 2.677s, episode steps: 352, steps per second: 132, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.892 [0.000, 5.000],  loss: 0.017582, mae: 3.176095, mean_q: 3.826544, mean_eps: 0.139942\n",
      " 2389837/3750000: episode: 3251, duration: 4.530s, episode steps: 606, steps per second: 134, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.365 [0.000, 5.000],  loss: 0.019722, mae: 3.161117, mean_q: 3.808383, mean_eps: 0.139769\n",
      " 2390702/3750000: episode: 3252, duration: 6.488s, episode steps: 865, steps per second: 133, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.012809, mae: 3.240087, mean_q: 3.899661, mean_eps: 0.139503\n",
      " 2391216/3750000: episode: 3253, duration: 3.812s, episode steps: 514, steps per second: 135, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.457 [0.000, 5.000],  loss: 0.015003, mae: 3.187756, mean_q: 3.835977, mean_eps: 0.139254\n",
      " 2392361/3750000: episode: 3254, duration: 8.563s, episode steps: 1145, steps per second: 134, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.017618, mae: 3.201436, mean_q: 3.851493, mean_eps: 0.138956\n",
      " 2393147/3750000: episode: 3255, duration: 5.816s, episode steps: 786, steps per second: 135, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.682 [0.000, 5.000],  loss: 0.019221, mae: 3.201919, mean_q: 3.846516, mean_eps: 0.138606\n",
      " 2394615/3750000: episode: 3256, duration: 10.927s, episode steps: 1468, steps per second: 134, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.014428, mae: 3.207575, mean_q: 3.859825, mean_eps: 0.138203\n",
      " 2395562/3750000: episode: 3257, duration: 7.103s, episode steps: 947, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.017941, mae: 3.205330, mean_q: 3.854128, mean_eps: 0.137768\n",
      " 2396607/3750000: episode: 3258, duration: 7.805s, episode steps: 1045, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.016586, mae: 3.181584, mean_q: 3.826398, mean_eps: 0.137408\n",
      " 2397268/3750000: episode: 3259, duration: 4.940s, episode steps: 661, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.014707, mae: 3.155381, mean_q: 3.795057, mean_eps: 0.137102\n",
      " 2398115/3750000: episode: 3260, duration: 6.285s, episode steps: 847, steps per second: 135, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.017378, mae: 3.192418, mean_q: 3.838788, mean_eps: 0.136832\n",
      " 2398660/3750000: episode: 3261, duration: 4.185s, episode steps: 545, steps per second: 130, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.019629, mae: 3.178504, mean_q: 3.828481, mean_eps: 0.136583\n",
      " 2399410/3750000: episode: 3262, duration: 5.604s, episode steps: 750, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.019075, mae: 3.161099, mean_q: 3.802613, mean_eps: 0.136349\n",
      " 2400095/3750000: episode: 3263, duration: 5.069s, episode steps: 685, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.101 [0.000, 5.000],  loss: 0.015744, mae: 3.176563, mean_q: 3.821422, mean_eps: 0.136090\n",
      " 2400910/3750000: episode: 3264, duration: 6.212s, episode steps: 815, steps per second: 131, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.011722, mae: 3.190201, mean_q: 3.842912, mean_eps: 0.135820\n",
      " 2401777/3750000: episode: 3265, duration: 6.432s, episode steps: 867, steps per second: 135, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.264 [0.000, 5.000],  loss: 0.020893, mae: 3.168936, mean_q: 3.817939, mean_eps: 0.135518\n",
      " 2402798/3750000: episode: 3266, duration: 7.643s, episode steps: 1021, steps per second: 134, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.014493, mae: 3.208407, mean_q: 3.861055, mean_eps: 0.135179\n",
      " 2403752/3750000: episode: 3267, duration: 7.120s, episode steps: 954, steps per second: 134, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.014568, mae: 3.224529, mean_q: 3.881085, mean_eps: 0.134823\n",
      " 2404568/3750000: episode: 3268, duration: 6.160s, episode steps: 816, steps per second: 132, episode reward:  9.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.018889, mae: 3.202975, mean_q: 3.864580, mean_eps: 0.134502\n",
      " 2405187/3750000: episode: 3269, duration: 4.633s, episode steps: 619, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.208 [0.000, 5.000],  loss: 0.018512, mae: 3.154276, mean_q: 3.795125, mean_eps: 0.134243\n",
      " 2405877/3750000: episode: 3270, duration: 5.094s, episode steps: 690, steps per second: 135, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.032205, mae: 3.158163, mean_q: 3.796207, mean_eps: 0.134009\n",
      " 2406675/3750000: episode: 3271, duration: 5.954s, episode steps: 798, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.013906, mae: 3.092984, mean_q: 3.723123, mean_eps: 0.133743\n",
      " 2407625/3750000: episode: 3272, duration: 7.178s, episode steps: 950, steps per second: 132, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.017292, mae: 3.183933, mean_q: 3.827686, mean_eps: 0.133426\n",
      " 2408423/3750000: episode: 3273, duration: 5.961s, episode steps: 798, steps per second: 134, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.017182, mae: 3.150158, mean_q: 3.791974, mean_eps: 0.133109\n",
      " 2409209/3750000: episode: 3274, duration: 5.915s, episode steps: 786, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.632 [0.000, 5.000],  loss: 0.018646, mae: 3.163855, mean_q: 3.814078, mean_eps: 0.132825\n",
      " 2409793/3750000: episode: 3275, duration: 4.333s, episode steps: 584, steps per second: 135, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.092 [0.000, 5.000],  loss: 0.016251, mae: 3.160470, mean_q: 3.807800, mean_eps: 0.132580\n",
      " 2410149/3750000: episode: 3276, duration: 2.708s, episode steps: 356, steps per second: 131, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.461 [0.000, 5.000],  loss: 0.022166, mae: 3.118938, mean_q: 3.753027, mean_eps: 0.132411\n",
      " 2410858/3750000: episode: 3277, duration: 5.286s, episode steps: 709, steps per second: 134, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.019845, mae: 3.150323, mean_q: 3.790577, mean_eps: 0.132220\n",
      " 2411727/3750000: episode: 3278, duration: 6.540s, episode steps: 869, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.017961, mae: 3.126200, mean_q: 3.760277, mean_eps: 0.131936\n",
      " 2412632/3750000: episode: 3279, duration: 6.722s, episode steps: 905, steps per second: 135, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 0.014130, mae: 3.162549, mean_q: 3.807292, mean_eps: 0.131615\n",
      " 2413597/3750000: episode: 3280, duration: 7.220s, episode steps: 965, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.014326, mae: 3.140298, mean_q: 3.776861, mean_eps: 0.131280\n",
      " 2414314/3750000: episode: 3281, duration: 5.338s, episode steps: 717, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.678 [0.000, 5.000],  loss: 0.013367, mae: 3.124390, mean_q: 3.762955, mean_eps: 0.130978\n",
      " 2415209/3750000: episode: 3282, duration: 6.705s, episode steps: 895, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.344 [0.000, 5.000],  loss: 0.016547, mae: 3.154221, mean_q: 3.793176, mean_eps: 0.130686\n",
      " 2416134/3750000: episode: 3283, duration: 6.910s, episode steps: 925, steps per second: 134, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.888 [0.000, 5.000],  loss: 0.015446, mae: 3.114383, mean_q: 3.745467, mean_eps: 0.130359\n",
      " 2417530/3750000: episode: 3284, duration: 10.526s, episode steps: 1396, steps per second: 133, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.011895, mae: 3.138306, mean_q: 3.774748, mean_eps: 0.129941\n",
      " 2418191/3750000: episode: 3285, duration: 4.937s, episode steps: 661, steps per second: 134, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.015461, mae: 3.136111, mean_q: 3.773040, mean_eps: 0.129570\n",
      " 2418904/3750000: episode: 3286, duration: 5.352s, episode steps: 713, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.018338, mae: 3.048858, mean_q: 3.665799, mean_eps: 0.129322\n",
      " 2419569/3750000: episode: 3287, duration: 4.996s, episode steps: 665, steps per second: 133, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.017264, mae: 3.129544, mean_q: 3.761289, mean_eps: 0.129074\n",
      " 2419942/3750000: episode: 3288, duration: 2.831s, episode steps: 373, steps per second: 132, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.013708, mae: 3.134694, mean_q: 3.765613, mean_eps: 0.128886\n",
      " 2420787/3750000: episode: 3289, duration: 6.253s, episode steps: 845, steps per second: 135, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.080 [0.000, 5.000],  loss: 0.015687, mae: 3.075008, mean_q: 3.696989, mean_eps: 0.128667\n",
      " 2421127/3750000: episode: 3290, duration: 2.547s, episode steps: 340, steps per second: 133, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.022535, mae: 3.113385, mean_q: 3.743603, mean_eps: 0.128454\n",
      " 2421800/3750000: episode: 3291, duration: 5.080s, episode steps: 673, steps per second: 132, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.014883, mae: 3.109405, mean_q: 3.744020, mean_eps: 0.128274\n",
      " 2422645/3750000: episode: 3292, duration: 6.347s, episode steps: 845, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.013552, mae: 3.158322, mean_q: 3.800263, mean_eps: 0.128001\n",
      " 2423672/3750000: episode: 3293, duration: 7.626s, episode steps: 1027, steps per second: 135, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.012087, mae: 3.059190, mean_q: 3.677090, mean_eps: 0.127662\n",
      " 2424528/3750000: episode: 3294, duration: 6.426s, episode steps: 856, steps per second: 133, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.102 [0.000, 5.000],  loss: 0.010863, mae: 3.092288, mean_q: 3.718016, mean_eps: 0.127324\n",
      " 2425365/3750000: episode: 3295, duration: 6.212s, episode steps: 837, steps per second: 135, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.017367, mae: 3.110033, mean_q: 3.741055, mean_eps: 0.127018\n",
      " 2426035/3750000: episode: 3296, duration: 4.993s, episode steps: 670, steps per second: 134, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.927 [0.000, 5.000],  loss: 0.015896, mae: 3.120479, mean_q: 3.755913, mean_eps: 0.126748\n",
      " 2426747/3750000: episode: 3297, duration: 5.310s, episode steps: 712, steps per second: 134, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.971 [0.000, 5.000],  loss: 0.019451, mae: 3.112578, mean_q: 3.738714, mean_eps: 0.126500\n",
      " 2427884/3750000: episode: 3298, duration: 8.444s, episode steps: 1137, steps per second: 135, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.417 [0.000, 5.000],  loss: 0.015930, mae: 3.074221, mean_q: 3.702212, mean_eps: 0.126165\n",
      " 2428427/3750000: episode: 3299, duration: 4.108s, episode steps: 543, steps per second: 132, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.295 [0.000, 5.000],  loss: 0.017113, mae: 3.100282, mean_q: 3.733877, mean_eps: 0.125862\n",
      " 2429335/3750000: episode: 3300, duration: 6.716s, episode steps: 908, steps per second: 135, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.015287, mae: 3.121315, mean_q: 3.762001, mean_eps: 0.125603\n",
      " 2430044/3750000: episode: 3301, duration: 5.363s, episode steps: 709, steps per second: 132, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.015526, mae: 3.110788, mean_q: 3.746084, mean_eps: 0.125312\n",
      " 2430902/3750000: episode: 3302, duration: 6.499s, episode steps: 858, steps per second: 132, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.431 [0.000, 5.000],  loss: 0.015307, mae: 3.119994, mean_q: 3.754102, mean_eps: 0.125027\n",
      " 2431260/3750000: episode: 3303, duration: 2.711s, episode steps: 358, steps per second: 132, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.966 [0.000, 5.000],  loss: 0.014478, mae: 3.105520, mean_q: 3.737642, mean_eps: 0.124811\n",
      " 2432131/3750000: episode: 3304, duration: 6.589s, episode steps: 871, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.015917, mae: 3.101705, mean_q: 3.731529, mean_eps: 0.124592\n",
      " 2432685/3750000: episode: 3305, duration: 4.278s, episode steps: 554, steps per second: 129, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.020491, mae: 3.091040, mean_q: 3.729731, mean_eps: 0.124332\n",
      " 2433696/3750000: episode: 3306, duration: 7.493s, episode steps: 1011, steps per second: 135, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.014154, mae: 3.107405, mean_q: 3.741219, mean_eps: 0.124052\n",
      " 2434313/3750000: episode: 3307, duration: 4.621s, episode steps: 617, steps per second: 134, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.235 [0.000, 5.000],  loss: 0.012818, mae: 3.138496, mean_q: 3.777576, mean_eps: 0.123760\n",
      " 2434814/3750000: episode: 3308, duration: 3.811s, episode steps: 501, steps per second: 131, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.006 [0.000, 5.000],  loss: 0.020328, mae: 3.154048, mean_q: 3.794126, mean_eps: 0.123558\n",
      " 2435607/3750000: episode: 3309, duration: 5.945s, episode steps: 793, steps per second: 133, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.252 [0.000, 5.000],  loss: 0.011686, mae: 3.119632, mean_q: 3.756299, mean_eps: 0.123324\n",
      " 2436408/3750000: episode: 3310, duration: 5.999s, episode steps: 801, steps per second: 134, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.011676, mae: 3.102871, mean_q: 3.736766, mean_eps: 0.123036\n",
      " 2437352/3750000: episode: 3311, duration: 7.103s, episode steps: 944, steps per second: 133, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.014317, mae: 3.107819, mean_q: 3.736864, mean_eps: 0.122723\n",
      " 2438186/3750000: episode: 3312, duration: 6.259s, episode steps: 834, steps per second: 133, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.269 [0.000, 5.000],  loss: 0.017571, mae: 3.126871, mean_q: 3.763825, mean_eps: 0.122403\n",
      " 2438755/3750000: episode: 3313, duration: 4.227s, episode steps: 569, steps per second: 135, episode reward: 18.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.017749, mae: 3.091297, mean_q: 3.725971, mean_eps: 0.122151\n",
      " 2439610/3750000: episode: 3314, duration: 6.414s, episode steps: 855, steps per second: 133, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.016100, mae: 3.134524, mean_q: 3.776813, mean_eps: 0.121895\n",
      " 2440230/3750000: episode: 3315, duration: 4.593s, episode steps: 620, steps per second: 135, episode reward: 20.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 3.106 [0.000, 5.000],  loss: 0.014146, mae: 3.149845, mean_q: 3.791609, mean_eps: 0.121629\n",
      " 2440955/3750000: episode: 3316, duration: 5.410s, episode steps: 725, steps per second: 134, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.013526, mae: 3.139789, mean_q: 3.774799, mean_eps: 0.121388\n",
      " 2441955/3750000: episode: 3317, duration: 7.420s, episode steps: 1000, steps per second: 135, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.019270, mae: 3.154797, mean_q: 3.793621, mean_eps: 0.121078\n",
      " 2442949/3750000: episode: 3318, duration: 7.489s, episode steps: 994, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.013994, mae: 3.135916, mean_q: 3.770317, mean_eps: 0.120718\n",
      " 2443547/3750000: episode: 3319, duration: 4.525s, episode steps: 598, steps per second: 132, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.014091, mae: 3.177776, mean_q: 3.817918, mean_eps: 0.120430\n",
      " 2444368/3750000: episode: 3320, duration: 6.098s, episode steps: 821, steps per second: 135, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.014892, mae: 3.133146, mean_q: 3.769147, mean_eps: 0.120174\n",
      " 2445311/3750000: episode: 3321, duration: 7.037s, episode steps: 943, steps per second: 134, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.210 [0.000, 5.000],  loss: 0.017106, mae: 3.129020, mean_q: 3.767226, mean_eps: 0.119858\n",
      " 2446048/3750000: episode: 3322, duration: 5.504s, episode steps: 737, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.018194, mae: 3.182051, mean_q: 3.823594, mean_eps: 0.119555\n",
      " 2446732/3750000: episode: 3323, duration: 5.078s, episode steps: 684, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.784 [0.000, 5.000],  loss: 0.023525, mae: 3.144312, mean_q: 3.780532, mean_eps: 0.119300\n",
      " 2447092/3750000: episode: 3324, duration: 2.707s, episode steps: 360, steps per second: 133, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.019674, mae: 3.157292, mean_q: 3.794880, mean_eps: 0.119112\n",
      " 2448352/3750000: episode: 3325, duration: 9.498s, episode steps: 1260, steps per second: 133, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 0.016733, mae: 3.139985, mean_q: 3.774750, mean_eps: 0.118821\n",
      " 2449146/3750000: episode: 3326, duration: 6.053s, episode steps: 794, steps per second: 131, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.368 [0.000, 5.000],  loss: 0.018074, mae: 3.151440, mean_q: 3.792389, mean_eps: 0.118450\n",
      " 2450372/3750000: episode: 3327, duration: 9.158s, episode steps: 1226, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.015320, mae: 3.137699, mean_q: 3.775847, mean_eps: 0.118086\n",
      " 2450838/3750000: episode: 3328, duration: 3.464s, episode steps: 466, steps per second: 135, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.940 [0.000, 5.000],  loss: 0.010511, mae: 3.165972, mean_q: 3.811964, mean_eps: 0.117784\n",
      " 2451533/3750000: episode: 3329, duration: 5.262s, episode steps: 695, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.240 [0.000, 5.000],  loss: 0.013382, mae: 3.153390, mean_q: 3.795396, mean_eps: 0.117575\n",
      " 2452497/3750000: episode: 3330, duration: 7.230s, episode steps: 964, steps per second: 133, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.018096, mae: 3.189345, mean_q: 3.834566, mean_eps: 0.117276\n",
      " 2452991/3750000: episode: 3331, duration: 3.719s, episode steps: 494, steps per second: 133, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.014409, mae: 3.203588, mean_q: 3.853914, mean_eps: 0.117014\n",
      " 2453621/3750000: episode: 3332, duration: 4.765s, episode steps: 630, steps per second: 132, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.016667, mae: 3.124007, mean_q: 3.758012, mean_eps: 0.116808\n",
      " 2454263/3750000: episode: 3333, duration: 4.820s, episode steps: 642, steps per second: 133, episode reward: 23.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 3.417 [0.000, 5.000],  loss: 0.019194, mae: 3.156116, mean_q: 3.804268, mean_eps: 0.116578\n",
      " 2454779/3750000: episode: 3334, duration: 3.768s, episode steps: 516, steps per second: 137, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.021715, mae: 3.138338, mean_q: 3.776288, mean_eps: 0.116373\n",
      " 2455511/3750000: episode: 3335, duration: 5.485s, episode steps: 732, steps per second: 133, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.616 [0.000, 5.000],  loss: 0.014951, mae: 3.183559, mean_q: 3.834110, mean_eps: 0.116150\n",
      " 2455932/3750000: episode: 3336, duration: 3.232s, episode steps: 421, steps per second: 130, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.017044, mae: 3.157185, mean_q: 3.800388, mean_eps: 0.115941\n",
      " 2456434/3750000: episode: 3337, duration: 3.743s, episode steps: 502, steps per second: 134, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.015195, mae: 3.136694, mean_q: 3.771019, mean_eps: 0.115775\n",
      " 2457077/3750000: episode: 3338, duration: 4.786s, episode steps: 643, steps per second: 134, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.194 [0.000, 5.000],  loss: 0.012169, mae: 3.137432, mean_q: 3.777943, mean_eps: 0.115570\n",
      " 2457816/3750000: episode: 3339, duration: 5.521s, episode steps: 739, steps per second: 134, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.015928, mae: 3.164794, mean_q: 3.806438, mean_eps: 0.115322\n",
      " 2459268/3750000: episode: 3340, duration: 10.888s, episode steps: 1452, steps per second: 133, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.849 [0.000, 5.000],  loss: 0.016169, mae: 3.126063, mean_q: 3.761962, mean_eps: 0.114926\n",
      " 2460098/3750000: episode: 3341, duration: 6.181s, episode steps: 830, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.016513, mae: 3.136807, mean_q: 3.779779, mean_eps: 0.114515\n",
      " 2460872/3750000: episode: 3342, duration: 5.851s, episode steps: 774, steps per second: 132, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.014039, mae: 3.143532, mean_q: 3.784895, mean_eps: 0.114227\n",
      " 2462207/3750000: episode: 3343, duration: 10.080s, episode steps: 1335, steps per second: 132, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.014751, mae: 3.119592, mean_q: 3.752324, mean_eps: 0.113846\n",
      " 2463048/3750000: episode: 3344, duration: 6.422s, episode steps: 841, steps per second: 131, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.012021, mae: 3.073238, mean_q: 3.701802, mean_eps: 0.113453\n",
      " 2463730/3750000: episode: 3345, duration: 5.102s, episode steps: 682, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.617 [0.000, 5.000],  loss: 0.014577, mae: 3.081052, mean_q: 3.707187, mean_eps: 0.113180\n",
      " 2464489/3750000: episode: 3346, duration: 5.786s, episode steps: 759, steps per second: 131, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.014441, mae: 3.112054, mean_q: 3.742233, mean_eps: 0.112920\n",
      " 2465386/3750000: episode: 3347, duration: 6.738s, episode steps: 897, steps per second: 133, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.014483, mae: 3.137701, mean_q: 3.774129, mean_eps: 0.112622\n",
      " 2465920/3750000: episode: 3348, duration: 4.011s, episode steps: 534, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.612 [0.000, 5.000],  loss: 0.022066, mae: 3.120908, mean_q: 3.758137, mean_eps: 0.112366\n",
      " 2466905/3750000: episode: 3349, duration: 7.482s, episode steps: 985, steps per second: 132, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.509 [0.000, 5.000],  loss: 0.018583, mae: 3.144997, mean_q: 3.785112, mean_eps: 0.112092\n",
      " 2467395/3750000: episode: 3350, duration: 3.619s, episode steps: 490, steps per second: 135, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.014615, mae: 3.120536, mean_q: 3.750929, mean_eps: 0.111826\n",
      " 2467928/3750000: episode: 3351, duration: 4.000s, episode steps: 533, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.388 [0.000, 5.000],  loss: 0.012521, mae: 3.114541, mean_q: 3.746191, mean_eps: 0.111642\n",
      " 2468703/3750000: episode: 3352, duration: 5.852s, episode steps: 775, steps per second: 132, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.015785, mae: 3.127081, mean_q: 3.761465, mean_eps: 0.111405\n",
      " 2469731/3750000: episode: 3353, duration: 7.638s, episode steps: 1028, steps per second: 135, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.450 [0.000, 5.000],  loss: 0.015849, mae: 3.039669, mean_q: 3.657289, mean_eps: 0.111081\n",
      " 2470757/3750000: episode: 3354, duration: 7.630s, episode steps: 1026, steps per second: 134, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 0.021176, mae: 3.040375, mean_q: 3.657859, mean_eps: 0.110714\n",
      " 2471982/3750000: episode: 3355, duration: 9.179s, episode steps: 1225, steps per second: 133, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.020307, mae: 3.058268, mean_q: 3.681473, mean_eps: 0.110307\n",
      " 2473156/3750000: episode: 3356, duration: 8.701s, episode steps: 1174, steps per second: 135, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.020776, mae: 3.042000, mean_q: 3.662250, mean_eps: 0.109875\n",
      " 2474056/3750000: episode: 3357, duration: 6.676s, episode steps: 900, steps per second: 135, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.993 [0.000, 5.000],  loss: 0.013130, mae: 3.054779, mean_q: 3.676290, mean_eps: 0.109504\n",
      " 2474580/3750000: episode: 3358, duration: 3.969s, episode steps: 524, steps per second: 132, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014301, mae: 3.028768, mean_q: 3.642292, mean_eps: 0.109248\n",
      " 2475553/3750000: episode: 3359, duration: 7.305s, episode steps: 973, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.175 [0.000, 5.000],  loss: 0.016098, mae: 3.048508, mean_q: 3.670195, mean_eps: 0.108978\n",
      " 2476635/3750000: episode: 3360, duration: 8.053s, episode steps: 1082, steps per second: 134, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 0.012665, mae: 3.028831, mean_q: 3.643473, mean_eps: 0.108608\n",
      " 2477815/3750000: episode: 3361, duration: 8.795s, episode steps: 1180, steps per second: 134, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.763 [0.000, 5.000],  loss: 0.017889, mae: 3.056267, mean_q: 3.676289, mean_eps: 0.108201\n",
      " 2478708/3750000: episode: 3362, duration: 6.650s, episode steps: 893, steps per second: 134, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 0.011847, mae: 2.990177, mean_q: 3.598414, mean_eps: 0.107826\n",
      " 2479979/3750000: episode: 3363, duration: 9.424s, episode steps: 1271, steps per second: 135, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.013891, mae: 3.014406, mean_q: 3.631576, mean_eps: 0.107438\n",
      " 2480858/3750000: episode: 3364, duration: 6.597s, episode steps: 879, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.014137, mae: 3.053760, mean_q: 3.675015, mean_eps: 0.107052\n",
      " 2481416/3750000: episode: 3365, duration: 4.193s, episode steps: 558, steps per second: 133, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.017065, mae: 3.049434, mean_q: 3.670409, mean_eps: 0.106793\n",
      " 2482487/3750000: episode: 3366, duration: 8.101s, episode steps: 1071, steps per second: 132, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.153 [0.000, 5.000],  loss: 0.011508, mae: 3.070202, mean_q: 3.695528, mean_eps: 0.106498\n",
      " 2483433/3750000: episode: 3367, duration: 7.008s, episode steps: 946, steps per second: 135, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.017864, mae: 3.070929, mean_q: 3.699711, mean_eps: 0.106134\n",
      " 2484335/3750000: episode: 3368, duration: 6.784s, episode steps: 902, steps per second: 133, episode reward:  5.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.911 [0.000, 5.000],  loss: 0.016373, mae: 3.021629, mean_q: 3.641604, mean_eps: 0.105803\n",
      " 2485002/3750000: episode: 3369, duration: 5.039s, episode steps: 667, steps per second: 132, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.013977, mae: 3.050414, mean_q: 3.673541, mean_eps: 0.105519\n",
      " 2485530/3750000: episode: 3370, duration: 3.888s, episode steps: 528, steps per second: 136, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.015452, mae: 3.011003, mean_q: 3.630484, mean_eps: 0.105303\n",
      " 2486317/3750000: episode: 3371, duration: 5.884s, episode steps: 787, steps per second: 134, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.015525, mae: 2.980583, mean_q: 3.591535, mean_eps: 0.105069\n",
      " 2487459/3750000: episode: 3372, duration: 8.453s, episode steps: 1142, steps per second: 135, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.011892, mae: 3.087412, mean_q: 3.716400, mean_eps: 0.104723\n",
      " 2488189/3750000: episode: 3373, duration: 5.561s, episode steps: 730, steps per second: 131, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.229 [0.000, 5.000],  loss: 0.019514, mae: 3.073921, mean_q: 3.699911, mean_eps: 0.104385\n",
      " 2488676/3750000: episode: 3374, duration: 3.630s, episode steps: 487, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.129 [0.000, 5.000],  loss: 0.018917, mae: 3.067206, mean_q: 3.690306, mean_eps: 0.104165\n",
      " 2489048/3750000: episode: 3375, duration: 2.853s, episode steps: 372, steps per second: 130, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.153 [0.000, 5.000],  loss: 0.018096, mae: 3.008652, mean_q: 3.624570, mean_eps: 0.104010\n",
      " 2490197/3750000: episode: 3376, duration: 8.573s, episode steps: 1149, steps per second: 134, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.464 [0.000, 5.000],  loss: 0.015521, mae: 3.048290, mean_q: 3.675388, mean_eps: 0.103737\n",
      " 2491183/3750000: episode: 3377, duration: 7.389s, episode steps: 986, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.016506, mae: 3.047939, mean_q: 3.671302, mean_eps: 0.103352\n",
      " 2492037/3750000: episode: 3378, duration: 6.285s, episode steps: 854, steps per second: 136, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.017610, mae: 3.077596, mean_q: 3.705881, mean_eps: 0.103020\n",
      " 2492945/3750000: episode: 3379, duration: 6.845s, episode steps: 908, steps per second: 133, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.016707, mae: 3.062958, mean_q: 3.683818, mean_eps: 0.102704\n",
      " 2494066/3750000: episode: 3380, duration: 8.307s, episode steps: 1121, steps per second: 135, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.016009, mae: 3.031784, mean_q: 3.648970, mean_eps: 0.102336\n",
      " 2494845/3750000: episode: 3381, duration: 6.029s, episode steps: 779, steps per second: 129, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.114 [0.000, 5.000],  loss: 0.012729, mae: 3.086892, mean_q: 3.716035, mean_eps: 0.101994\n",
      " 2495253/3750000: episode: 3382, duration: 3.043s, episode steps: 408, steps per second: 134, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.108 [0.000, 5.000],  loss: 0.009990, mae: 3.059624, mean_q: 3.681440, mean_eps: 0.101782\n",
      " 2496110/3750000: episode: 3383, duration: 6.394s, episode steps: 857, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.010619, mae: 2.992178, mean_q: 3.602379, mean_eps: 0.101555\n",
      " 2496541/3750000: episode: 3384, duration: 3.366s, episode steps: 431, steps per second: 128, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.703 [0.000, 5.000],  loss: 0.013985, mae: 3.017632, mean_q: 3.632107, mean_eps: 0.101321\n",
      " 2497247/3750000: episode: 3385, duration: 5.238s, episode steps: 706, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.017800, mae: 3.058954, mean_q: 3.680060, mean_eps: 0.101116\n",
      " 2497800/3750000: episode: 3386, duration: 4.155s, episode steps: 553, steps per second: 133, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.665 [0.000, 5.000],  loss: 0.017833, mae: 3.099527, mean_q: 3.728620, mean_eps: 0.100893\n",
      " 2498957/3750000: episode: 3387, duration: 8.662s, episode steps: 1157, steps per second: 134, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.020770, mae: 3.093916, mean_q: 3.726323, mean_eps: 0.100587\n",
      " 2499766/3750000: episode: 3388, duration: 6.045s, episode steps: 809, steps per second: 134, episode reward: 28.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.017555, mae: 3.068451, mean_q: 3.694993, mean_eps: 0.100230\n",
      " 2500312/3750000: episode: 3389, duration: 4.176s, episode steps: 546, steps per second: 131, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.251 [0.000, 5.000],  loss: 0.014356, mae: 3.068714, mean_q: 3.702251, mean_eps: 0.100018\n",
      " 2501042/3750000: episode: 3390, duration: 5.583s, episode steps: 730, steps per second: 131, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.018607, mae: 3.055274, mean_q: 3.678495, mean_eps: 0.100000\n",
      " 2502093/3750000: episode: 3391, duration: 7.797s, episode steps: 1051, steps per second: 135, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.310 [0.000, 5.000],  loss: 0.019351, mae: 3.048307, mean_q: 3.674844, mean_eps: 0.100000\n",
      " 2502766/3750000: episode: 3392, duration: 5.044s, episode steps: 673, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.267 [0.000, 5.000],  loss: 0.019727, mae: 3.099696, mean_q: 3.729528, mean_eps: 0.100000\n",
      " 2503576/3750000: episode: 3393, duration: 6.044s, episode steps: 810, steps per second: 134, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.016783, mae: 3.056548, mean_q: 3.678785, mean_eps: 0.100000\n",
      " 2505086/3750000: episode: 3394, duration: 11.363s, episode steps: 1510, steps per second: 133, episode reward: 27.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.074 [0.000, 5.000],  loss: 0.016599, mae: 3.062757, mean_q: 3.687034, mean_eps: 0.100000\n",
      " 2505563/3750000: episode: 3395, duration: 3.574s, episode steps: 477, steps per second: 133, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.015716, mae: 3.103712, mean_q: 3.743660, mean_eps: 0.100000\n",
      " 2506120/3750000: episode: 3396, duration: 4.187s, episode steps: 557, steps per second: 133, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.964 [0.000, 5.000],  loss: 0.018145, mae: 3.177698, mean_q: 3.827051, mean_eps: 0.100000\n",
      " 2507274/3750000: episode: 3397, duration: 8.682s, episode steps: 1154, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.014837, mae: 3.062992, mean_q: 3.689410, mean_eps: 0.100000\n",
      " 2508043/3750000: episode: 3398, duration: 5.788s, episode steps: 769, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.015252, mae: 3.076951, mean_q: 3.704648, mean_eps: 0.100000\n",
      " 2509319/3750000: episode: 3399, duration: 9.529s, episode steps: 1276, steps per second: 134, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.017980, mae: 3.098605, mean_q: 3.727677, mean_eps: 0.100000\n",
      " 2510278/3750000: episode: 3400, duration: 7.123s, episode steps: 959, steps per second: 135, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.389 [0.000, 5.000],  loss: 0.018233, mae: 3.157482, mean_q: 3.799698, mean_eps: 0.100000\n",
      " 2511102/3750000: episode: 3401, duration: 6.279s, episode steps: 824, steps per second: 131, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.020492, mae: 3.076879, mean_q: 3.698851, mean_eps: 0.100000\n",
      " 2512676/3750000: episode: 3402, duration: 11.856s, episode steps: 1574, steps per second: 133, episode reward: 21.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.623 [0.000, 5.000],  loss: 0.016042, mae: 3.100560, mean_q: 3.732777, mean_eps: 0.100000\n",
      " 2513155/3750000: episode: 3403, duration: 3.600s, episode steps: 479, steps per second: 133, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.157 [0.000, 5.000],  loss: 0.016815, mae: 3.120960, mean_q: 3.755090, mean_eps: 0.100000\n",
      " 2513563/3750000: episode: 3404, duration: 3.141s, episode steps: 408, steps per second: 130, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.014207, mae: 3.141646, mean_q: 3.782355, mean_eps: 0.100000\n",
      " 2514222/3750000: episode: 3405, duration: 4.909s, episode steps: 659, steps per second: 134, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.187 [0.000, 5.000],  loss: 0.014559, mae: 3.042117, mean_q: 3.664209, mean_eps: 0.100000\n",
      " 2514916/3750000: episode: 3406, duration: 5.100s, episode steps: 694, steps per second: 136, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.390 [0.000, 5.000],  loss: 0.018268, mae: 3.127006, mean_q: 3.761140, mean_eps: 0.100000\n",
      " 2515701/3750000: episode: 3407, duration: 6.006s, episode steps: 785, steps per second: 131, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.013950, mae: 3.103998, mean_q: 3.736925, mean_eps: 0.100000\n",
      " 2516775/3750000: episode: 3408, duration: 7.944s, episode steps: 1074, steps per second: 135, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 0.014589, mae: 3.118850, mean_q: 3.753319, mean_eps: 0.100000\n",
      " 2517693/3750000: episode: 3409, duration: 6.833s, episode steps: 918, steps per second: 134, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.015489, mae: 3.135656, mean_q: 3.770714, mean_eps: 0.100000\n",
      " 2518390/3750000: episode: 3410, duration: 5.225s, episode steps: 697, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.012648, mae: 3.190712, mean_q: 3.837912, mean_eps: 0.100000\n",
      " 2519014/3750000: episode: 3411, duration: 4.626s, episode steps: 624, steps per second: 135, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.963 [0.000, 5.000],  loss: 0.019873, mae: 3.110618, mean_q: 3.746730, mean_eps: 0.100000\n",
      " 2519880/3750000: episode: 3412, duration: 6.533s, episode steps: 866, steps per second: 133, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.352 [0.000, 5.000],  loss: 0.020249, mae: 3.150626, mean_q: 3.793030, mean_eps: 0.100000\n",
      " 2520821/3750000: episode: 3413, duration: 7.161s, episode steps: 941, steps per second: 131, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.012373, mae: 3.112143, mean_q: 3.747044, mean_eps: 0.100000\n",
      " 2521712/3750000: episode: 3414, duration: 6.570s, episode steps: 891, steps per second: 136, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.652 [0.000, 5.000],  loss: 0.015267, mae: 3.124658, mean_q: 3.758555, mean_eps: 0.100000\n",
      " 2522654/3750000: episode: 3415, duration: 7.067s, episode steps: 942, steps per second: 133, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.018140, mae: 3.091841, mean_q: 3.717950, mean_eps: 0.100000\n",
      " 2523450/3750000: episode: 3416, duration: 6.037s, episode steps: 796, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.222 [0.000, 5.000],  loss: 0.014341, mae: 3.120065, mean_q: 3.756656, mean_eps: 0.100000\n",
      " 2524160/3750000: episode: 3417, duration: 5.361s, episode steps: 710, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.013982, mae: 3.100606, mean_q: 3.734891, mean_eps: 0.100000\n",
      " 2525046/3750000: episode: 3418, duration: 6.693s, episode steps: 886, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.018188, mae: 3.085073, mean_q: 3.712325, mean_eps: 0.100000\n",
      " 2526049/3750000: episode: 3419, duration: 7.480s, episode steps: 1003, steps per second: 134, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.015190, mae: 3.145585, mean_q: 3.784852, mean_eps: 0.100000\n",
      " 2526890/3750000: episode: 3420, duration: 6.422s, episode steps: 841, steps per second: 131, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.016960, mae: 3.127884, mean_q: 3.763237, mean_eps: 0.100000\n",
      " 2527606/3750000: episode: 3421, duration: 5.398s, episode steps: 716, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.012670, mae: 3.100398, mean_q: 3.732610, mean_eps: 0.100000\n",
      " 2528279/3750000: episode: 3422, duration: 5.019s, episode steps: 673, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.015666, mae: 3.183323, mean_q: 3.831099, mean_eps: 0.100000\n",
      " 2529141/3750000: episode: 3423, duration: 6.647s, episode steps: 862, steps per second: 130, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.907 [0.000, 5.000],  loss: 0.011891, mae: 3.188644, mean_q: 3.845219, mean_eps: 0.100000\n",
      " 2529659/3750000: episode: 3424, duration: 3.761s, episode steps: 518, steps per second: 138, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.011449, mae: 3.096653, mean_q: 3.723638, mean_eps: 0.100000\n",
      " 2530421/3750000: episode: 3425, duration: 5.802s, episode steps: 762, steps per second: 131, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.757 [0.000, 5.000],  loss: 0.016613, mae: 3.128781, mean_q: 3.765689, mean_eps: 0.100000\n",
      " 2531382/3750000: episode: 3426, duration: 7.191s, episode steps: 961, steps per second: 134, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.941 [0.000, 5.000],  loss: 0.015844, mae: 3.134872, mean_q: 3.772135, mean_eps: 0.100000\n",
      " 2532165/3750000: episode: 3427, duration: 5.895s, episode steps: 783, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.215 [0.000, 5.000],  loss: 0.010873, mae: 3.124830, mean_q: 3.761888, mean_eps: 0.100000\n",
      " 2533327/3750000: episode: 3428, duration: 8.741s, episode steps: 1162, steps per second: 133, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.013419, mae: 3.126481, mean_q: 3.767719, mean_eps: 0.100000\n",
      " 2534001/3750000: episode: 3429, duration: 5.041s, episode steps: 674, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.553 [0.000, 5.000],  loss: 0.015706, mae: 3.115287, mean_q: 3.749266, mean_eps: 0.100000\n",
      " 2535244/3750000: episode: 3430, duration: 9.254s, episode steps: 1243, steps per second: 134, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.020497, mae: 3.144275, mean_q: 3.789942, mean_eps: 0.100000\n",
      " 2535977/3750000: episode: 3431, duration: 5.397s, episode steps: 733, steps per second: 136, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.022450, mae: 3.110145, mean_q: 3.737231, mean_eps: 0.100000\n",
      " 2536878/3750000: episode: 3432, duration: 6.725s, episode steps: 901, steps per second: 134, episode reward: 29.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.016061, mae: 3.116707, mean_q: 3.748490, mean_eps: 0.100000\n",
      " 2537526/3750000: episode: 3433, duration: 4.916s, episode steps: 648, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.011307, mae: 3.107253, mean_q: 3.743354, mean_eps: 0.100000\n",
      " 2538017/3750000: episode: 3434, duration: 3.616s, episode steps: 491, steps per second: 136, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.017773, mae: 3.162536, mean_q: 3.807041, mean_eps: 0.100000\n",
      " 2538639/3750000: episode: 3435, duration: 4.654s, episode steps: 622, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013586, mae: 3.138036, mean_q: 3.781426, mean_eps: 0.100000\n",
      " 2539472/3750000: episode: 3436, duration: 6.285s, episode steps: 833, steps per second: 133, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.017719, mae: 3.120340, mean_q: 3.755012, mean_eps: 0.100000\n",
      " 2540476/3750000: episode: 3437, duration: 7.498s, episode steps: 1004, steps per second: 134, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.809 [0.000, 5.000],  loss: 0.020399, mae: 3.136247, mean_q: 3.770099, mean_eps: 0.100000\n",
      " 2541413/3750000: episode: 3438, duration: 7.058s, episode steps: 937, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.019706, mae: 3.076048, mean_q: 3.697694, mean_eps: 0.100000\n",
      " 2542422/3750000: episode: 3439, duration: 7.578s, episode steps: 1009, steps per second: 133, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.018783, mae: 3.145318, mean_q: 3.783510, mean_eps: 0.100000\n",
      " 2543333/3750000: episode: 3440, duration: 6.771s, episode steps: 911, steps per second: 135, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.014878, mae: 3.167412, mean_q: 3.810444, mean_eps: 0.100000\n",
      " 2544447/3750000: episode: 3441, duration: 8.541s, episode steps: 1114, steps per second: 130, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.113 [0.000, 5.000],  loss: 0.013024, mae: 3.083094, mean_q: 3.719414, mean_eps: 0.100000\n",
      " 2545717/3750000: episode: 3442, duration: 9.483s, episode steps: 1270, steps per second: 134, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.094 [0.000, 5.000],  loss: 0.016323, mae: 3.162248, mean_q: 3.802813, mean_eps: 0.100000\n",
      " 2546398/3750000: episode: 3443, duration: 5.107s, episode steps: 681, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.016088, mae: 3.136381, mean_q: 3.771955, mean_eps: 0.100000\n",
      " 2547483/3750000: episode: 3444, duration: 8.172s, episode steps: 1085, steps per second: 133, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.338 [0.000, 5.000],  loss: 0.014213, mae: 3.144818, mean_q: 3.789147, mean_eps: 0.100000\n",
      " 2548101/3750000: episode: 3445, duration: 4.703s, episode steps: 618, steps per second: 131, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.017350, mae: 3.150068, mean_q: 3.791028, mean_eps: 0.100000\n",
      " 2548916/3750000: episode: 3446, duration: 6.023s, episode steps: 815, steps per second: 135, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.690 [0.000, 5.000],  loss: 0.019071, mae: 3.137153, mean_q: 3.779856, mean_eps: 0.100000\n",
      " 2549797/3750000: episode: 3447, duration: 6.658s, episode steps: 881, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.900 [0.000, 5.000],  loss: 0.018242, mae: 3.156325, mean_q: 3.794624, mean_eps: 0.100000\n",
      " 2550961/3750000: episode: 3448, duration: 8.711s, episode steps: 1164, steps per second: 134, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.369 [0.000, 5.000],  loss: 0.013587, mae: 3.151387, mean_q: 3.789648, mean_eps: 0.100000\n",
      " 2552239/3750000: episode: 3449, duration: 9.461s, episode steps: 1278, steps per second: 135, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.119 [0.000, 5.000],  loss: 0.014808, mae: 3.115767, mean_q: 3.749672, mean_eps: 0.100000\n",
      " 2552835/3750000: episode: 3450, duration: 4.460s, episode steps: 596, steps per second: 134, episode reward: 17.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.243 [0.000, 5.000],  loss: 0.015196, mae: 3.210083, mean_q: 3.864576, mean_eps: 0.100000\n",
      " 2553446/3750000: episode: 3451, duration: 4.861s, episode steps: 611, steps per second: 126, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.879 [0.000, 5.000],  loss: 0.015535, mae: 3.125046, mean_q: 3.761695, mean_eps: 0.100000\n",
      " 2554118/3750000: episode: 3452, duration: 5.039s, episode steps: 672, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.165 [0.000, 5.000],  loss: 0.015834, mae: 3.208131, mean_q: 3.854796, mean_eps: 0.100000\n",
      " 2555243/3750000: episode: 3453, duration: 8.445s, episode steps: 1125, steps per second: 133, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.060 [0.000, 5.000],  loss: 0.014222, mae: 3.165408, mean_q: 3.803200, mean_eps: 0.100000\n",
      " 2556474/3750000: episode: 3454, duration: 9.199s, episode steps: 1231, steps per second: 134, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.017154, mae: 3.165985, mean_q: 3.812716, mean_eps: 0.100000\n",
      " 2557339/3750000: episode: 3455, duration: 6.425s, episode steps: 865, steps per second: 135, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.563 [0.000, 5.000],  loss: 0.012633, mae: 3.118682, mean_q: 3.753709, mean_eps: 0.100000\n",
      " 2558061/3750000: episode: 3456, duration: 5.460s, episode steps: 722, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.016006, mae: 3.110026, mean_q: 3.738380, mean_eps: 0.100000\n",
      " 2559476/3750000: episode: 3457, duration: 10.493s, episode steps: 1415, steps per second: 135, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.019044, mae: 3.134279, mean_q: 3.774753, mean_eps: 0.100000\n",
      " 2560646/3750000: episode: 3458, duration: 8.861s, episode steps: 1170, steps per second: 132, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.016392, mae: 3.124888, mean_q: 3.761874, mean_eps: 0.100000\n",
      " 2561737/3750000: episode: 3459, duration: 8.080s, episode steps: 1091, steps per second: 135, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.296 [0.000, 5.000],  loss: 0.017556, mae: 3.095469, mean_q: 3.723629, mean_eps: 0.100000\n",
      " 2562404/3750000: episode: 3460, duration: 5.030s, episode steps: 667, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.019771, mae: 3.025003, mean_q: 3.642617, mean_eps: 0.100000\n",
      " 2562880/3750000: episode: 3461, duration: 3.602s, episode steps: 476, steps per second: 132, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 4.090 [0.000, 5.000],  loss: 0.015297, mae: 3.170761, mean_q: 3.816866, mean_eps: 0.100000\n",
      " 2564444/3750000: episode: 3462, duration: 11.793s, episode steps: 1564, steps per second: 133, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.015828, mae: 3.081493, mean_q: 3.711981, mean_eps: 0.100000\n",
      " 2565350/3750000: episode: 3463, duration: 6.815s, episode steps: 906, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.161 [0.000, 5.000],  loss: 0.013764, mae: 3.107346, mean_q: 3.740705, mean_eps: 0.100000\n",
      " 2566270/3750000: episode: 3464, duration: 6.868s, episode steps: 920, steps per second: 134, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.014267, mae: 3.096947, mean_q: 3.731639, mean_eps: 0.100000\n",
      " 2567459/3750000: episode: 3465, duration: 8.931s, episode steps: 1189, steps per second: 133, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.018526, mae: 3.086602, mean_q: 3.718217, mean_eps: 0.100000\n",
      " 2568256/3750000: episode: 3466, duration: 5.962s, episode steps: 797, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.013993, mae: 3.108644, mean_q: 3.745042, mean_eps: 0.100000\n",
      " 2569032/3750000: episode: 3467, duration: 5.876s, episode steps: 776, steps per second: 132, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.013009, mae: 3.127588, mean_q: 3.770899, mean_eps: 0.100000\n",
      " 2569543/3750000: episode: 3468, duration: 3.860s, episode steps: 511, steps per second: 132, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.015556, mae: 3.119956, mean_q: 3.757983, mean_eps: 0.100000\n",
      " 2570601/3750000: episode: 3469, duration: 7.899s, episode steps: 1058, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.018103, mae: 3.115628, mean_q: 3.752048, mean_eps: 0.100000\n",
      " 2571440/3750000: episode: 3470, duration: 6.308s, episode steps: 839, steps per second: 133, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.020841, mae: 3.097145, mean_q: 3.730189, mean_eps: 0.100000\n",
      " 2572180/3750000: episode: 3471, duration: 5.717s, episode steps: 740, steps per second: 129, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.020 [0.000, 5.000],  loss: 0.012852, mae: 3.177268, mean_q: 3.822206, mean_eps: 0.100000\n",
      " 2572819/3750000: episode: 3472, duration: 4.792s, episode steps: 639, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.155 [0.000, 5.000],  loss: 0.018174, mae: 3.084665, mean_q: 3.711574, mean_eps: 0.100000\n",
      " 2573699/3750000: episode: 3473, duration: 6.630s, episode steps: 880, steps per second: 133, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.448 [0.000, 5.000],  loss: 0.011187, mae: 3.179634, mean_q: 3.833001, mean_eps: 0.100000\n",
      " 2574192/3750000: episode: 3474, duration: 3.714s, episode steps: 493, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.396 [0.000, 5.000],  loss: 0.011805, mae: 3.151390, mean_q: 3.796426, mean_eps: 0.100000\n",
      " 2575028/3750000: episode: 3475, duration: 6.411s, episode steps: 836, steps per second: 130, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.015685, mae: 3.170279, mean_q: 3.815706, mean_eps: 0.100000\n",
      " 2575675/3750000: episode: 3476, duration: 4.888s, episode steps: 647, steps per second: 132, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.731 [0.000, 5.000],  loss: 0.018839, mae: 3.195369, mean_q: 3.850883, mean_eps: 0.100000\n",
      " 2577300/3750000: episode: 3477, duration: 12.245s, episode steps: 1625, steps per second: 133, episode reward: 37.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.019446, mae: 3.131714, mean_q: 3.766264, mean_eps: 0.100000\n",
      " 2578051/3750000: episode: 3478, duration: 5.800s, episode steps: 751, steps per second: 129, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.830 [0.000, 5.000],  loss: 0.015174, mae: 3.155158, mean_q: 3.801245, mean_eps: 0.100000\n",
      " 2579300/3750000: episode: 3479, duration: 9.518s, episode steps: 1249, steps per second: 131, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.681 [0.000, 5.000],  loss: 0.014942, mae: 3.155903, mean_q: 3.800291, mean_eps: 0.100000\n",
      " 2580396/3750000: episode: 3480, duration: 8.266s, episode steps: 1096, steps per second: 133, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.237 [0.000, 5.000],  loss: 0.013036, mae: 3.145240, mean_q: 3.790784, mean_eps: 0.100000\n",
      " 2581044/3750000: episode: 3481, duration: 4.904s, episode steps: 648, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.017593, mae: 3.122109, mean_q: 3.757951, mean_eps: 0.100000\n",
      " 2581675/3750000: episode: 3482, duration: 4.726s, episode steps: 631, steps per second: 134, episode reward: 22.000, mean reward:  0.035 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.019006, mae: 3.137792, mean_q: 3.771251, mean_eps: 0.100000\n",
      " 2582867/3750000: episode: 3483, duration: 9.015s, episode steps: 1192, steps per second: 132, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.126 [0.000, 5.000],  loss: 0.015012, mae: 3.120012, mean_q: 3.757794, mean_eps: 0.100000\n",
      " 2584070/3750000: episode: 3484, duration: 8.989s, episode steps: 1203, steps per second: 134, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.700 [0.000, 5.000],  loss: 0.015272, mae: 3.152797, mean_q: 3.801831, mean_eps: 0.100000\n",
      " 2585156/3750000: episode: 3485, duration: 8.063s, episode steps: 1086, steps per second: 135, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.016669, mae: 3.113939, mean_q: 3.752747, mean_eps: 0.100000\n",
      " 2586286/3750000: episode: 3486, duration: 8.572s, episode steps: 1130, steps per second: 132, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.016857, mae: 3.068929, mean_q: 3.696916, mean_eps: 0.100000\n",
      " 2586948/3750000: episode: 3487, duration: 4.912s, episode steps: 662, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.010340, mae: 3.078873, mean_q: 3.705721, mean_eps: 0.100000\n",
      " 2587714/3750000: episode: 3488, duration: 5.703s, episode steps: 766, steps per second: 134, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.015004, mae: 3.090886, mean_q: 3.719541, mean_eps: 0.100000\n",
      " 2588484/3750000: episode: 3489, duration: 5.860s, episode steps: 770, steps per second: 131, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.922 [0.000, 5.000],  loss: 0.015174, mae: 3.075896, mean_q: 3.700513, mean_eps: 0.100000\n",
      " 2589506/3750000: episode: 3490, duration: 7.593s, episode steps: 1022, steps per second: 135, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.795 [0.000, 5.000],  loss: 0.013910, mae: 3.058276, mean_q: 3.679089, mean_eps: 0.100000\n",
      " 2590523/3750000: episode: 3491, duration: 7.687s, episode steps: 1017, steps per second: 132, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 0.711 [0.000, 5.000],  loss: 0.016883, mae: 3.116018, mean_q: 3.753752, mean_eps: 0.100000\n",
      " 2591389/3750000: episode: 3492, duration: 6.457s, episode steps: 866, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.670 [0.000, 5.000],  loss: 0.019711, mae: 3.066836, mean_q: 3.689429, mean_eps: 0.100000\n",
      " 2592592/3750000: episode: 3493, duration: 9.097s, episode steps: 1203, steps per second: 132, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.850 [0.000, 5.000],  loss: 0.016815, mae: 3.057583, mean_q: 3.677339, mean_eps: 0.100000\n",
      " 2593383/3750000: episode: 3494, duration: 5.943s, episode steps: 791, steps per second: 133, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.013779, mae: 3.108821, mean_q: 3.745479, mean_eps: 0.100000\n",
      " 2594179/3750000: episode: 3495, duration: 5.894s, episode steps: 796, steps per second: 135, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.015582, mae: 3.116669, mean_q: 3.750869, mean_eps: 0.100000\n",
      " 2595087/3750000: episode: 3496, duration: 6.878s, episode steps: 908, steps per second: 132, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.013428, mae: 3.137813, mean_q: 3.773343, mean_eps: 0.100000\n",
      " 2596069/3750000: episode: 3497, duration: 7.324s, episode steps: 982, steps per second: 134, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.011809, mae: 3.122240, mean_q: 3.756872, mean_eps: 0.100000\n",
      " 2597509/3750000: episode: 3498, duration: 10.842s, episode steps: 1440, steps per second: 133, episode reward: 21.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.015101, mae: 3.107828, mean_q: 3.735987, mean_eps: 0.100000\n",
      " 2598115/3750000: episode: 3499, duration: 4.508s, episode steps: 606, steps per second: 134, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.013559, mae: 3.046437, mean_q: 3.665464, mean_eps: 0.100000\n",
      " 2599494/3750000: episode: 3500, duration: 10.363s, episode steps: 1379, steps per second: 133, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.133 [0.000, 5.000],  loss: 0.017265, mae: 3.117214, mean_q: 3.754599, mean_eps: 0.100000\n",
      " 2600517/3750000: episode: 3501, duration: 7.652s, episode steps: 1023, steps per second: 134, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.017327, mae: 3.116609, mean_q: 3.753110, mean_eps: 0.100000\n",
      " 2601663/3750000: episode: 3502, duration: 8.690s, episode steps: 1146, steps per second: 132, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.017017, mae: 3.120202, mean_q: 3.752648, mean_eps: 0.100000\n",
      " 2602514/3750000: episode: 3503, duration: 6.310s, episode steps: 851, steps per second: 135, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013609, mae: 3.112420, mean_q: 3.749989, mean_eps: 0.100000\n",
      " 2603624/3750000: episode: 3504, duration: 8.448s, episode steps: 1110, steps per second: 131, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.013651, mae: 3.099543, mean_q: 3.732307, mean_eps: 0.100000\n",
      " 2604285/3750000: episode: 3505, duration: 4.930s, episode steps: 661, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.814 [0.000, 5.000],  loss: 0.013180, mae: 3.124697, mean_q: 3.764085, mean_eps: 0.100000\n",
      " 2604797/3750000: episode: 3506, duration: 3.772s, episode steps: 512, steps per second: 136, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.018822, mae: 3.124485, mean_q: 3.764492, mean_eps: 0.100000\n",
      " 2606005/3750000: episode: 3507, duration: 9.108s, episode steps: 1208, steps per second: 133, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.272 [0.000, 5.000],  loss: 0.014278, mae: 3.122895, mean_q: 3.762253, mean_eps: 0.100000\n",
      " 2607065/3750000: episode: 3508, duration: 7.912s, episode steps: 1060, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013567, mae: 3.130618, mean_q: 3.768475, mean_eps: 0.100000\n",
      " 2607735/3750000: episode: 3509, duration: 4.999s, episode steps: 670, steps per second: 134, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.013436, mae: 3.072506, mean_q: 3.695547, mean_eps: 0.100000\n",
      " 2608371/3750000: episode: 3510, duration: 4.843s, episode steps: 636, steps per second: 131, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.017216, mae: 3.103623, mean_q: 3.731546, mean_eps: 0.100000\n",
      " 2609180/3750000: episode: 3511, duration: 6.154s, episode steps: 809, steps per second: 131, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.253 [0.000, 5.000],  loss: 0.011537, mae: 3.078999, mean_q: 3.703748, mean_eps: 0.100000\n",
      " 2609943/3750000: episode: 3512, duration: 5.926s, episode steps: 763, steps per second: 129, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.595 [0.000, 5.000],  loss: 0.018562, mae: 3.079628, mean_q: 3.703152, mean_eps: 0.100000\n",
      " 2610474/3750000: episode: 3513, duration: 3.988s, episode steps: 531, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.011612, mae: 3.090606, mean_q: 3.719663, mean_eps: 0.100000\n",
      " 2611287/3750000: episode: 3514, duration: 6.103s, episode steps: 813, steps per second: 133, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 0.017258, mae: 3.123902, mean_q: 3.759043, mean_eps: 0.100000\n",
      " 2612187/3750000: episode: 3515, duration: 6.787s, episode steps: 900, steps per second: 133, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.015036, mae: 3.094906, mean_q: 3.723688, mean_eps: 0.100000\n",
      " 2612821/3750000: episode: 3516, duration: 4.762s, episode steps: 634, steps per second: 133, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.334 [0.000, 5.000],  loss: 0.021041, mae: 3.100993, mean_q: 3.739333, mean_eps: 0.100000\n",
      " 2613883/3750000: episode: 3517, duration: 7.971s, episode steps: 1062, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.335 [0.000, 5.000],  loss: 0.013281, mae: 3.129342, mean_q: 3.766849, mean_eps: 0.100000\n",
      " 2614660/3750000: episode: 3518, duration: 5.765s, episode steps: 777, steps per second: 135, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.017426, mae: 3.146320, mean_q: 3.788381, mean_eps: 0.100000\n",
      " 2615527/3750000: episode: 3519, duration: 6.547s, episode steps: 867, steps per second: 132, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.015492, mae: 3.120930, mean_q: 3.757876, mean_eps: 0.100000\n",
      " 2615879/3750000: episode: 3520, duration: 2.648s, episode steps: 352, steps per second: 133, episode reward:  9.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.489 [0.000, 5.000],  loss: 0.013917, mae: 3.110976, mean_q: 3.740287, mean_eps: 0.100000\n",
      " 2616240/3750000: episode: 3521, duration: 2.775s, episode steps: 361, steps per second: 130, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.584 [0.000, 5.000],  loss: 0.016683, mae: 3.110763, mean_q: 3.738427, mean_eps: 0.100000\n",
      " 2617140/3750000: episode: 3522, duration: 6.785s, episode steps: 900, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: 0.016510, mae: 3.111336, mean_q: 3.743236, mean_eps: 0.100000\n",
      " 2617809/3750000: episode: 3523, duration: 5.039s, episode steps: 669, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.021123, mae: 3.093851, mean_q: 3.721263, mean_eps: 0.100000\n",
      " 2619038/3750000: episode: 3524, duration: 9.146s, episode steps: 1229, steps per second: 134, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.014864, mae: 3.119137, mean_q: 3.756031, mean_eps: 0.100000\n",
      " 2619662/3750000: episode: 3525, duration: 4.754s, episode steps: 624, steps per second: 131, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.016430, mae: 3.171307, mean_q: 3.812257, mean_eps: 0.100000\n",
      " 2620025/3750000: episode: 3526, duration: 2.765s, episode steps: 363, steps per second: 131, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 4.069 [0.000, 5.000],  loss: 0.015330, mae: 3.150427, mean_q: 3.799181, mean_eps: 0.100000\n",
      " 2621048/3750000: episode: 3527, duration: 7.620s, episode steps: 1023, steps per second: 134, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.013796, mae: 3.130310, mean_q: 3.767406, mean_eps: 0.100000\n",
      " 2622106/3750000: episode: 3528, duration: 7.893s, episode steps: 1058, steps per second: 134, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.015232, mae: 3.111668, mean_q: 3.744302, mean_eps: 0.100000\n",
      " 2622974/3750000: episode: 3529, duration: 6.473s, episode steps: 868, steps per second: 134, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.022001, mae: 3.150093, mean_q: 3.792376, mean_eps: 0.100000\n",
      " 2623685/3750000: episode: 3530, duration: 5.364s, episode steps: 711, steps per second: 133, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.014403, mae: 3.137339, mean_q: 3.779624, mean_eps: 0.100000\n",
      " 2624817/3750000: episode: 3531, duration: 8.651s, episode steps: 1132, steps per second: 131, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.732 [0.000, 5.000],  loss: 0.015162, mae: 3.160345, mean_q: 3.804058, mean_eps: 0.100000\n",
      " 2625499/3750000: episode: 3532, duration: 5.111s, episode steps: 682, steps per second: 133, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.588 [0.000, 5.000],  loss: 0.018731, mae: 3.169225, mean_q: 3.814449, mean_eps: 0.100000\n",
      " 2626435/3750000: episode: 3533, duration: 7.051s, episode steps: 936, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.015245, mae: 3.152710, mean_q: 3.793170, mean_eps: 0.100000\n",
      " 2627110/3750000: episode: 3534, duration: 5.068s, episode steps: 675, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.767 [0.000, 5.000],  loss: 0.012774, mae: 3.202044, mean_q: 3.851791, mean_eps: 0.100000\n",
      " 2628047/3750000: episode: 3535, duration: 7.051s, episode steps: 937, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.953 [0.000, 5.000],  loss: 0.013056, mae: 3.216092, mean_q: 3.870663, mean_eps: 0.100000\n",
      " 2628938/3750000: episode: 3536, duration: 6.745s, episode steps: 891, steps per second: 132, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.292 [0.000, 5.000],  loss: 0.014227, mae: 3.206025, mean_q: 3.863898, mean_eps: 0.100000\n",
      " 2630402/3750000: episode: 3537, duration: 11.024s, episode steps: 1464, steps per second: 133, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.013774, mae: 3.186812, mean_q: 3.833566, mean_eps: 0.100000\n",
      " 2631042/3750000: episode: 3538, duration: 4.831s, episode steps: 640, steps per second: 132, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.763 [0.000, 5.000],  loss: 0.017105, mae: 3.165675, mean_q: 3.812329, mean_eps: 0.100000\n",
      " 2631776/3750000: episode: 3539, duration: 5.439s, episode steps: 734, steps per second: 135, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.215 [0.000, 5.000],  loss: 0.018961, mae: 3.247890, mean_q: 3.902840, mean_eps: 0.100000\n",
      " 2632911/3750000: episode: 3540, duration: 8.551s, episode steps: 1135, steps per second: 133, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.017038, mae: 3.198244, mean_q: 3.844891, mean_eps: 0.100000\n",
      " 2633634/3750000: episode: 3541, duration: 5.432s, episode steps: 723, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.017329, mae: 3.156002, mean_q: 3.796241, mean_eps: 0.100000\n",
      " 2634162/3750000: episode: 3542, duration: 3.991s, episode steps: 528, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.316 [0.000, 5.000],  loss: 0.018290, mae: 3.161713, mean_q: 3.805981, mean_eps: 0.100000\n",
      " 2634905/3750000: episode: 3543, duration: 5.543s, episode steps: 743, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.019388, mae: 3.179226, mean_q: 3.822064, mean_eps: 0.100000\n",
      " 2635951/3750000: episode: 3544, duration: 7.792s, episode steps: 1046, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.148 [0.000, 5.000],  loss: 0.015332, mae: 3.207145, mean_q: 3.860110, mean_eps: 0.100000\n",
      " 2636308/3750000: episode: 3545, duration: 2.719s, episode steps: 357, steps per second: 131, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.020427, mae: 3.172451, mean_q: 3.813667, mean_eps: 0.100000\n",
      " 2637403/3750000: episode: 3546, duration: 8.214s, episode steps: 1095, steps per second: 133, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.019614, mae: 3.173102, mean_q: 3.819971, mean_eps: 0.100000\n",
      " 2638325/3750000: episode: 3547, duration: 6.892s, episode steps: 922, steps per second: 134, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.015895, mae: 3.191365, mean_q: 3.836239, mean_eps: 0.100000\n",
      " 2639279/3750000: episode: 3548, duration: 7.072s, episode steps: 954, steps per second: 135, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.201 [0.000, 5.000],  loss: 0.016207, mae: 3.184480, mean_q: 3.828484, mean_eps: 0.100000\n",
      " 2640097/3750000: episode: 3549, duration: 6.111s, episode steps: 818, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.014776, mae: 3.134083, mean_q: 3.767969, mean_eps: 0.100000\n",
      " 2640633/3750000: episode: 3550, duration: 4.033s, episode steps: 536, steps per second: 133, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.729 [0.000, 5.000],  loss: 0.011910, mae: 3.200803, mean_q: 3.848944, mean_eps: 0.100000\n",
      " 2641174/3750000: episode: 3551, duration: 4.039s, episode steps: 541, steps per second: 134, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.015077, mae: 3.194338, mean_q: 3.841144, mean_eps: 0.100000\n",
      " 2641823/3750000: episode: 3552, duration: 4.969s, episode steps: 649, steps per second: 131, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.015606, mae: 3.136633, mean_q: 3.775528, mean_eps: 0.100000\n",
      " 2642841/3750000: episode: 3553, duration: 7.584s, episode steps: 1018, steps per second: 134, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.268 [0.000, 5.000],  loss: 0.018841, mae: 3.169790, mean_q: 3.811995, mean_eps: 0.100000\n",
      " 2643505/3750000: episode: 3554, duration: 5.004s, episode steps: 664, steps per second: 133, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.015671, mae: 3.172137, mean_q: 3.817298, mean_eps: 0.100000\n",
      " 2644270/3750000: episode: 3555, duration: 5.770s, episode steps: 765, steps per second: 133, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.108 [0.000, 5.000],  loss: 0.012523, mae: 3.225299, mean_q: 3.889806, mean_eps: 0.100000\n",
      " 2644833/3750000: episode: 3556, duration: 4.183s, episode steps: 563, steps per second: 135, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.313 [0.000, 5.000],  loss: 0.015706, mae: 3.250991, mean_q: 3.913339, mean_eps: 0.100000\n",
      " 2645819/3750000: episode: 3557, duration: 7.354s, episode steps: 986, steps per second: 134, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.339 [0.000, 5.000],  loss: 0.016379, mae: 3.187700, mean_q: 3.844232, mean_eps: 0.100000\n",
      " 2646530/3750000: episode: 3558, duration: 5.361s, episode steps: 711, steps per second: 133, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.016668, mae: 3.169475, mean_q: 3.810209, mean_eps: 0.100000\n",
      " 2646887/3750000: episode: 3559, duration: 2.682s, episode steps: 357, steps per second: 133, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.106 [0.000, 5.000],  loss: 0.023419, mae: 3.171768, mean_q: 3.808383, mean_eps: 0.100000\n",
      " 2647584/3750000: episode: 3560, duration: 5.256s, episode steps: 697, steps per second: 133, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.011923, mae: 3.203687, mean_q: 3.856354, mean_eps: 0.100000\n",
      " 2648319/3750000: episode: 3561, duration: 5.481s, episode steps: 735, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.017109, mae: 3.115634, mean_q: 3.745536, mean_eps: 0.100000\n",
      " 2649160/3750000: episode: 3562, duration: 6.417s, episode steps: 841, steps per second: 131, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.015831, mae: 3.199228, mean_q: 3.847331, mean_eps: 0.100000\n",
      " 2649528/3750000: episode: 3563, duration: 2.802s, episode steps: 368, steps per second: 131, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.266 [0.000, 5.000],  loss: 0.013996, mae: 3.196233, mean_q: 3.842388, mean_eps: 0.100000\n",
      " 2650444/3750000: episode: 3564, duration: 6.942s, episode steps: 916, steps per second: 132, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.017659, mae: 3.151316, mean_q: 3.787672, mean_eps: 0.100000\n",
      " 2651291/3750000: episode: 3565, duration: 6.299s, episode steps: 847, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.015154, mae: 3.185807, mean_q: 3.842370, mean_eps: 0.100000\n",
      " 2651800/3750000: episode: 3566, duration: 3.863s, episode steps: 509, steps per second: 132, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.019113, mae: 3.188039, mean_q: 3.840489, mean_eps: 0.100000\n",
      " 2652409/3750000: episode: 3567, duration: 4.685s, episode steps: 609, steps per second: 130, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.018504, mae: 3.142591, mean_q: 3.779922, mean_eps: 0.100000\n",
      " 2653087/3750000: episode: 3568, duration: 5.060s, episode steps: 678, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.429 [0.000, 5.000],  loss: 0.013753, mae: 3.187053, mean_q: 3.832052, mean_eps: 0.100000\n",
      " 2653707/3750000: episode: 3569, duration: 4.640s, episode steps: 620, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.029 [0.000, 5.000],  loss: 0.013239, mae: 3.109421, mean_q: 3.738989, mean_eps: 0.100000\n",
      " 2654361/3750000: episode: 3570, duration: 5.154s, episode steps: 654, steps per second: 127, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.014091, mae: 3.209227, mean_q: 3.867678, mean_eps: 0.100000\n",
      " 2655331/3750000: episode: 3571, duration: 7.198s, episode steps: 970, steps per second: 135, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.434 [0.000, 5.000],  loss: 0.019942, mae: 3.169694, mean_q: 3.816208, mean_eps: 0.100000\n",
      " 2656413/3750000: episode: 3572, duration: 8.195s, episode steps: 1082, steps per second: 132, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.014083, mae: 3.201777, mean_q: 3.852775, mean_eps: 0.100000\n",
      " 2657670/3750000: episode: 3573, duration: 9.385s, episode steps: 1257, steps per second: 134, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.015075, mae: 3.141991, mean_q: 3.779732, mean_eps: 0.100000\n",
      " 2658245/3750000: episode: 3574, duration: 4.335s, episode steps: 575, steps per second: 133, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 0.016799, mae: 3.203678, mean_q: 3.852694, mean_eps: 0.100000\n",
      " 2658839/3750000: episode: 3575, duration: 4.405s, episode steps: 594, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.350 [0.000, 5.000],  loss: 0.019714, mae: 3.190603, mean_q: 3.834043, mean_eps: 0.100000\n",
      " 2659841/3750000: episode: 3576, duration: 7.540s, episode steps: 1002, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.010405, mae: 3.163713, mean_q: 3.804070, mean_eps: 0.100000\n",
      " 2661612/3750000: episode: 3577, duration: 13.159s, episode steps: 1771, steps per second: 135, episode reward: 32.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014709, mae: 3.140056, mean_q: 3.780557, mean_eps: 0.100000\n",
      " 2662940/3750000: episode: 3578, duration: 10.034s, episode steps: 1328, steps per second: 132, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.016557, mae: 3.163015, mean_q: 3.804916, mean_eps: 0.100000\n",
      " 2663832/3750000: episode: 3579, duration: 6.682s, episode steps: 892, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.014764, mae: 3.215876, mean_q: 3.870326, mean_eps: 0.100000\n",
      " 2665062/3750000: episode: 3580, duration: 9.253s, episode steps: 1230, steps per second: 133, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.014372, mae: 3.192822, mean_q: 3.846022, mean_eps: 0.100000\n",
      " 2665902/3750000: episode: 3581, duration: 6.324s, episode steps: 840, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.014946, mae: 3.194146, mean_q: 3.842808, mean_eps: 0.100000\n",
      " 2666603/3750000: episode: 3582, duration: 5.194s, episode steps: 701, steps per second: 135, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.058 [0.000, 5.000],  loss: 0.014986, mae: 3.164909, mean_q: 3.812428, mean_eps: 0.100000\n",
      " 2667618/3750000: episode: 3583, duration: 7.580s, episode steps: 1015, steps per second: 134, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.673 [0.000, 5.000],  loss: 0.019253, mae: 3.109598, mean_q: 3.746532, mean_eps: 0.100000\n",
      " 2668593/3750000: episode: 3584, duration: 7.282s, episode steps: 975, steps per second: 134, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.161 [0.000, 5.000],  loss: 0.014168, mae: 3.133319, mean_q: 3.776254, mean_eps: 0.100000\n",
      " 2669438/3750000: episode: 3585, duration: 6.350s, episode steps: 845, steps per second: 133, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.019053, mae: 3.133164, mean_q: 3.771094, mean_eps: 0.100000\n",
      " 2670333/3750000: episode: 3586, duration: 6.674s, episode steps: 895, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.014775, mae: 3.153420, mean_q: 3.797867, mean_eps: 0.100000\n",
      " 2671123/3750000: episode: 3587, duration: 5.951s, episode steps: 790, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.015420, mae: 3.159404, mean_q: 3.804353, mean_eps: 0.100000\n",
      " 2671953/3750000: episode: 3588, duration: 6.260s, episode steps: 830, steps per second: 133, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.014547, mae: 3.105343, mean_q: 3.735434, mean_eps: 0.100000\n",
      " 2672679/3750000: episode: 3589, duration: 5.417s, episode steps: 726, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.015586, mae: 3.129520, mean_q: 3.767422, mean_eps: 0.100000\n",
      " 2673569/3750000: episode: 3590, duration: 6.741s, episode steps: 890, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.013167, mae: 3.113218, mean_q: 3.743529, mean_eps: 0.100000\n",
      " 2674271/3750000: episode: 3591, duration: 5.206s, episode steps: 702, steps per second: 135, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.012705, mae: 3.135426, mean_q: 3.771137, mean_eps: 0.100000\n",
      " 2674872/3750000: episode: 3592, duration: 4.473s, episode steps: 601, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012638, mae: 3.114943, mean_q: 3.747851, mean_eps: 0.100000\n",
      " 2676211/3750000: episode: 3593, duration: 10.033s, episode steps: 1339, steps per second: 133, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.015337, mae: 3.128301, mean_q: 3.762281, mean_eps: 0.100000\n",
      " 2677104/3750000: episode: 3594, duration: 6.762s, episode steps: 893, steps per second: 132, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.012171, mae: 3.075990, mean_q: 3.697087, mean_eps: 0.100000\n",
      " 2677868/3750000: episode: 3595, duration: 5.747s, episode steps: 764, steps per second: 133, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.016829, mae: 3.094967, mean_q: 3.723172, mean_eps: 0.100000\n",
      " 2678871/3750000: episode: 3596, duration: 7.449s, episode steps: 1003, steps per second: 135, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.013734, mae: 3.126411, mean_q: 3.764901, mean_eps: 0.100000\n",
      " 2679249/3750000: episode: 3597, duration: 2.830s, episode steps: 378, steps per second: 134, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.135 [0.000, 5.000],  loss: 0.014640, mae: 3.124331, mean_q: 3.758779, mean_eps: 0.100000\n",
      " 2680067/3750000: episode: 3598, duration: 6.162s, episode steps: 818, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.597 [0.000, 5.000],  loss: 0.015398, mae: 3.116609, mean_q: 3.748331, mean_eps: 0.100000\n",
      " 2681058/3750000: episode: 3599, duration: 7.343s, episode steps: 991, steps per second: 135, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017647, mae: 3.109362, mean_q: 3.741515, mean_eps: 0.100000\n",
      " 2681502/3750000: episode: 3600, duration: 3.398s, episode steps: 444, steps per second: 131, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.065 [0.000, 5.000],  loss: 0.020368, mae: 3.154923, mean_q: 3.794081, mean_eps: 0.100000\n",
      " 2682600/3750000: episode: 3601, duration: 8.304s, episode steps: 1098, steps per second: 132, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.632 [0.000, 5.000],  loss: 0.014392, mae: 3.092403, mean_q: 3.719565, mean_eps: 0.100000\n",
      " 2683259/3750000: episode: 3602, duration: 4.981s, episode steps: 659, steps per second: 132, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.017890, mae: 3.105813, mean_q: 3.749571, mean_eps: 0.100000\n",
      " 2684280/3750000: episode: 3603, duration: 7.756s, episode steps: 1021, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.281 [0.000, 5.000],  loss: 0.013380, mae: 3.204103, mean_q: 3.854125, mean_eps: 0.100000\n",
      " 2685238/3750000: episode: 3604, duration: 7.179s, episode steps: 958, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.011414, mae: 3.205791, mean_q: 3.856044, mean_eps: 0.100000\n",
      " 2686143/3750000: episode: 3605, duration: 6.862s, episode steps: 905, steps per second: 132, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 4.052 [0.000, 5.000],  loss: 0.020193, mae: 3.193853, mean_q: 3.840923, mean_eps: 0.100000\n",
      " 2687141/3750000: episode: 3606, duration: 7.496s, episode steps: 998, steps per second: 133, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.013904, mae: 3.174228, mean_q: 3.818890, mean_eps: 0.100000\n",
      " 2687712/3750000: episode: 3607, duration: 4.251s, episode steps: 571, steps per second: 134, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.013354, mae: 3.199903, mean_q: 3.856016, mean_eps: 0.100000\n",
      " 2688229/3750000: episode: 3608, duration: 4.008s, episode steps: 517, steps per second: 129, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.878 [0.000, 5.000],  loss: 0.017728, mae: 3.178719, mean_q: 3.822579, mean_eps: 0.100000\n",
      " 2689346/3750000: episode: 3609, duration: 8.409s, episode steps: 1117, steps per second: 133, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.357 [0.000, 5.000],  loss: 0.014317, mae: 3.163059, mean_q: 3.810680, mean_eps: 0.100000\n",
      " 2690187/3750000: episode: 3610, duration: 6.267s, episode steps: 841, steps per second: 134, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.015030, mae: 3.205834, mean_q: 3.857213, mean_eps: 0.100000\n",
      " 2691032/3750000: episode: 3611, duration: 6.346s, episode steps: 845, steps per second: 133, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.256 [0.000, 5.000],  loss: 0.015869, mae: 3.173976, mean_q: 3.820085, mean_eps: 0.100000\n",
      " 2692258/3750000: episode: 3612, duration: 9.072s, episode steps: 1226, steps per second: 135, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.966 [0.000, 5.000],  loss: 0.017219, mae: 3.169207, mean_q: 3.814746, mean_eps: 0.100000\n",
      " 2692860/3750000: episode: 3613, duration: 4.647s, episode steps: 602, steps per second: 130, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.736 [0.000, 5.000],  loss: 0.015931, mae: 3.209817, mean_q: 3.863412, mean_eps: 0.100000\n",
      " 2693798/3750000: episode: 3614, duration: 6.979s, episode steps: 938, steps per second: 134, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.016487, mae: 3.166552, mean_q: 3.814597, mean_eps: 0.100000\n",
      " 2694426/3750000: episode: 3615, duration: 4.739s, episode steps: 628, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.374 [0.000, 5.000],  loss: 0.013112, mae: 3.233310, mean_q: 3.890826, mean_eps: 0.100000\n",
      " 2695238/3750000: episode: 3616, duration: 6.073s, episode steps: 812, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.016361, mae: 3.196332, mean_q: 3.848411, mean_eps: 0.100000\n",
      " 2696039/3750000: episode: 3617, duration: 6.089s, episode steps: 801, steps per second: 132, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.014515, mae: 3.161545, mean_q: 3.807161, mean_eps: 0.100000\n",
      " 2696529/3750000: episode: 3618, duration: 3.715s, episode steps: 490, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.014324, mae: 3.200176, mean_q: 3.855931, mean_eps: 0.100000\n",
      " 2697222/3750000: episode: 3619, duration: 5.278s, episode steps: 693, steps per second: 131, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.015027, mae: 3.153062, mean_q: 3.802796, mean_eps: 0.100000\n",
      " 2697673/3750000: episode: 3620, duration: 3.335s, episode steps: 451, steps per second: 135, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.707 [0.000, 5.000],  loss: 0.015062, mae: 3.197314, mean_q: 3.843082, mean_eps: 0.100000\n",
      " 2698076/3750000: episode: 3621, duration: 3.024s, episode steps: 403, steps per second: 133, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.020111, mae: 3.167500, mean_q: 3.808485, mean_eps: 0.100000\n",
      " 2699238/3750000: episode: 3622, duration: 8.738s, episode steps: 1162, steps per second: 133, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.016564, mae: 3.224006, mean_q: 3.884946, mean_eps: 0.100000\n",
      " 2699851/3750000: episode: 3623, duration: 4.653s, episode steps: 613, steps per second: 132, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.465 [0.000, 5.000],  loss: 0.017332, mae: 3.173431, mean_q: 3.825417, mean_eps: 0.100000\n",
      " 2700728/3750000: episode: 3624, duration: 6.562s, episode steps: 877, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.670 [0.000, 5.000],  loss: 0.015285, mae: 3.205936, mean_q: 3.857734, mean_eps: 0.100000\n",
      " 2701583/3750000: episode: 3625, duration: 6.462s, episode steps: 855, steps per second: 132, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.283 [0.000, 5.000],  loss: 0.016971, mae: 3.183034, mean_q: 3.829788, mean_eps: 0.100000\n",
      " 2702234/3750000: episode: 3626, duration: 4.823s, episode steps: 651, steps per second: 135, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.014812, mae: 3.209806, mean_q: 3.864591, mean_eps: 0.100000\n",
      " 2703109/3750000: episode: 3627, duration: 6.669s, episode steps: 875, steps per second: 131, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.931 [0.000, 5.000],  loss: 0.014273, mae: 3.151313, mean_q: 3.793810, mean_eps: 0.100000\n",
      " 2703451/3750000: episode: 3628, duration: 2.584s, episode steps: 342, steps per second: 132, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.014163, mae: 3.212820, mean_q: 3.862410, mean_eps: 0.100000\n",
      " 2704217/3750000: episode: 3629, duration: 5.800s, episode steps: 766, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.531 [0.000, 5.000],  loss: 0.019538, mae: 3.172633, mean_q: 3.822462, mean_eps: 0.100000\n",
      " 2704842/3750000: episode: 3630, duration: 4.722s, episode steps: 625, steps per second: 132, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.829 [0.000, 5.000],  loss: 0.012350, mae: 3.180326, mean_q: 3.826574, mean_eps: 0.100000\n",
      " 2705436/3750000: episode: 3631, duration: 4.463s, episode steps: 594, steps per second: 133, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.379 [0.000, 5.000],  loss: 0.029615, mae: 3.181158, mean_q: 3.822442, mean_eps: 0.100000\n",
      " 2706116/3750000: episode: 3632, duration: 5.080s, episode steps: 680, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013033, mae: 3.134908, mean_q: 3.767000, mean_eps: 0.100000\n",
      " 2706642/3750000: episode: 3633, duration: 3.969s, episode steps: 526, steps per second: 133, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.871 [0.000, 5.000],  loss: 0.014825, mae: 3.253972, mean_q: 3.913673, mean_eps: 0.100000\n",
      " 2707933/3750000: episode: 3634, duration: 9.620s, episode steps: 1291, steps per second: 134, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.015207, mae: 3.178578, mean_q: 3.830569, mean_eps: 0.100000\n",
      " 2708857/3750000: episode: 3635, duration: 6.904s, episode steps: 924, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014797, mae: 3.196512, mean_q: 3.846322, mean_eps: 0.100000\n",
      " 2710019/3750000: episode: 3636, duration: 8.725s, episode steps: 1162, steps per second: 133, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.177 [0.000, 5.000],  loss: 0.015306, mae: 3.191481, mean_q: 3.836747, mean_eps: 0.100000\n",
      " 2710573/3750000: episode: 3637, duration: 4.195s, episode steps: 554, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.307 [0.000, 5.000],  loss: 0.011456, mae: 3.191457, mean_q: 3.837309, mean_eps: 0.100000\n",
      " 2711209/3750000: episode: 3638, duration: 4.769s, episode steps: 636, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.011872, mae: 3.132265, mean_q: 3.768599, mean_eps: 0.100000\n",
      " 2712136/3750000: episode: 3639, duration: 6.961s, episode steps: 927, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.880 [0.000, 5.000],  loss: 0.014480, mae: 3.149189, mean_q: 3.790731, mean_eps: 0.100000\n",
      " 2713179/3750000: episode: 3640, duration: 7.736s, episode steps: 1043, steps per second: 135, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.014664, mae: 3.194647, mean_q: 3.842525, mean_eps: 0.100000\n",
      " 2714201/3750000: episode: 3641, duration: 7.744s, episode steps: 1022, steps per second: 132, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.642 [0.000, 5.000],  loss: 0.016451, mae: 3.122878, mean_q: 3.753495, mean_eps: 0.100000\n",
      " 2715168/3750000: episode: 3642, duration: 7.198s, episode steps: 967, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.013007, mae: 3.125251, mean_q: 3.756416, mean_eps: 0.100000\n",
      " 2716294/3750000: episode: 3643, duration: 8.475s, episode steps: 1126, steps per second: 133, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.182 [0.000, 5.000],  loss: 0.018658, mae: 3.157046, mean_q: 3.796882, mean_eps: 0.100000\n",
      " 2716864/3750000: episode: 3644, duration: 4.318s, episode steps: 570, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.795 [0.000, 5.000],  loss: 0.016041, mae: 3.142298, mean_q: 3.783241, mean_eps: 0.100000\n",
      " 2717456/3750000: episode: 3645, duration: 4.360s, episode steps: 592, steps per second: 136, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.011479, mae: 3.150261, mean_q: 3.797584, mean_eps: 0.100000\n",
      " 2718230/3750000: episode: 3646, duration: 5.863s, episode steps: 774, steps per second: 132, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.014067, mae: 3.090323, mean_q: 3.713946, mean_eps: 0.100000\n",
      " 2719062/3750000: episode: 3647, duration: 6.238s, episode steps: 832, steps per second: 133, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.014937, mae: 3.099584, mean_q: 3.727503, mean_eps: 0.100000\n",
      " 2720229/3750000: episode: 3648, duration: 8.759s, episode steps: 1167, steps per second: 133, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.017964, mae: 3.081573, mean_q: 3.705189, mean_eps: 0.100000\n",
      " 2720840/3750000: episode: 3649, duration: 4.609s, episode steps: 611, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.010821, mae: 3.074199, mean_q: 3.699052, mean_eps: 0.100000\n",
      " 2721754/3750000: episode: 3650, duration: 6.861s, episode steps: 914, steps per second: 133, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.565 [0.000, 5.000],  loss: 0.012496, mae: 3.060333, mean_q: 3.678749, mean_eps: 0.100000\n",
      " 2722430/3750000: episode: 3651, duration: 5.125s, episode steps: 676, steps per second: 132, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.126 [0.000, 5.000],  loss: 0.016940, mae: 3.097029, mean_q: 3.723166, mean_eps: 0.100000\n",
      " 2723333/3750000: episode: 3652, duration: 6.741s, episode steps: 903, steps per second: 134, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.877 [0.000, 5.000],  loss: 0.015411, mae: 3.050815, mean_q: 3.669422, mean_eps: 0.100000\n",
      " 2724235/3750000: episode: 3653, duration: 6.745s, episode steps: 902, steps per second: 134, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.044 [0.000, 5.000],  loss: 0.014746, mae: 3.104763, mean_q: 3.734980, mean_eps: 0.100000\n",
      " 2724731/3750000: episode: 3654, duration: 3.774s, episode steps: 496, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.010626, mae: 3.157813, mean_q: 3.794566, mean_eps: 0.100000\n",
      " 2725920/3750000: episode: 3655, duration: 8.903s, episode steps: 1189, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.861 [0.000, 5.000],  loss: 0.012887, mae: 3.049746, mean_q: 3.669269, mean_eps: 0.100000\n",
      " 2726885/3750000: episode: 3656, duration: 7.334s, episode steps: 965, steps per second: 132, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.013311, mae: 3.035521, mean_q: 3.655817, mean_eps: 0.100000\n",
      " 2727688/3750000: episode: 3657, duration: 5.995s, episode steps: 803, steps per second: 134, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.324 [0.000, 5.000],  loss: 0.015119, mae: 3.051040, mean_q: 3.667101, mean_eps: 0.100000\n",
      " 2728371/3750000: episode: 3658, duration: 5.161s, episode steps: 683, steps per second: 132, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.015843, mae: 3.059755, mean_q: 3.677960, mean_eps: 0.100000\n",
      " 2729680/3750000: episode: 3659, duration: 9.829s, episode steps: 1309, steps per second: 133, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.014820, mae: 3.010684, mean_q: 3.619908, mean_eps: 0.100000\n",
      " 2730478/3750000: episode: 3660, duration: 5.956s, episode steps: 798, steps per second: 134, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 4.013 [0.000, 5.000],  loss: 0.014001, mae: 3.027659, mean_q: 3.653657, mean_eps: 0.100000\n",
      " 2731188/3750000: episode: 3661, duration: 5.407s, episode steps: 710, steps per second: 131, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.880 [0.000, 5.000],  loss: 0.011766, mae: 2.978276, mean_q: 3.583937, mean_eps: 0.100000\n",
      " 2731721/3750000: episode: 3662, duration: 4.024s, episode steps: 533, steps per second: 132, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.865 [0.000, 5.000],  loss: 0.012783, mae: 3.023944, mean_q: 3.637734, mean_eps: 0.100000\n",
      " 2732361/3750000: episode: 3663, duration: 4.785s, episode steps: 640, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.010525, mae: 2.988132, mean_q: 3.597002, mean_eps: 0.100000\n",
      " 2733018/3750000: episode: 3664, duration: 4.929s, episode steps: 657, steps per second: 133, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.013082, mae: 3.035557, mean_q: 3.655111, mean_eps: 0.100000\n",
      " 2734068/3750000: episode: 3665, duration: 7.853s, episode steps: 1050, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.173 [0.000, 5.000],  loss: 0.012896, mae: 3.048826, mean_q: 3.666174, mean_eps: 0.100000\n",
      " 2734713/3750000: episode: 3666, duration: 4.747s, episode steps: 645, steps per second: 136, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.078 [0.000, 5.000],  loss: 0.011207, mae: 3.009057, mean_q: 3.619142, mean_eps: 0.100000\n",
      " 2735747/3750000: episode: 3667, duration: 7.895s, episode steps: 1034, steps per second: 131, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.942 [0.000, 5.000],  loss: 0.016215, mae: 3.031506, mean_q: 3.644660, mean_eps: 0.100000\n",
      " 2736456/3750000: episode: 3668, duration: 5.310s, episode steps: 709, steps per second: 134, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.142 [0.000, 5.000],  loss: 0.009830, mae: 2.995212, mean_q: 3.610361, mean_eps: 0.100000\n",
      " 2737210/3750000: episode: 3669, duration: 5.687s, episode steps: 754, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.018287, mae: 3.022966, mean_q: 3.636120, mean_eps: 0.100000\n",
      " 2737869/3750000: episode: 3670, duration: 4.943s, episode steps: 659, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.476 [0.000, 5.000],  loss: 0.018421, mae: 3.072466, mean_q: 3.695241, mean_eps: 0.100000\n",
      " 2739058/3750000: episode: 3671, duration: 8.819s, episode steps: 1189, steps per second: 135, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.524 [0.000, 5.000],  loss: 0.013387, mae: 3.062371, mean_q: 3.683680, mean_eps: 0.100000\n",
      " 2739680/3750000: episode: 3672, duration: 4.776s, episode steps: 622, steps per second: 130, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.487 [0.000, 5.000],  loss: 0.014106, mae: 3.028038, mean_q: 3.637354, mean_eps: 0.100000\n",
      " 2740969/3750000: episode: 3673, duration: 9.692s, episode steps: 1289, steps per second: 133, episode reward: 25.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.916 [0.000, 5.000],  loss: 0.013690, mae: 3.053701, mean_q: 3.672823, mean_eps: 0.100000\n",
      " 2741457/3750000: episode: 3674, duration: 3.636s, episode steps: 488, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.016793, mae: 3.051780, mean_q: 3.674393, mean_eps: 0.100000\n",
      " 2742251/3750000: episode: 3675, duration: 5.965s, episode steps: 794, steps per second: 133, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.793 [0.000, 5.000],  loss: 0.017904, mae: 3.008233, mean_q: 3.620228, mean_eps: 0.100000\n",
      " 2743302/3750000: episode: 3676, duration: 7.850s, episode steps: 1051, steps per second: 134, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.282 [0.000, 5.000],  loss: 0.015624, mae: 3.042222, mean_q: 3.658821, mean_eps: 0.100000\n",
      " 2744320/3750000: episode: 3677, duration: 7.645s, episode steps: 1018, steps per second: 133, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.014952, mae: 3.060626, mean_q: 3.680648, mean_eps: 0.100000\n",
      " 2745283/3750000: episode: 3678, duration: 7.302s, episode steps: 963, steps per second: 132, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.017606, mae: 3.054077, mean_q: 3.674008, mean_eps: 0.100000\n",
      " 2745711/3750000: episode: 3679, duration: 3.208s, episode steps: 428, steps per second: 133, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.017313, mae: 3.050694, mean_q: 3.668019, mean_eps: 0.100000\n",
      " 2746551/3750000: episode: 3680, duration: 6.287s, episode steps: 840, steps per second: 134, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.013770, mae: 3.080906, mean_q: 3.710652, mean_eps: 0.100000\n",
      " 2747378/3750000: episode: 3681, duration: 6.106s, episode steps: 827, steps per second: 135, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.012545, mae: 3.058398, mean_q: 3.675753, mean_eps: 0.100000\n",
      " 2747978/3750000: episode: 3682, duration: 4.537s, episode steps: 600, steps per second: 132, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.016276, mae: 3.044797, mean_q: 3.660538, mean_eps: 0.100000\n",
      " 2748611/3750000: episode: 3683, duration: 4.770s, episode steps: 633, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.020290, mae: 3.099012, mean_q: 3.736159, mean_eps: 0.100000\n",
      " 2749378/3750000: episode: 3684, duration: 5.687s, episode steps: 767, steps per second: 135, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.014425, mae: 3.096968, mean_q: 3.720698, mean_eps: 0.100000\n",
      " 2750317/3750000: episode: 3685, duration: 7.211s, episode steps: 939, steps per second: 130, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.015836, mae: 3.086023, mean_q: 3.708931, mean_eps: 0.100000\n",
      " 2751239/3750000: episode: 3686, duration: 6.927s, episode steps: 922, steps per second: 133, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 0.012629, mae: 3.040647, mean_q: 3.667180, mean_eps: 0.100000\n",
      " 2752095/3750000: episode: 3687, duration: 6.525s, episode steps: 856, steps per second: 131, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013575, mae: 3.061051, mean_q: 3.684352, mean_eps: 0.100000\n",
      " 2752903/3750000: episode: 3688, duration: 6.112s, episode steps: 808, steps per second: 132, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.328 [0.000, 5.000],  loss: 0.013167, mae: 3.032453, mean_q: 3.645882, mean_eps: 0.100000\n",
      " 2753348/3750000: episode: 3689, duration: 3.309s, episode steps: 445, steps per second: 135, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.019423, mae: 3.039361, mean_q: 3.656658, mean_eps: 0.100000\n",
      " 2754024/3750000: episode: 3690, duration: 5.074s, episode steps: 676, steps per second: 133, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.138 [0.000, 5.000],  loss: 0.014644, mae: 3.026095, mean_q: 3.647741, mean_eps: 0.100000\n",
      " 2754833/3750000: episode: 3691, duration: 6.068s, episode steps: 809, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.019712, mae: 2.986067, mean_q: 3.589189, mean_eps: 0.100000\n",
      " 2755933/3750000: episode: 3692, duration: 8.175s, episode steps: 1100, steps per second: 135, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.014515, mae: 2.983995, mean_q: 3.593299, mean_eps: 0.100000\n",
      " 2756784/3750000: episode: 3693, duration: 6.481s, episode steps: 851, steps per second: 131, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.016343, mae: 2.980467, mean_q: 3.588221, mean_eps: 0.100000\n",
      " 2757148/3750000: episode: 3694, duration: 2.709s, episode steps: 364, steps per second: 134, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.012722, mae: 2.990170, mean_q: 3.600762, mean_eps: 0.100000\n",
      " 2757702/3750000: episode: 3695, duration: 4.149s, episode steps: 554, steps per second: 134, episode reward: 20.000, mean reward:  0.036 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.015817, mae: 2.934248, mean_q: 3.529139, mean_eps: 0.100000\n",
      " 2758322/3750000: episode: 3696, duration: 4.602s, episode steps: 620, steps per second: 135, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.013027, mae: 2.962640, mean_q: 3.572750, mean_eps: 0.100000\n",
      " 2759090/3750000: episode: 3697, duration: 5.767s, episode steps: 768, steps per second: 133, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.471 [0.000, 5.000],  loss: 0.018237, mae: 2.980485, mean_q: 3.592109, mean_eps: 0.100000\n",
      " 2759940/3750000: episode: 3698, duration: 6.334s, episode steps: 850, steps per second: 134, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.013726, mae: 2.989002, mean_q: 3.599137, mean_eps: 0.100000\n",
      " 2760951/3750000: episode: 3699, duration: 7.643s, episode steps: 1011, steps per second: 132, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.012199, mae: 2.940715, mean_q: 3.538123, mean_eps: 0.100000\n",
      " 2761662/3750000: episode: 3700, duration: 5.314s, episode steps: 711, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.729 [0.000, 5.000],  loss: 0.014777, mae: 3.005807, mean_q: 3.621921, mean_eps: 0.100000\n",
      " 2762421/3750000: episode: 3701, duration: 5.720s, episode steps: 759, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.859 [0.000, 5.000],  loss: 0.011905, mae: 3.017283, mean_q: 3.636615, mean_eps: 0.100000\n",
      " 2762964/3750000: episode: 3702, duration: 4.123s, episode steps: 543, steps per second: 132, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.901 [0.000, 5.000],  loss: 0.013077, mae: 3.012642, mean_q: 3.626353, mean_eps: 0.100000\n",
      " 2763639/3750000: episode: 3703, duration: 5.014s, episode steps: 675, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.009809, mae: 3.026579, mean_q: 3.650074, mean_eps: 0.100000\n",
      " 2764445/3750000: episode: 3704, duration: 6.036s, episode steps: 806, steps per second: 134, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.548 [0.000, 5.000],  loss: 0.012617, mae: 2.949903, mean_q: 3.550945, mean_eps: 0.100000\n",
      " 2765376/3750000: episode: 3705, duration: 6.967s, episode steps: 931, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.013723, mae: 3.021858, mean_q: 3.636308, mean_eps: 0.100000\n",
      " 2766443/3750000: episode: 3706, duration: 7.964s, episode steps: 1067, steps per second: 134, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.082 [0.000, 5.000],  loss: 0.016759, mae: 3.023959, mean_q: 3.639475, mean_eps: 0.100000\n",
      " 2767471/3750000: episode: 3707, duration: 7.680s, episode steps: 1028, steps per second: 134, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.010835, mae: 3.024429, mean_q: 3.638898, mean_eps: 0.100000\n",
      " 2768360/3750000: episode: 3708, duration: 6.770s, episode steps: 889, steps per second: 131, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.012160, mae: 2.997428, mean_q: 3.610101, mean_eps: 0.100000\n",
      " 2769333/3750000: episode: 3709, duration: 7.323s, episode steps: 973, steps per second: 133, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.390 [0.000, 5.000],  loss: 0.013821, mae: 2.951832, mean_q: 3.552420, mean_eps: 0.100000\n",
      " 2770573/3750000: episode: 3710, duration: 9.201s, episode steps: 1240, steps per second: 135, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.406 [0.000, 5.000],  loss: 0.016885, mae: 2.977018, mean_q: 3.586229, mean_eps: 0.100000\n",
      " 2771255/3750000: episode: 3711, duration: 5.098s, episode steps: 682, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.018495, mae: 2.970558, mean_q: 3.578183, mean_eps: 0.100000\n",
      " 2772580/3750000: episode: 3712, duration: 9.945s, episode steps: 1325, steps per second: 133, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.012169, mae: 2.994325, mean_q: 3.607725, mean_eps: 0.100000\n",
      " 2773722/3750000: episode: 3713, duration: 8.630s, episode steps: 1142, steps per second: 132, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.013862, mae: 3.009839, mean_q: 3.625490, mean_eps: 0.100000\n",
      " 2774998/3750000: episode: 3714, duration: 9.382s, episode steps: 1276, steps per second: 136, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.015333, mae: 3.003555, mean_q: 3.614614, mean_eps: 0.100000\n",
      " 2775796/3750000: episode: 3715, duration: 6.048s, episode steps: 798, steps per second: 132, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.010427, mae: 3.013031, mean_q: 3.630780, mean_eps: 0.100000\n",
      " 2776877/3750000: episode: 3716, duration: 8.089s, episode steps: 1081, steps per second: 134, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.013296, mae: 3.034918, mean_q: 3.654702, mean_eps: 0.100000\n",
      " 2777714/3750000: episode: 3717, duration: 6.350s, episode steps: 837, steps per second: 132, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.017183, mae: 3.052751, mean_q: 3.669635, mean_eps: 0.100000\n",
      " 2778270/3750000: episode: 3718, duration: 4.160s, episode steps: 556, steps per second: 134, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.482 [0.000, 5.000],  loss: 0.012104, mae: 3.088893, mean_q: 3.717148, mean_eps: 0.100000\n",
      " 2779634/3750000: episode: 3719, duration: 10.123s, episode steps: 1364, steps per second: 135, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.014061, mae: 3.018520, mean_q: 3.629193, mean_eps: 0.100000\n",
      " 2780471/3750000: episode: 3720, duration: 6.291s, episode steps: 837, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.109 [0.000, 5.000],  loss: 0.015023, mae: 2.969323, mean_q: 3.573920, mean_eps: 0.100000\n",
      " 2781397/3750000: episode: 3721, duration: 6.878s, episode steps: 926, steps per second: 135, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.010813, mae: 3.030803, mean_q: 3.647576, mean_eps: 0.100000\n",
      " 2781964/3750000: episode: 3722, duration: 4.311s, episode steps: 567, steps per second: 132, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.013626, mae: 3.074138, mean_q: 3.702138, mean_eps: 0.100000\n",
      " 2782997/3750000: episode: 3723, duration: 7.637s, episode steps: 1033, steps per second: 135, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.014087, mae: 3.030329, mean_q: 3.643539, mean_eps: 0.100000\n",
      " 2783681/3750000: episode: 3724, duration: 5.194s, episode steps: 684, steps per second: 132, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.800 [0.000, 5.000],  loss: 0.016087, mae: 2.993388, mean_q: 3.607116, mean_eps: 0.100000\n",
      " 2784515/3750000: episode: 3725, duration: 6.334s, episode steps: 834, steps per second: 132, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.400 [0.000, 5.000],  loss: 0.010756, mae: 3.006727, mean_q: 3.625252, mean_eps: 0.100000\n",
      " 2784906/3750000: episode: 3726, duration: 2.953s, episode steps: 391, steps per second: 132, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.437 [0.000, 5.000],  loss: 0.019157, mae: 3.009418, mean_q: 3.615652, mean_eps: 0.100000\n",
      " 2785418/3750000: episode: 3727, duration: 3.804s, episode steps: 512, steps per second: 135, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 4.334 [0.000, 5.000],  loss: 0.022196, mae: 3.011553, mean_q: 3.623767, mean_eps: 0.100000\n",
      " 2786397/3750000: episode: 3728, duration: 7.376s, episode steps: 979, steps per second: 133, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.015019, mae: 3.003250, mean_q: 3.611168, mean_eps: 0.100000\n",
      " 2786764/3750000: episode: 3729, duration: 2.806s, episode steps: 367, steps per second: 131, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.191 [0.000, 5.000],  loss: 0.013688, mae: 2.997916, mean_q: 3.607989, mean_eps: 0.100000\n",
      " 2787414/3750000: episode: 3730, duration: 4.806s, episode steps: 650, steps per second: 135, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.013650, mae: 3.048696, mean_q: 3.669982, mean_eps: 0.100000\n",
      " 2788031/3750000: episode: 3731, duration: 4.623s, episode steps: 617, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.017361, mae: 3.015004, mean_q: 3.628062, mean_eps: 0.100000\n",
      " 2788974/3750000: episode: 3732, duration: 7.069s, episode steps: 943, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.011119, mae: 3.054960, mean_q: 3.684121, mean_eps: 0.100000\n",
      " 2789791/3750000: episode: 3733, duration: 6.121s, episode steps: 817, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.015576, mae: 3.044482, mean_q: 3.661683, mean_eps: 0.100000\n",
      " 2790145/3750000: episode: 3734, duration: 2.674s, episode steps: 354, steps per second: 132, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.636 [0.000, 5.000],  loss: 0.013683, mae: 3.080545, mean_q: 3.712656, mean_eps: 0.100000\n",
      " 2791431/3750000: episode: 3735, duration: 9.542s, episode steps: 1286, steps per second: 135, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.014640, mae: 3.033198, mean_q: 3.646982, mean_eps: 0.100000\n",
      " 2792076/3750000: episode: 3736, duration: 4.784s, episode steps: 645, steps per second: 135, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.481 [0.000, 5.000],  loss: 0.014410, mae: 3.051011, mean_q: 3.667876, mean_eps: 0.100000\n",
      " 2793266/3750000: episode: 3737, duration: 8.928s, episode steps: 1190, steps per second: 133, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.012677, mae: 3.058203, mean_q: 3.679443, mean_eps: 0.100000\n",
      " 2794123/3750000: episode: 3738, duration: 6.369s, episode steps: 857, steps per second: 135, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.923 [0.000, 5.000],  loss: 0.019846, mae: 3.052848, mean_q: 3.672369, mean_eps: 0.100000\n",
      " 2794954/3750000: episode: 3739, duration: 6.184s, episode steps: 831, steps per second: 134, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.013461, mae: 3.014025, mean_q: 3.634217, mean_eps: 0.100000\n",
      " 2795712/3750000: episode: 3740, duration: 5.699s, episode steps: 758, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013399, mae: 3.089739, mean_q: 3.715533, mean_eps: 0.100000\n",
      " 2796399/3750000: episode: 3741, duration: 5.101s, episode steps: 687, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.571 [0.000, 5.000],  loss: 0.016868, mae: 3.049259, mean_q: 3.664674, mean_eps: 0.100000\n",
      " 2797775/3750000: episode: 3742, duration: 10.317s, episode steps: 1376, steps per second: 133, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.020394, mae: 3.091712, mean_q: 3.718988, mean_eps: 0.100000\n",
      " 2799110/3750000: episode: 3743, duration: 10.035s, episode steps: 1335, steps per second: 133, episode reward: 22.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.148 [0.000, 5.000],  loss: 0.011870, mae: 3.041397, mean_q: 3.663499, mean_eps: 0.100000\n",
      " 2799729/3750000: episode: 3744, duration: 4.686s, episode steps: 619, steps per second: 132, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.017383, mae: 3.044035, mean_q: 3.667690, mean_eps: 0.100000\n",
      " 2800667/3750000: episode: 3745, duration: 7.048s, episode steps: 938, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.032 [0.000, 5.000],  loss: 0.014594, mae: 3.089458, mean_q: 3.714909, mean_eps: 0.100000\n",
      " 2801327/3750000: episode: 3746, duration: 5.034s, episode steps: 660, steps per second: 131, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.015073, mae: 3.081457, mean_q: 3.713987, mean_eps: 0.100000\n",
      " 2801687/3750000: episode: 3747, duration: 2.700s, episode steps: 360, steps per second: 133, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.369 [0.000, 5.000],  loss: 0.012629, mae: 3.020675, mean_q: 3.641927, mean_eps: 0.100000\n",
      " 2802882/3750000: episode: 3748, duration: 8.939s, episode steps: 1195, steps per second: 134, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.016530, mae: 3.127124, mean_q: 3.763131, mean_eps: 0.100000\n",
      " 2803400/3750000: episode: 3749, duration: 3.931s, episode steps: 518, steps per second: 132, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.017506, mae: 3.127606, mean_q: 3.757913, mean_eps: 0.100000\n",
      " 2804157/3750000: episode: 3750, duration: 5.659s, episode steps: 757, steps per second: 134, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.018038, mae: 3.027389, mean_q: 3.643961, mean_eps: 0.100000\n",
      " 2805602/3750000: episode: 3751, duration: 10.835s, episode steps: 1445, steps per second: 133, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.014396, mae: 3.048276, mean_q: 3.671163, mean_eps: 0.100000\n",
      " 2806241/3750000: episode: 3752, duration: 4.797s, episode steps: 639, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.012997, mae: 3.066735, mean_q: 3.689637, mean_eps: 0.100000\n",
      " 2806864/3750000: episode: 3753, duration: 4.629s, episode steps: 623, steps per second: 135, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.353 [0.000, 5.000],  loss: 0.018313, mae: 3.053925, mean_q: 3.676907, mean_eps: 0.100000\n",
      " 2807580/3750000: episode: 3754, duration: 5.380s, episode steps: 716, steps per second: 133, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.018597, mae: 3.128065, mean_q: 3.767049, mean_eps: 0.100000\n",
      " 2808614/3750000: episode: 3755, duration: 7.785s, episode steps: 1034, steps per second: 133, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.236 [0.000, 5.000],  loss: 0.015582, mae: 3.045445, mean_q: 3.664058, mean_eps: 0.100000\n",
      " 2809706/3750000: episode: 3756, duration: 8.199s, episode steps: 1092, steps per second: 133, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.013377, mae: 3.036220, mean_q: 3.654402, mean_eps: 0.100000\n",
      " 2810263/3750000: episode: 3757, duration: 4.224s, episode steps: 557, steps per second: 132, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.013020, mae: 3.042565, mean_q: 3.659620, mean_eps: 0.100000\n",
      " 2811153/3750000: episode: 3758, duration: 6.589s, episode steps: 890, steps per second: 135, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013211, mae: 2.997894, mean_q: 3.604722, mean_eps: 0.100000\n",
      " 2812060/3750000: episode: 3759, duration: 6.862s, episode steps: 907, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.011804, mae: 3.073493, mean_q: 3.695839, mean_eps: 0.100000\n",
      " 2812606/3750000: episode: 3760, duration: 4.145s, episode steps: 546, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.167 [0.000, 5.000],  loss: 0.012379, mae: 3.077261, mean_q: 3.701206, mean_eps: 0.100000\n",
      " 2813391/3750000: episode: 3761, duration: 5.816s, episode steps: 785, steps per second: 135, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.111 [0.000, 5.000],  loss: 0.013565, mae: 3.102709, mean_q: 3.731835, mean_eps: 0.100000\n",
      " 2813878/3750000: episode: 3762, duration: 3.610s, episode steps: 487, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.117 [0.000, 5.000],  loss: 0.017759, mae: 3.075333, mean_q: 3.705196, mean_eps: 0.100000\n",
      " 2814493/3750000: episode: 3763, duration: 5.093s, episode steps: 615, steps per second: 121, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.351 [0.000, 5.000],  loss: 0.013450, mae: 3.064422, mean_q: 3.687676, mean_eps: 0.100000\n",
      " 2815250/3750000: episode: 3764, duration: 5.685s, episode steps: 757, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.433 [0.000, 5.000],  loss: 0.012672, mae: 3.089212, mean_q: 3.720207, mean_eps: 0.100000\n",
      " 2816176/3750000: episode: 3765, duration: 6.989s, episode steps: 926, steps per second: 132, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.010475, mae: 3.075245, mean_q: 3.705917, mean_eps: 0.100000\n",
      " 2817182/3750000: episode: 3766, duration: 7.546s, episode steps: 1006, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.018493, mae: 3.089058, mean_q: 3.716721, mean_eps: 0.100000\n",
      " 2818045/3750000: episode: 3767, duration: 6.390s, episode steps: 863, steps per second: 135, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.829 [0.000, 5.000],  loss: 0.013068, mae: 3.088227, mean_q: 3.718130, mean_eps: 0.100000\n",
      " 2818979/3750000: episode: 3768, duration: 6.958s, episode steps: 934, steps per second: 134, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.013383, mae: 3.051201, mean_q: 3.672022, mean_eps: 0.100000\n",
      " 2819993/3750000: episode: 3769, duration: 7.553s, episode steps: 1014, steps per second: 134, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.014802, mae: 3.037026, mean_q: 3.653905, mean_eps: 0.100000\n",
      " 2820918/3750000: episode: 3770, duration: 6.934s, episode steps: 925, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.286 [0.000, 5.000],  loss: 0.016813, mae: 3.012053, mean_q: 3.624630, mean_eps: 0.100000\n",
      " 2821443/3750000: episode: 3771, duration: 3.962s, episode steps: 525, steps per second: 133, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.008929, mae: 3.029486, mean_q: 3.651463, mean_eps: 0.100000\n",
      " 2822133/3750000: episode: 3772, duration: 5.109s, episode steps: 690, steps per second: 135, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.014082, mae: 3.019461, mean_q: 3.637857, mean_eps: 0.100000\n",
      " 2822949/3750000: episode: 3773, duration: 6.324s, episode steps: 816, steps per second: 129, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.018071, mae: 3.033474, mean_q: 3.647851, mean_eps: 0.100000\n",
      " 2823920/3750000: episode: 3774, duration: 7.249s, episode steps: 971, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.011975, mae: 3.009330, mean_q: 3.622627, mean_eps: 0.100000\n",
      " 2824684/3750000: episode: 3775, duration: 5.811s, episode steps: 764, steps per second: 131, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.013642, mae: 3.099755, mean_q: 3.732451, mean_eps: 0.100000\n",
      " 2825380/3750000: episode: 3776, duration: 5.229s, episode steps: 696, steps per second: 133, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.015413, mae: 3.014499, mean_q: 3.631028, mean_eps: 0.100000\n",
      " 2826332/3750000: episode: 3777, duration: 7.146s, episode steps: 952, steps per second: 133, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.064 [0.000, 5.000],  loss: 0.014704, mae: 3.008477, mean_q: 3.621999, mean_eps: 0.100000\n",
      " 2827030/3750000: episode: 3778, duration: 5.328s, episode steps: 698, steps per second: 131, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.017214, mae: 3.045557, mean_q: 3.673502, mean_eps: 0.100000\n",
      " 2827650/3750000: episode: 3779, duration: 4.626s, episode steps: 620, steps per second: 134, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.440 [0.000, 5.000],  loss: 0.021912, mae: 3.046222, mean_q: 3.671971, mean_eps: 0.100000\n",
      " 2828455/3750000: episode: 3780, duration: 5.970s, episode steps: 805, steps per second: 135, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.015843, mae: 3.045049, mean_q: 3.669548, mean_eps: 0.100000\n",
      " 2829398/3750000: episode: 3781, duration: 7.075s, episode steps: 943, steps per second: 133, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.015399, mae: 3.032268, mean_q: 3.654632, mean_eps: 0.100000\n",
      " 2829946/3750000: episode: 3782, duration: 4.177s, episode steps: 548, steps per second: 131, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.013931, mae: 2.990102, mean_q: 3.597803, mean_eps: 0.100000\n",
      " 2830803/3750000: episode: 3783, duration: 6.393s, episode steps: 857, steps per second: 134, episode reward: 26.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.012769, mae: 3.042178, mean_q: 3.665038, mean_eps: 0.100000\n",
      " 2831708/3750000: episode: 3784, duration: 6.918s, episode steps: 905, steps per second: 131, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.010041, mae: 3.025732, mean_q: 3.644623, mean_eps: 0.100000\n",
      " 2832933/3750000: episode: 3785, duration: 9.136s, episode steps: 1225, steps per second: 134, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.019049, mae: 3.034865, mean_q: 3.653739, mean_eps: 0.100000\n",
      " 2833736/3750000: episode: 3786, duration: 6.061s, episode steps: 803, steps per second: 132, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.016255, mae: 3.003630, mean_q: 3.626817, mean_eps: 0.100000\n",
      " 2834735/3750000: episode: 3787, duration: 7.516s, episode steps: 999, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.015992, mae: 3.027112, mean_q: 3.646188, mean_eps: 0.100000\n",
      " 2835468/3750000: episode: 3788, duration: 5.570s, episode steps: 733, steps per second: 132, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.667 [0.000, 5.000],  loss: 0.014367, mae: 3.039049, mean_q: 3.659358, mean_eps: 0.100000\n",
      " 2836107/3750000: episode: 3789, duration: 4.809s, episode steps: 639, steps per second: 133, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.878 [0.000, 5.000],  loss: 0.015699, mae: 3.047607, mean_q: 3.665604, mean_eps: 0.100000\n",
      " 2837107/3750000: episode: 3790, duration: 7.478s, episode steps: 1000, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.013465, mae: 3.080127, mean_q: 3.708978, mean_eps: 0.100000\n",
      " 2838103/3750000: episode: 3791, duration: 7.519s, episode steps: 996, steps per second: 132, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.538 [0.000, 5.000],  loss: 0.012127, mae: 3.122686, mean_q: 3.766148, mean_eps: 0.100000\n",
      " 2839438/3750000: episode: 3792, duration: 9.918s, episode steps: 1335, steps per second: 135, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.016077, mae: 3.084909, mean_q: 3.712507, mean_eps: 0.100000\n",
      " 2839933/3750000: episode: 3793, duration: 3.782s, episode steps: 495, steps per second: 131, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 4.147 [0.000, 5.000],  loss: 0.013276, mae: 3.021440, mean_q: 3.642670, mean_eps: 0.100000\n",
      " 2840448/3750000: episode: 3794, duration: 3.863s, episode steps: 515, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 4.377 [0.000, 5.000],  loss: 0.010184, mae: 3.033418, mean_q: 3.664244, mean_eps: 0.100000\n",
      " 2841129/3750000: episode: 3795, duration: 5.092s, episode steps: 681, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.687 [0.000, 5.000],  loss: 0.013265, mae: 3.028342, mean_q: 3.646905, mean_eps: 0.100000\n",
      " 2841927/3750000: episode: 3796, duration: 6.024s, episode steps: 798, steps per second: 132, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.010748, mae: 3.030069, mean_q: 3.648560, mean_eps: 0.100000\n",
      " 2842429/3750000: episode: 3797, duration: 3.752s, episode steps: 502, steps per second: 134, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.685 [0.000, 5.000],  loss: 0.016869, mae: 3.052192, mean_q: 3.673152, mean_eps: 0.100000\n",
      " 2843056/3750000: episode: 3798, duration: 4.651s, episode steps: 627, steps per second: 135, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 4.257 [0.000, 5.000],  loss: 0.014846, mae: 3.112029, mean_q: 3.748682, mean_eps: 0.100000\n",
      " 2843869/3750000: episode: 3799, duration: 6.196s, episode steps: 813, steps per second: 131, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.017251, mae: 3.093591, mean_q: 3.724334, mean_eps: 0.100000\n",
      " 2844594/3750000: episode: 3800, duration: 5.425s, episode steps: 725, steps per second: 134, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.847 [0.000, 5.000],  loss: 0.014498, mae: 3.065910, mean_q: 3.689384, mean_eps: 0.100000\n",
      " 2845195/3750000: episode: 3801, duration: 4.544s, episode steps: 601, steps per second: 132, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.433 [0.000, 5.000],  loss: 0.014471, mae: 3.026617, mean_q: 3.642949, mean_eps: 0.100000\n",
      " 2845712/3750000: episode: 3802, duration: 3.900s, episode steps: 517, steps per second: 133, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.015155, mae: 3.004393, mean_q: 3.612061, mean_eps: 0.100000\n",
      " 2846803/3750000: episode: 3803, duration: 8.268s, episode steps: 1091, steps per second: 132, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.011227, mae: 3.093326, mean_q: 3.726456, mean_eps: 0.100000\n",
      " 2847999/3750000: episode: 3804, duration: 8.917s, episode steps: 1196, steps per second: 134, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.015794, mae: 3.123349, mean_q: 3.760940, mean_eps: 0.100000\n",
      " 2848489/3750000: episode: 3805, duration: 3.841s, episode steps: 490, steps per second: 128, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 4.367 [0.000, 5.000],  loss: 0.013264, mae: 3.081893, mean_q: 3.707812, mean_eps: 0.100000\n",
      " 2849703/3750000: episode: 3806, duration: 9.096s, episode steps: 1214, steps per second: 133, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.815 [0.000, 5.000],  loss: 0.014587, mae: 3.084786, mean_q: 3.711509, mean_eps: 0.100000\n",
      " 2850527/3750000: episode: 3807, duration: 6.193s, episode steps: 824, steps per second: 133, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.919 [0.000, 5.000],  loss: 0.014015, mae: 3.063175, mean_q: 3.692569, mean_eps: 0.100000\n",
      " 2851491/3750000: episode: 3808, duration: 7.195s, episode steps: 964, steps per second: 134, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.992 [0.000, 5.000],  loss: 0.012593, mae: 3.025698, mean_q: 3.649758, mean_eps: 0.100000\n",
      " 2852039/3750000: episode: 3809, duration: 4.031s, episode steps: 548, steps per second: 136, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.181 [0.000, 5.000],  loss: 0.019929, mae: 3.051064, mean_q: 3.679634, mean_eps: 0.100000\n",
      " 2852794/3750000: episode: 3810, duration: 5.719s, episode steps: 755, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.013286, mae: 3.058751, mean_q: 3.681537, mean_eps: 0.100000\n",
      " 2853460/3750000: episode: 3811, duration: 5.055s, episode steps: 666, steps per second: 132, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.013326, mae: 3.050876, mean_q: 3.677026, mean_eps: 0.100000\n",
      " 2854132/3750000: episode: 3812, duration: 5.067s, episode steps: 672, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.606 [0.000, 5.000],  loss: 0.012329, mae: 3.054209, mean_q: 3.686566, mean_eps: 0.100000\n",
      " 2855133/3750000: episode: 3813, duration: 7.507s, episode steps: 1001, steps per second: 133, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.763 [0.000, 5.000],  loss: 0.014312, mae: 3.012224, mean_q: 3.624229, mean_eps: 0.100000\n",
      " 2855734/3750000: episode: 3814, duration: 4.512s, episode steps: 601, steps per second: 133, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.013350, mae: 2.996210, mean_q: 3.605541, mean_eps: 0.100000\n",
      " 2856454/3750000: episode: 3815, duration: 5.346s, episode steps: 720, steps per second: 135, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.016740, mae: 3.054661, mean_q: 3.677332, mean_eps: 0.100000\n",
      " 2857074/3750000: episode: 3816, duration: 4.710s, episode steps: 620, steps per second: 132, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.017336, mae: 3.017262, mean_q: 3.634167, mean_eps: 0.100000\n",
      " 2858173/3750000: episode: 3817, duration: 8.176s, episode steps: 1099, steps per second: 134, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.265 [0.000, 5.000],  loss: 0.016068, mae: 3.047730, mean_q: 3.678004, mean_eps: 0.100000\n",
      " 2858801/3750000: episode: 3818, duration: 4.781s, episode steps: 628, steps per second: 131, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.017635, mae: 2.974454, mean_q: 3.585960, mean_eps: 0.100000\n",
      " 2859334/3750000: episode: 3819, duration: 3.966s, episode steps: 533, steps per second: 134, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016115, mae: 3.029111, mean_q: 3.649030, mean_eps: 0.100000\n",
      " 2859763/3750000: episode: 3820, duration: 3.268s, episode steps: 429, steps per second: 131, episode reward: 11.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.513 [0.000, 5.000],  loss: 0.013834, mae: 3.021337, mean_q: 3.634430, mean_eps: 0.100000\n",
      " 2860369/3750000: episode: 3821, duration: 4.495s, episode steps: 606, steps per second: 135, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.010820, mae: 3.025297, mean_q: 3.642896, mean_eps: 0.100000\n",
      " 2861108/3750000: episode: 3822, duration: 5.559s, episode steps: 739, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.015324, mae: 3.106999, mean_q: 3.741120, mean_eps: 0.100000\n",
      " 2862186/3750000: episode: 3823, duration: 8.005s, episode steps: 1078, steps per second: 135, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.544 [0.000, 5.000],  loss: 0.017642, mae: 3.047096, mean_q: 3.672135, mean_eps: 0.100000\n",
      " 2862944/3750000: episode: 3824, duration: 5.693s, episode steps: 758, steps per second: 133, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.013835, mae: 3.060269, mean_q: 3.685453, mean_eps: 0.100000\n",
      " 2863751/3750000: episode: 3825, duration: 6.040s, episode steps: 807, steps per second: 134, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.019394, mae: 3.042648, mean_q: 3.664339, mean_eps: 0.100000\n",
      " 2864314/3750000: episode: 3826, duration: 4.256s, episode steps: 563, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.547 [0.000, 5.000],  loss: 0.021572, mae: 3.100399, mean_q: 3.737418, mean_eps: 0.100000\n",
      " 2865043/3750000: episode: 3827, duration: 5.466s, episode steps: 729, steps per second: 133, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.011968, mae: 3.098525, mean_q: 3.730200, mean_eps: 0.100000\n",
      " 2865878/3750000: episode: 3828, duration: 6.203s, episode steps: 835, steps per second: 135, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.015944, mae: 3.140364, mean_q: 3.781060, mean_eps: 0.100000\n",
      " 2866580/3750000: episode: 3829, duration: 5.301s, episode steps: 702, steps per second: 132, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 4.007 [0.000, 5.000],  loss: 0.014426, mae: 3.162724, mean_q: 3.822146, mean_eps: 0.100000\n",
      " 2867444/3750000: episode: 3830, duration: 6.569s, episode steps: 864, steps per second: 132, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.014324, mae: 3.207370, mean_q: 3.869024, mean_eps: 0.100000\n",
      " 2868302/3750000: episode: 3831, duration: 6.386s, episode steps: 858, steps per second: 134, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.012124, mae: 3.103381, mean_q: 3.735983, mean_eps: 0.100000\n",
      " 2869008/3750000: episode: 3832, duration: 5.214s, episode steps: 706, steps per second: 135, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.360 [0.000, 5.000],  loss: 0.015410, mae: 3.182743, mean_q: 3.830786, mean_eps: 0.100000\n",
      " 2869735/3750000: episode: 3833, duration: 5.457s, episode steps: 727, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013987, mae: 3.138374, mean_q: 3.781609, mean_eps: 0.100000\n",
      " 2871061/3750000: episode: 3834, duration: 9.919s, episode steps: 1326, steps per second: 134, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.012483, mae: 3.127241, mean_q: 3.764427, mean_eps: 0.100000\n",
      " 2871577/3750000: episode: 3835, duration: 3.839s, episode steps: 516, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.020745, mae: 3.169346, mean_q: 3.811843, mean_eps: 0.100000\n",
      " 2872145/3750000: episode: 3836, duration: 4.299s, episode steps: 568, steps per second: 132, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.581 [0.000, 5.000],  loss: 0.021099, mae: 3.181505, mean_q: 3.830897, mean_eps: 0.100000\n",
      " 2872737/3750000: episode: 3837, duration: 4.357s, episode steps: 592, steps per second: 136, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.466 [0.000, 5.000],  loss: 0.020184, mae: 3.134596, mean_q: 3.788749, mean_eps: 0.100000\n",
      " 2873694/3750000: episode: 3838, duration: 7.213s, episode steps: 957, steps per second: 133, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.018688, mae: 3.146935, mean_q: 3.786379, mean_eps: 0.100000\n",
      " 2874593/3750000: episode: 3839, duration: 6.779s, episode steps: 899, steps per second: 133, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.591 [0.000, 5.000],  loss: 0.013157, mae: 3.099765, mean_q: 3.732455, mean_eps: 0.100000\n",
      " 2875230/3750000: episode: 3840, duration: 4.768s, episode steps: 637, steps per second: 134, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.143 [0.000, 5.000],  loss: 0.016783, mae: 3.117875, mean_q: 3.759380, mean_eps: 0.100000\n",
      " 2875832/3750000: episode: 3841, duration: 4.519s, episode steps: 602, steps per second: 133, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.017434, mae: 3.124270, mean_q: 3.760111, mean_eps: 0.100000\n",
      " 2876507/3750000: episode: 3842, duration: 5.101s, episode steps: 675, steps per second: 132, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.017582, mae: 3.213876, mean_q: 3.866183, mean_eps: 0.100000\n",
      " 2877592/3750000: episode: 3843, duration: 8.087s, episode steps: 1085, steps per second: 134, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.012761, mae: 3.149567, mean_q: 3.793588, mean_eps: 0.100000\n",
      " 2878992/3750000: episode: 3844, duration: 10.420s, episode steps: 1400, steps per second: 134, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.018960, mae: 3.176622, mean_q: 3.822117, mean_eps: 0.100000\n",
      " 2879783/3750000: episode: 3845, duration: 5.999s, episode steps: 791, steps per second: 132, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.303 [0.000, 5.000],  loss: 0.019341, mae: 3.209344, mean_q: 3.864575, mean_eps: 0.100000\n",
      " 2880871/3750000: episode: 3846, duration: 8.147s, episode steps: 1088, steps per second: 134, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.022579, mae: 3.186363, mean_q: 3.845663, mean_eps: 0.100000\n",
      " 2881399/3750000: episode: 3847, duration: 3.916s, episode steps: 528, steps per second: 135, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.752 [0.000, 5.000],  loss: 0.017295, mae: 3.149532, mean_q: 3.794502, mean_eps: 0.100000\n",
      " 2882579/3750000: episode: 3848, duration: 8.917s, episode steps: 1180, steps per second: 132, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.505 [0.000, 5.000],  loss: 0.011980, mae: 3.196564, mean_q: 3.851298, mean_eps: 0.100000\n",
      " 2883643/3750000: episode: 3849, duration: 7.971s, episode steps: 1064, steps per second: 133, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.015658, mae: 3.171514, mean_q: 3.812964, mean_eps: 0.100000\n",
      " 2884758/3750000: episode: 3850, duration: 8.288s, episode steps: 1115, steps per second: 135, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.861 [0.000, 5.000],  loss: 0.012377, mae: 3.114000, mean_q: 3.747201, mean_eps: 0.100000\n",
      " 2885889/3750000: episode: 3851, duration: 8.446s, episode steps: 1131, steps per second: 134, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.018951, mae: 3.178730, mean_q: 3.826925, mean_eps: 0.100000\n",
      " 2886392/3750000: episode: 3852, duration: 3.767s, episode steps: 503, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.011144, mae: 3.133710, mean_q: 3.773987, mean_eps: 0.100000\n",
      " 2886911/3750000: episode: 3853, duration: 3.924s, episode steps: 519, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.010400, mae: 3.162699, mean_q: 3.808954, mean_eps: 0.100000\n",
      " 2887538/3750000: episode: 3854, duration: 4.682s, episode steps: 627, steps per second: 134, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.019941, mae: 3.144773, mean_q: 3.783620, mean_eps: 0.100000\n",
      " 2888057/3750000: episode: 3855, duration: 3.922s, episode steps: 519, steps per second: 132, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.048 [0.000, 5.000],  loss: 0.018102, mae: 3.169672, mean_q: 3.813436, mean_eps: 0.100000\n",
      " 2888744/3750000: episode: 3856, duration: 5.247s, episode steps: 687, steps per second: 131, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.017469, mae: 3.133545, mean_q: 3.769082, mean_eps: 0.100000\n",
      " 2889489/3750000: episode: 3857, duration: 5.574s, episode steps: 745, steps per second: 134, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.016372, mae: 3.187566, mean_q: 3.832553, mean_eps: 0.100000\n",
      " 2890414/3750000: episode: 3858, duration: 6.966s, episode steps: 925, steps per second: 133, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.026 [0.000, 5.000],  loss: 0.014662, mae: 3.172044, mean_q: 3.821021, mean_eps: 0.100000\n",
      " 2891127/3750000: episode: 3859, duration: 5.445s, episode steps: 713, steps per second: 131, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.014007, mae: 3.171048, mean_q: 3.823399, mean_eps: 0.100000\n",
      " 2892102/3750000: episode: 3860, duration: 7.314s, episode steps: 975, steps per second: 133, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013884, mae: 3.225959, mean_q: 3.881259, mean_eps: 0.100000\n",
      " 2892866/3750000: episode: 3861, duration: 5.744s, episode steps: 764, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.556 [0.000, 5.000],  loss: 0.014336, mae: 3.159747, mean_q: 3.803158, mean_eps: 0.100000\n",
      " 2893518/3750000: episode: 3862, duration: 4.846s, episode steps: 652, steps per second: 135, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.534 [0.000, 5.000],  loss: 0.013318, mae: 3.204071, mean_q: 3.865103, mean_eps: 0.100000\n",
      " 2894663/3750000: episode: 3863, duration: 8.646s, episode steps: 1145, steps per second: 132, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.589 [0.000, 5.000],  loss: 0.013238, mae: 3.140029, mean_q: 3.779748, mean_eps: 0.100000\n",
      " 2895599/3750000: episode: 3864, duration: 7.016s, episode steps: 936, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.013040, mae: 3.200101, mean_q: 3.848133, mean_eps: 0.100000\n",
      " 2895996/3750000: episode: 3865, duration: 3.017s, episode steps: 397, steps per second: 132, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.055 [0.000, 5.000],  loss: 0.012421, mae: 3.176047, mean_q: 3.832411, mean_eps: 0.100000\n",
      " 2896661/3750000: episode: 3866, duration: 5.049s, episode steps: 665, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.023152, mae: 3.107083, mean_q: 3.746815, mean_eps: 0.100000\n",
      " 2897309/3750000: episode: 3867, duration: 4.918s, episode steps: 648, steps per second: 132, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.860 [0.000, 5.000],  loss: 0.014785, mae: 3.165320, mean_q: 3.818524, mean_eps: 0.100000\n",
      " 2897798/3750000: episode: 3868, duration: 3.618s, episode steps: 489, steps per second: 135, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.022 [0.000, 5.000],  loss: 0.015174, mae: 3.191213, mean_q: 3.842407, mean_eps: 0.100000\n",
      " 2899440/3750000: episode: 3869, duration: 12.335s, episode steps: 1642, steps per second: 133, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.677 [0.000, 5.000],  loss: 0.013205, mae: 3.177803, mean_q: 3.826273, mean_eps: 0.100000\n",
      " 2900249/3750000: episode: 3870, duration: 6.075s, episode steps: 809, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.596 [0.000, 5.000],  loss: 0.014358, mae: 3.160193, mean_q: 3.802333, mean_eps: 0.100000\n",
      " 2900900/3750000: episode: 3871, duration: 4.863s, episode steps: 651, steps per second: 134, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.969 [0.000, 5.000],  loss: 0.013659, mae: 3.163902, mean_q: 3.808100, mean_eps: 0.100000\n",
      " 2901832/3750000: episode: 3872, duration: 7.075s, episode steps: 932, steps per second: 132, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.013383, mae: 3.158152, mean_q: 3.802812, mean_eps: 0.100000\n",
      " 2902937/3750000: episode: 3873, duration: 8.237s, episode steps: 1105, steps per second: 134, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.013660, mae: 3.162094, mean_q: 3.807249, mean_eps: 0.100000\n",
      " 2904047/3750000: episode: 3874, duration: 8.315s, episode steps: 1110, steps per second: 133, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 0.015190, mae: 3.173941, mean_q: 3.818218, mean_eps: 0.100000\n",
      " 2904978/3750000: episode: 3875, duration: 6.892s, episode steps: 931, steps per second: 135, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.013509, mae: 3.174061, mean_q: 3.812883, mean_eps: 0.100000\n",
      " 2905631/3750000: episode: 3876, duration: 4.943s, episode steps: 653, steps per second: 132, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.055 [0.000, 5.000],  loss: 0.015820, mae: 3.192576, mean_q: 3.838653, mean_eps: 0.100000\n",
      " 2906743/3750000: episode: 3877, duration: 8.342s, episode steps: 1112, steps per second: 133, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.012263, mae: 3.152538, mean_q: 3.792776, mean_eps: 0.100000\n",
      " 2907800/3750000: episode: 3878, duration: 7.902s, episode steps: 1057, steps per second: 134, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.014780, mae: 3.132873, mean_q: 3.769306, mean_eps: 0.100000\n",
      " 2908530/3750000: episode: 3879, duration: 5.552s, episode steps: 730, steps per second: 131, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.015510, mae: 3.130105, mean_q: 3.763391, mean_eps: 0.100000\n",
      " 2909553/3750000: episode: 3880, duration: 7.629s, episode steps: 1023, steps per second: 134, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.012959, mae: 3.157541, mean_q: 3.795122, mean_eps: 0.100000\n",
      " 2910689/3750000: episode: 3881, duration: 8.532s, episode steps: 1136, steps per second: 133, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.016657, mae: 3.197331, mean_q: 3.851038, mean_eps: 0.100000\n",
      " 2911368/3750000: episode: 3882, duration: 5.067s, episode steps: 679, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.010768, mae: 3.203291, mean_q: 3.858859, mean_eps: 0.100000\n",
      " 2911869/3750000: episode: 3883, duration: 3.851s, episode steps: 501, steps per second: 130, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.449 [0.000, 5.000],  loss: 0.012822, mae: 3.212839, mean_q: 3.864600, mean_eps: 0.100000\n",
      " 2912689/3750000: episode: 3884, duration: 6.169s, episode steps: 820, steps per second: 133, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.016642, mae: 3.212358, mean_q: 3.864593, mean_eps: 0.100000\n",
      " 2913720/3750000: episode: 3885, duration: 7.760s, episode steps: 1031, steps per second: 133, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.012904, mae: 3.225245, mean_q: 3.885554, mean_eps: 0.100000\n",
      " 2914363/3750000: episode: 3886, duration: 4.976s, episode steps: 643, steps per second: 129, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.991 [0.000, 5.000],  loss: 0.018343, mae: 3.262127, mean_q: 3.924043, mean_eps: 0.100000\n",
      " 2915596/3750000: episode: 3887, duration: 9.098s, episode steps: 1233, steps per second: 136, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.161 [0.000, 5.000],  loss: 0.014312, mae: 3.188437, mean_q: 3.838717, mean_eps: 0.100000\n",
      " 2916722/3750000: episode: 3888, duration: 8.517s, episode steps: 1126, steps per second: 132, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.018116, mae: 3.201038, mean_q: 3.849039, mean_eps: 0.100000\n",
      " 2917524/3750000: episode: 3889, duration: 5.961s, episode steps: 802, steps per second: 135, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.017156, mae: 3.166827, mean_q: 3.808142, mean_eps: 0.100000\n",
      " 2918685/3750000: episode: 3890, duration: 8.739s, episode steps: 1161, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.211 [0.000, 5.000],  loss: 0.017211, mae: 3.208167, mean_q: 3.868931, mean_eps: 0.100000\n",
      " 2920138/3750000: episode: 3891, duration: 10.796s, episode steps: 1453, steps per second: 135, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.677 [0.000, 5.000],  loss: 0.017218, mae: 3.203561, mean_q: 3.863311, mean_eps: 0.100000\n",
      " 2920906/3750000: episode: 3892, duration: 5.870s, episode steps: 768, steps per second: 131, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.014819, mae: 3.218942, mean_q: 3.872280, mean_eps: 0.100000\n",
      " 2921860/3750000: episode: 3893, duration: 7.119s, episode steps: 954, steps per second: 134, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.015033, mae: 3.200637, mean_q: 3.848168, mean_eps: 0.100000\n",
      " 2922746/3750000: episode: 3894, duration: 6.724s, episode steps: 886, steps per second: 132, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.740 [0.000, 5.000],  loss: 0.013236, mae: 3.178964, mean_q: 3.821749, mean_eps: 0.100000\n",
      " 2923442/3750000: episode: 3895, duration: 5.215s, episode steps: 696, steps per second: 133, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.017750, mae: 3.122829, mean_q: 3.754941, mean_eps: 0.100000\n",
      " 2924097/3750000: episode: 3896, duration: 4.812s, episode steps: 655, steps per second: 136, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.105 [0.000, 5.000],  loss: 0.015772, mae: 3.101979, mean_q: 3.737802, mean_eps: 0.100000\n",
      " 2924844/3750000: episode: 3897, duration: 5.647s, episode steps: 747, steps per second: 132, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 0.015916, mae: 3.072954, mean_q: 3.701097, mean_eps: 0.100000\n",
      " 2925692/3750000: episode: 3898, duration: 6.294s, episode steps: 848, steps per second: 135, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.016345, mae: 3.128205, mean_q: 3.762458, mean_eps: 0.100000\n",
      " 2926531/3750000: episode: 3899, duration: 6.243s, episode steps: 839, steps per second: 134, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.585 [0.000, 5.000],  loss: 0.013421, mae: 3.160491, mean_q: 3.803924, mean_eps: 0.100000\n",
      " 2927047/3750000: episode: 3900, duration: 4.054s, episode steps: 516, steps per second: 127, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.016136, mae: 3.165505, mean_q: 3.810296, mean_eps: 0.100000\n",
      " 2927734/3750000: episode: 3901, duration: 5.219s, episode steps: 687, steps per second: 132, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.405 [0.000, 5.000],  loss: 0.012093, mae: 3.121225, mean_q: 3.761605, mean_eps: 0.100000\n",
      " 2928749/3750000: episode: 3902, duration: 7.662s, episode steps: 1015, steps per second: 132, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.012896, mae: 3.209291, mean_q: 3.860238, mean_eps: 0.100000\n",
      " 2929346/3750000: episode: 3903, duration: 4.519s, episode steps: 597, steps per second: 132, episode reward: 18.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.039 [0.000, 5.000],  loss: 0.015989, mae: 3.121560, mean_q: 3.756279, mean_eps: 0.100000\n",
      " 2930454/3750000: episode: 3904, duration: 8.338s, episode steps: 1108, steps per second: 133, episode reward: 31.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.595 [0.000, 5.000],  loss: 0.017080, mae: 3.185452, mean_q: 3.831557, mean_eps: 0.100000\n",
      " 2931294/3750000: episode: 3905, duration: 6.393s, episode steps: 840, steps per second: 131, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.864 [0.000, 5.000],  loss: 0.015620, mae: 3.213501, mean_q: 3.874311, mean_eps: 0.100000\n",
      " 2932118/3750000: episode: 3906, duration: 6.195s, episode steps: 824, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.012785, mae: 3.194937, mean_q: 3.851242, mean_eps: 0.100000\n",
      " 2933413/3750000: episode: 3907, duration: 9.716s, episode steps: 1295, steps per second: 133, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.017213, mae: 3.151105, mean_q: 3.789905, mean_eps: 0.100000\n",
      " 2934207/3750000: episode: 3908, duration: 5.950s, episode steps: 794, steps per second: 133, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.015729, mae: 3.104022, mean_q: 3.736035, mean_eps: 0.100000\n",
      " 2935165/3750000: episode: 3909, duration: 7.210s, episode steps: 958, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.016723, mae: 3.112841, mean_q: 3.745927, mean_eps: 0.100000\n",
      " 2935870/3750000: episode: 3910, duration: 5.354s, episode steps: 705, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.226 [0.000, 5.000],  loss: 0.012850, mae: 3.137234, mean_q: 3.775802, mean_eps: 0.100000\n",
      " 2936589/3750000: episode: 3911, duration: 5.414s, episode steps: 719, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.014142, mae: 3.073109, mean_q: 3.698014, mean_eps: 0.100000\n",
      " 2937763/3750000: episode: 3912, duration: 8.836s, episode steps: 1174, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.012107, mae: 3.085031, mean_q: 3.711839, mean_eps: 0.100000\n",
      " 2938846/3750000: episode: 3913, duration: 8.120s, episode steps: 1083, steps per second: 133, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.012365, mae: 3.103349, mean_q: 3.735526, mean_eps: 0.100000\n",
      " 2940314/3750000: episode: 3914, duration: 10.980s, episode steps: 1468, steps per second: 134, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.017125, mae: 3.102702, mean_q: 3.736158, mean_eps: 0.100000\n",
      " 2941377/3750000: episode: 3915, duration: 7.903s, episode steps: 1063, steps per second: 135, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.014698, mae: 3.132310, mean_q: 3.772114, mean_eps: 0.100000\n",
      " 2942382/3750000: episode: 3916, duration: 7.648s, episode steps: 1005, steps per second: 131, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.015927, mae: 3.127559, mean_q: 3.765778, mean_eps: 0.100000\n",
      " 2943544/3750000: episode: 3917, duration: 8.736s, episode steps: 1162, steps per second: 133, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.014372, mae: 3.139309, mean_q: 3.781513, mean_eps: 0.100000\n",
      " 2944403/3750000: episode: 3918, duration: 6.537s, episode steps: 859, steps per second: 131, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.013517, mae: 3.140521, mean_q: 3.780811, mean_eps: 0.100000\n",
      " 2945023/3750000: episode: 3919, duration: 4.611s, episode steps: 620, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.845 [0.000, 5.000],  loss: 0.017128, mae: 3.070847, mean_q: 3.695529, mean_eps: 0.100000\n",
      " 2945628/3750000: episode: 3920, duration: 4.499s, episode steps: 605, steps per second: 134, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.012762, mae: 3.142022, mean_q: 3.782082, mean_eps: 0.100000\n",
      " 2946964/3750000: episode: 3921, duration: 10.000s, episode steps: 1336, steps per second: 134, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.012578, mae: 3.127461, mean_q: 3.763230, mean_eps: 0.100000\n",
      " 2948277/3750000: episode: 3922, duration: 9.706s, episode steps: 1313, steps per second: 135, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.014901, mae: 3.119698, mean_q: 3.754167, mean_eps: 0.100000\n",
      " 2948899/3750000: episode: 3923, duration: 4.650s, episode steps: 622, steps per second: 134, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.018988, mae: 3.076070, mean_q: 3.700314, mean_eps: 0.100000\n",
      " 2949693/3750000: episode: 3924, duration: 5.935s, episode steps: 794, steps per second: 134, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.012872, mae: 3.071558, mean_q: 3.697780, mean_eps: 0.100000\n",
      " 2950836/3750000: episode: 3925, duration: 8.665s, episode steps: 1143, steps per second: 132, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.731 [0.000, 5.000],  loss: 0.013794, mae: 3.081587, mean_q: 3.713250, mean_eps: 0.100000\n",
      " 2951750/3750000: episode: 3926, duration: 6.824s, episode steps: 914, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.013260, mae: 3.045892, mean_q: 3.667346, mean_eps: 0.100000\n",
      " 2952640/3750000: episode: 3927, duration: 6.718s, episode steps: 890, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.013877, mae: 3.096513, mean_q: 3.724889, mean_eps: 0.100000\n",
      " 2953317/3750000: episode: 3928, duration: 5.087s, episode steps: 677, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.013100, mae: 3.028910, mean_q: 3.649918, mean_eps: 0.100000\n",
      " 2954275/3750000: episode: 3929, duration: 7.165s, episode steps: 958, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.859 [0.000, 5.000],  loss: 0.014987, mae: 3.092159, mean_q: 3.718820, mean_eps: 0.100000\n",
      " 2955008/3750000: episode: 3930, duration: 5.567s, episode steps: 733, steps per second: 132, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 0.012579, mae: 3.128512, mean_q: 3.766026, mean_eps: 0.100000\n",
      " 2956123/3750000: episode: 3931, duration: 8.356s, episode steps: 1115, steps per second: 133, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.016917, mae: 3.088977, mean_q: 3.711441, mean_eps: 0.100000\n",
      " 2957124/3750000: episode: 3932, duration: 7.505s, episode steps: 1001, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015971, mae: 3.037485, mean_q: 3.658687, mean_eps: 0.100000\n",
      " 2957836/3750000: episode: 3933, duration: 5.248s, episode steps: 712, steps per second: 136, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.016412, mae: 3.040082, mean_q: 3.662324, mean_eps: 0.100000\n",
      " 2958461/3750000: episode: 3934, duration: 4.727s, episode steps: 625, steps per second: 132, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.701 [0.000, 5.000],  loss: 0.014731, mae: 3.061253, mean_q: 3.685925, mean_eps: 0.100000\n",
      " 2959440/3750000: episode: 3935, duration: 7.400s, episode steps: 979, steps per second: 132, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.013452, mae: 3.059805, mean_q: 3.683882, mean_eps: 0.100000\n",
      " 2959897/3750000: episode: 3936, duration: 3.476s, episode steps: 457, steps per second: 131, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 4.063 [0.000, 5.000],  loss: 0.021290, mae: 3.087786, mean_q: 3.714004, mean_eps: 0.100000\n",
      " 2960757/3750000: episode: 3937, duration: 6.401s, episode steps: 860, steps per second: 134, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011310, mae: 3.032606, mean_q: 3.652731, mean_eps: 0.100000\n",
      " 2961697/3750000: episode: 3938, duration: 7.079s, episode steps: 940, steps per second: 133, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.521 [0.000, 5.000],  loss: 0.013669, mae: 3.059656, mean_q: 3.684208, mean_eps: 0.100000\n",
      " 2962816/3750000: episode: 3939, duration: 8.359s, episode steps: 1119, steps per second: 134, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.318 [0.000, 5.000],  loss: 0.013105, mae: 3.073834, mean_q: 3.702600, mean_eps: 0.100000\n",
      " 2963525/3750000: episode: 3940, duration: 5.418s, episode steps: 709, steps per second: 131, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.772 [0.000, 5.000],  loss: 0.016848, mae: 3.055047, mean_q: 3.677658, mean_eps: 0.100000\n",
      " 2964120/3750000: episode: 3941, duration: 4.449s, episode steps: 595, steps per second: 134, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.016257, mae: 3.052769, mean_q: 3.684717, mean_eps: 0.100000\n",
      " 2965594/3750000: episode: 3942, duration: 11.017s, episode steps: 1474, steps per second: 134, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.014177, mae: 3.098097, mean_q: 3.732089, mean_eps: 0.100000\n",
      " 2966433/3750000: episode: 3943, duration: 6.244s, episode steps: 839, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.014937, mae: 3.122279, mean_q: 3.761105, mean_eps: 0.100000\n",
      " 2967344/3750000: episode: 3944, duration: 6.879s, episode steps: 911, steps per second: 132, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.902 [0.000, 5.000],  loss: 0.013980, mae: 3.036876, mean_q: 3.656019, mean_eps: 0.100000\n",
      " 2967887/3750000: episode: 3945, duration: 4.118s, episode steps: 543, steps per second: 132, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.017718, mae: 3.087111, mean_q: 3.722813, mean_eps: 0.100000\n",
      " 2968706/3750000: episode: 3946, duration: 6.120s, episode steps: 819, steps per second: 134, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.136 [0.000, 5.000],  loss: 0.013392, mae: 3.101600, mean_q: 3.736339, mean_eps: 0.100000\n",
      " 2969426/3750000: episode: 3947, duration: 5.389s, episode steps: 720, steps per second: 134, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.014558, mae: 3.073995, mean_q: 3.696121, mean_eps: 0.100000\n",
      " 2970108/3750000: episode: 3948, duration: 5.122s, episode steps: 682, steps per second: 133, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.016058, mae: 3.069686, mean_q: 3.691364, mean_eps: 0.100000\n",
      " 2970753/3750000: episode: 3949, duration: 4.783s, episode steps: 645, steps per second: 135, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.014612, mae: 3.021334, mean_q: 3.634200, mean_eps: 0.100000\n",
      " 2971952/3750000: episode: 3950, duration: 8.958s, episode steps: 1199, steps per second: 134, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.662 [0.000, 5.000],  loss: 0.011430, mae: 3.045573, mean_q: 3.662749, mean_eps: 0.100000\n",
      " 2972698/3750000: episode: 3951, duration: 5.547s, episode steps: 746, steps per second: 134, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.432 [0.000, 5.000],  loss: 0.014195, mae: 3.045801, mean_q: 3.664550, mean_eps: 0.100000\n",
      " 2973199/3750000: episode: 3952, duration: 3.762s, episode steps: 501, steps per second: 133, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.017119, mae: 3.040997, mean_q: 3.662560, mean_eps: 0.100000\n",
      " 2974344/3750000: episode: 3953, duration: 8.880s, episode steps: 1145, steps per second: 129, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.463 [0.000, 5.000],  loss: 0.013697, mae: 3.035566, mean_q: 3.654891, mean_eps: 0.100000\n",
      " 2974964/3750000: episode: 3954, duration: 4.630s, episode steps: 620, steps per second: 134, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.019955, mae: 3.066717, mean_q: 3.690396, mean_eps: 0.100000\n",
      " 2975652/3750000: episode: 3955, duration: 5.177s, episode steps: 688, steps per second: 133, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.446 [0.000, 5.000],  loss: 0.014154, mae: 3.094943, mean_q: 3.731080, mean_eps: 0.100000\n",
      " 2976948/3750000: episode: 3956, duration: 9.738s, episode steps: 1296, steps per second: 133, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.708 [0.000, 5.000],  loss: 0.010627, mae: 3.101986, mean_q: 3.733891, mean_eps: 0.100000\n",
      " 2977632/3750000: episode: 3957, duration: 5.087s, episode steps: 684, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.819 [0.000, 5.000],  loss: 0.013754, mae: 3.105607, mean_q: 3.734848, mean_eps: 0.100000\n",
      " 2978254/3750000: episode: 3958, duration: 4.678s, episode steps: 622, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.595 [0.000, 5.000],  loss: 0.022784, mae: 3.048566, mean_q: 3.670431, mean_eps: 0.100000\n",
      " 2978983/3750000: episode: 3959, duration: 5.449s, episode steps: 729, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.816 [0.000, 5.000],  loss: 0.012015, mae: 3.026920, mean_q: 3.640554, mean_eps: 0.100000\n",
      " 2979851/3750000: episode: 3960, duration: 6.438s, episode steps: 868, steps per second: 135, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.012575, mae: 3.110250, mean_q: 3.742074, mean_eps: 0.100000\n",
      " 2980543/3750000: episode: 3961, duration: 5.242s, episode steps: 692, steps per second: 132, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.014744, mae: 3.142245, mean_q: 3.778326, mean_eps: 0.100000\n",
      " 2981833/3750000: episode: 3962, duration: 9.552s, episode steps: 1290, steps per second: 135, episode reward: 35.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.634 [0.000, 5.000],  loss: 0.015371, mae: 3.122303, mean_q: 3.756038, mean_eps: 0.100000\n",
      " 2982250/3750000: episode: 3963, duration: 3.121s, episode steps: 417, steps per second: 134, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.012314, mae: 3.121423, mean_q: 3.758346, mean_eps: 0.100000\n",
      " 2983295/3750000: episode: 3964, duration: 7.803s, episode steps: 1045, steps per second: 134, episode reward: 30.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.329 [0.000, 5.000],  loss: 0.017332, mae: 3.159644, mean_q: 3.800905, mean_eps: 0.100000\n",
      " 2984616/3750000: episode: 3965, duration: 9.856s, episode steps: 1321, steps per second: 134, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.017827, mae: 3.143606, mean_q: 3.782842, mean_eps: 0.100000\n",
      " 2985125/3750000: episode: 3966, duration: 3.829s, episode steps: 509, steps per second: 133, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.013878, mae: 3.156970, mean_q: 3.800741, mean_eps: 0.100000\n",
      " 2985755/3750000: episode: 3967, duration: 4.658s, episode steps: 630, steps per second: 135, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.138 [0.000, 5.000],  loss: 0.015911, mae: 3.202994, mean_q: 3.855321, mean_eps: 0.100000\n",
      " 2986529/3750000: episode: 3968, duration: 5.833s, episode steps: 774, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.048 [0.000, 5.000],  loss: 0.011905, mae: 3.147173, mean_q: 3.790384, mean_eps: 0.100000\n",
      " 2987438/3750000: episode: 3969, duration: 6.762s, episode steps: 909, steps per second: 134, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.014118, mae: 3.092174, mean_q: 3.725637, mean_eps: 0.100000\n",
      " 2988856/3750000: episode: 3970, duration: 10.549s, episode steps: 1418, steps per second: 134, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.902 [0.000, 5.000],  loss: 0.018394, mae: 3.096871, mean_q: 3.728084, mean_eps: 0.100000\n",
      " 2989643/3750000: episode: 3971, duration: 5.936s, episode steps: 787, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.013657, mae: 3.059961, mean_q: 3.681862, mean_eps: 0.100000\n",
      " 2990563/3750000: episode: 3972, duration: 6.828s, episode steps: 920, steps per second: 135, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.202 [0.000, 5.000],  loss: 0.015553, mae: 3.152855, mean_q: 3.794983, mean_eps: 0.100000\n",
      " 2991355/3750000: episode: 3973, duration: 5.970s, episode steps: 792, steps per second: 133, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.014547, mae: 3.131691, mean_q: 3.775505, mean_eps: 0.100000\n",
      " 2992399/3750000: episode: 3974, duration: 7.814s, episode steps: 1044, steps per second: 134, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.018853, mae: 3.116655, mean_q: 3.755026, mean_eps: 0.100000\n",
      " 2993554/3750000: episode: 3975, duration: 8.718s, episode steps: 1155, steps per second: 132, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.019510, mae: 3.096750, mean_q: 3.728724, mean_eps: 0.100000\n",
      " 2994397/3750000: episode: 3976, duration: 6.256s, episode steps: 843, steps per second: 135, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.015056, mae: 3.157100, mean_q: 3.796441, mean_eps: 0.100000\n",
      " 2995228/3750000: episode: 3977, duration: 6.275s, episode steps: 831, steps per second: 132, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.785 [0.000, 5.000],  loss: 0.010287, mae: 3.197114, mean_q: 3.847943, mean_eps: 0.100000\n",
      " 2995931/3750000: episode: 3978, duration: 5.278s, episode steps: 703, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.009899, mae: 3.162834, mean_q: 3.805148, mean_eps: 0.100000\n",
      " 2996421/3750000: episode: 3979, duration: 3.704s, episode steps: 490, steps per second: 132, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.014586, mae: 3.175475, mean_q: 3.820539, mean_eps: 0.100000\n",
      " 2997674/3750000: episode: 3980, duration: 9.345s, episode steps: 1253, steps per second: 134, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.015044, mae: 3.194008, mean_q: 3.842666, mean_eps: 0.100000\n",
      " 2998625/3750000: episode: 3981, duration: 7.095s, episode steps: 951, steps per second: 134, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.014896, mae: 3.195960, mean_q: 3.846203, mean_eps: 0.100000\n",
      " 2999957/3750000: episode: 3982, duration: 9.885s, episode steps: 1332, steps per second: 135, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.014928, mae: 3.167618, mean_q: 3.811356, mean_eps: 0.100000\n",
      " 3000663/3750000: episode: 3983, duration: 5.446s, episode steps: 706, steps per second: 130, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.975 [0.000, 5.000],  loss: 0.014483, mae: 3.180008, mean_q: 3.823341, mean_eps: 0.100000\n",
      " 3001286/3750000: episode: 3984, duration: 4.644s, episode steps: 623, steps per second: 134, episode reward: 23.000, mean reward:  0.037 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.015716, mae: 3.150997, mean_q: 3.789726, mean_eps: 0.100000\n",
      " 3002374/3750000: episode: 3985, duration: 8.114s, episode steps: 1088, steps per second: 134, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.016227, mae: 3.192839, mean_q: 3.841822, mean_eps: 0.100000\n",
      " 3003439/3750000: episode: 3986, duration: 7.892s, episode steps: 1065, steps per second: 135, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.013190, mae: 3.143786, mean_q: 3.786709, mean_eps: 0.100000\n",
      " 3003984/3750000: episode: 3987, duration: 4.174s, episode steps: 545, steps per second: 131, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.804 [0.000, 5.000],  loss: 0.014173, mae: 3.111793, mean_q: 3.742687, mean_eps: 0.100000\n",
      " 3004834/3750000: episode: 3988, duration: 6.273s, episode steps: 850, steps per second: 136, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.017049, mae: 3.144794, mean_q: 3.781780, mean_eps: 0.100000\n",
      " 3005643/3750000: episode: 3989, duration: 6.066s, episode steps: 809, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.016445, mae: 3.177554, mean_q: 3.818509, mean_eps: 0.100000\n",
      " 3006331/3750000: episode: 3990, duration: 5.148s, episode steps: 688, steps per second: 134, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.015970, mae: 3.101286, mean_q: 3.730736, mean_eps: 0.100000\n",
      " 3007271/3750000: episode: 3991, duration: 7.033s, episode steps: 940, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.010654, mae: 3.151243, mean_q: 3.790156, mean_eps: 0.100000\n",
      " 3007974/3750000: episode: 3992, duration: 5.311s, episode steps: 703, steps per second: 132, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.011853, mae: 3.181720, mean_q: 3.826344, mean_eps: 0.100000\n",
      " 3009165/3750000: episode: 3993, duration: 8.983s, episode steps: 1191, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.021359, mae: 3.133549, mean_q: 3.767957, mean_eps: 0.100000\n",
      " 3010457/3750000: episode: 3994, duration: 9.625s, episode steps: 1292, steps per second: 134, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.830 [0.000, 5.000],  loss: 0.021030, mae: 3.147775, mean_q: 3.792826, mean_eps: 0.100000\n",
      " 3011255/3750000: episode: 3995, duration: 5.962s, episode steps: 798, steps per second: 134, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 0.969 [0.000, 5.000],  loss: 0.015531, mae: 3.133728, mean_q: 3.770590, mean_eps: 0.100000\n",
      " 3012153/3750000: episode: 3996, duration: 6.692s, episode steps: 898, steps per second: 134, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.797 [0.000, 5.000],  loss: 0.015751, mae: 3.112613, mean_q: 3.745070, mean_eps: 0.100000\n",
      " 3012760/3750000: episode: 3997, duration: 4.654s, episode steps: 607, steps per second: 130, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.014234, mae: 3.127680, mean_q: 3.757290, mean_eps: 0.100000\n",
      " 3013736/3750000: episode: 3998, duration: 7.305s, episode steps: 976, steps per second: 134, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.016550, mae: 3.150681, mean_q: 3.794889, mean_eps: 0.100000\n",
      " 3014303/3750000: episode: 3999, duration: 4.319s, episode steps: 567, steps per second: 131, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.012457, mae: 3.140393, mean_q: 3.774766, mean_eps: 0.100000\n",
      " 3015563/3750000: episode: 4000, duration: 9.442s, episode steps: 1260, steps per second: 133, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.702 [0.000, 5.000],  loss: 0.015239, mae: 3.130995, mean_q: 3.764505, mean_eps: 0.100000\n",
      " 3016580/3750000: episode: 4001, duration: 7.619s, episode steps: 1017, steps per second: 133, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.016750, mae: 3.131256, mean_q: 3.767086, mean_eps: 0.100000\n",
      " 3017659/3750000: episode: 4002, duration: 8.125s, episode steps: 1079, steps per second: 133, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.013267, mae: 3.118423, mean_q: 3.750549, mean_eps: 0.100000\n",
      " 3019353/3750000: episode: 4003, duration: 12.627s, episode steps: 1694, steps per second: 134, episode reward: 34.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.329 [0.000, 5.000],  loss: 0.013384, mae: 3.103134, mean_q: 3.741719, mean_eps: 0.100000\n",
      " 3020001/3750000: episode: 4004, duration: 4.895s, episode steps: 648, steps per second: 132, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.360 [0.000, 5.000],  loss: 0.015306, mae: 3.100737, mean_q: 3.730595, mean_eps: 0.100000\n",
      " 3020790/3750000: episode: 4005, duration: 5.807s, episode steps: 789, steps per second: 136, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.010619, mae: 3.095220, mean_q: 3.725847, mean_eps: 0.100000\n",
      " 3021414/3750000: episode: 4006, duration: 4.698s, episode steps: 624, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.012180, mae: 3.110333, mean_q: 3.742792, mean_eps: 0.100000\n",
      " 3022799/3750000: episode: 4007, duration: 10.239s, episode steps: 1385, steps per second: 135, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.279 [0.000, 5.000],  loss: 0.013713, mae: 3.130434, mean_q: 3.768567, mean_eps: 0.100000\n",
      " 3023171/3750000: episode: 4008, duration: 2.860s, episode steps: 372, steps per second: 130, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.989 [0.000, 5.000],  loss: 0.010370, mae: 3.103016, mean_q: 3.737210, mean_eps: 0.100000\n",
      " 3023769/3750000: episode: 4009, duration: 4.531s, episode steps: 598, steps per second: 132, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.162 [0.000, 5.000],  loss: 0.010338, mae: 3.093688, mean_q: 3.716489, mean_eps: 0.100000\n",
      " 3025080/3750000: episode: 4010, duration: 9.849s, episode steps: 1311, steps per second: 133, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.768 [0.000, 5.000],  loss: 0.016593, mae: 3.139947, mean_q: 3.774581, mean_eps: 0.100000\n",
      " 3026436/3750000: episode: 4011, duration: 10.127s, episode steps: 1356, steps per second: 134, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.015618, mae: 3.107231, mean_q: 3.741281, mean_eps: 0.100000\n",
      " 3027084/3750000: episode: 4012, duration: 4.875s, episode steps: 648, steps per second: 133, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.010309, mae: 3.057059, mean_q: 3.683597, mean_eps: 0.100000\n",
      " 3027779/3750000: episode: 4013, duration: 5.239s, episode steps: 695, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.380 [0.000, 5.000],  loss: 0.013599, mae: 3.076090, mean_q: 3.701406, mean_eps: 0.100000\n",
      " 3028526/3750000: episode: 4014, duration: 5.617s, episode steps: 747, steps per second: 133, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.018514, mae: 3.073417, mean_q: 3.693473, mean_eps: 0.100000\n",
      " 3029030/3750000: episode: 4015, duration: 3.730s, episode steps: 504, steps per second: 135, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.019652, mae: 3.062891, mean_q: 3.678428, mean_eps: 0.100000\n",
      " 3029890/3750000: episode: 4016, duration: 6.436s, episode steps: 860, steps per second: 134, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.313 [0.000, 5.000],  loss: 0.015179, mae: 3.007429, mean_q: 3.618871, mean_eps: 0.100000\n",
      " 3030596/3750000: episode: 4017, duration: 5.258s, episode steps: 706, steps per second: 134, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.014189, mae: 3.008548, mean_q: 3.618491, mean_eps: 0.100000\n",
      " 3031183/3750000: episode: 4018, duration: 4.434s, episode steps: 587, steps per second: 132, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.012143, mae: 3.045784, mean_q: 3.662904, mean_eps: 0.100000\n",
      " 3032111/3750000: episode: 4019, duration: 6.906s, episode steps: 928, steps per second: 134, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.020573, mae: 3.035094, mean_q: 3.655385, mean_eps: 0.100000\n",
      " 3032866/3750000: episode: 4020, duration: 5.592s, episode steps: 755, steps per second: 135, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 4.050 [0.000, 5.000],  loss: 0.016966, mae: 3.061300, mean_q: 3.693892, mean_eps: 0.100000\n",
      " 3033718/3750000: episode: 4021, duration: 6.342s, episode steps: 852, steps per second: 134, episode reward: 26.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.391 [0.000, 5.000],  loss: 0.013686, mae: 3.070354, mean_q: 3.693683, mean_eps: 0.100000\n",
      " 3034389/3750000: episode: 4022, duration: 5.102s, episode steps: 671, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.611 [0.000, 5.000],  loss: 0.017821, mae: 3.061513, mean_q: 3.690545, mean_eps: 0.100000\n",
      " 3034796/3750000: episode: 4023, duration: 2.990s, episode steps: 407, steps per second: 136, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.263 [0.000, 5.000],  loss: 0.013409, mae: 3.089114, mean_q: 3.714917, mean_eps: 0.100000\n",
      " 3035623/3750000: episode: 4024, duration: 6.182s, episode steps: 827, steps per second: 134, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.051 [0.000, 5.000],  loss: 0.014587, mae: 3.077593, mean_q: 3.699717, mean_eps: 0.100000\n",
      " 3036552/3750000: episode: 4025, duration: 6.963s, episode steps: 929, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.011477, mae: 3.083759, mean_q: 3.713237, mean_eps: 0.100000\n",
      " 3037178/3750000: episode: 4026, duration: 4.654s, episode steps: 626, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.014757, mae: 3.090749, mean_q: 3.716387, mean_eps: 0.100000\n",
      " 3037916/3750000: episode: 4027, duration: 5.515s, episode steps: 738, steps per second: 134, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.514 [0.000, 5.000],  loss: 0.011329, mae: 3.056532, mean_q: 3.681381, mean_eps: 0.100000\n",
      " 3038436/3750000: episode: 4028, duration: 3.959s, episode steps: 520, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 4.240 [0.000, 5.000],  loss: 0.014417, mae: 3.095795, mean_q: 3.733843, mean_eps: 0.100000\n",
      " 3039651/3750000: episode: 4029, duration: 9.033s, episode steps: 1215, steps per second: 135, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.235 [0.000, 5.000],  loss: 0.014976, mae: 3.125257, mean_q: 3.765070, mean_eps: 0.100000\n",
      " 3041096/3750000: episode: 4030, duration: 10.796s, episode steps: 1445, steps per second: 134, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.694 [0.000, 5.000],  loss: 0.019767, mae: 3.106860, mean_q: 3.737384, mean_eps: 0.100000\n",
      " 3041622/3750000: episode: 4031, duration: 4.037s, episode steps: 526, steps per second: 130, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 0.913 [0.000, 5.000],  loss: 0.013450, mae: 3.100806, mean_q: 3.730546, mean_eps: 0.100000\n",
      " 3042726/3750000: episode: 4032, duration: 8.261s, episode steps: 1104, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.013132, mae: 3.095751, mean_q: 3.723804, mean_eps: 0.100000\n",
      " 3043418/3750000: episode: 4033, duration: 5.142s, episode steps: 692, steps per second: 135, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.011272, mae: 3.083466, mean_q: 3.712543, mean_eps: 0.100000\n",
      " 3044251/3750000: episode: 4034, duration: 6.257s, episode steps: 833, steps per second: 133, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.016273, mae: 3.090273, mean_q: 3.716978, mean_eps: 0.100000\n",
      " 3044890/3750000: episode: 4035, duration: 4.814s, episode steps: 639, steps per second: 133, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 4.031 [0.000, 5.000],  loss: 0.010350, mae: 3.033208, mean_q: 3.656716, mean_eps: 0.100000\n",
      " 3045690/3750000: episode: 4036, duration: 5.965s, episode steps: 800, steps per second: 134, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.062 [0.000, 5.000],  loss: 0.015827, mae: 3.140848, mean_q: 3.774243, mean_eps: 0.100000\n",
      " 3046276/3750000: episode: 4037, duration: 4.329s, episode steps: 586, steps per second: 135, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.215 [0.000, 5.000],  loss: 0.018323, mae: 3.047569, mean_q: 3.662227, mean_eps: 0.100000\n",
      " 3047061/3750000: episode: 4038, duration: 6.032s, episode steps: 785, steps per second: 130, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.015335, mae: 3.109093, mean_q: 3.739266, mean_eps: 0.100000\n",
      " 3047852/3750000: episode: 4039, duration: 6.000s, episode steps: 791, steps per second: 132, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.015898, mae: 3.092498, mean_q: 3.720879, mean_eps: 0.100000\n",
      " 3048537/3750000: episode: 4040, duration: 5.282s, episode steps: 685, steps per second: 130, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.014166, mae: 3.142284, mean_q: 3.784847, mean_eps: 0.100000\n",
      " 3049052/3750000: episode: 4041, duration: 4.046s, episode steps: 515, steps per second: 127, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.016964, mae: 3.102652, mean_q: 3.735194, mean_eps: 0.100000\n",
      " 3050175/3750000: episode: 4042, duration: 8.387s, episode steps: 1123, steps per second: 134, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.820 [0.000, 5.000],  loss: 0.013261, mae: 3.122593, mean_q: 3.759112, mean_eps: 0.100000\n",
      " 3050905/3750000: episode: 4043, duration: 5.635s, episode steps: 730, steps per second: 130, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.012787, mae: 3.079614, mean_q: 3.707016, mean_eps: 0.100000\n",
      " 3051528/3750000: episode: 4044, duration: 4.654s, episode steps: 623, steps per second: 134, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.015306, mae: 3.149218, mean_q: 3.788913, mean_eps: 0.100000\n",
      " 3052783/3750000: episode: 4045, duration: 9.355s, episode steps: 1255, steps per second: 134, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.764 [0.000, 5.000],  loss: 0.015937, mae: 3.118745, mean_q: 3.754374, mean_eps: 0.100000\n",
      " 3053914/3750000: episode: 4046, duration: 8.520s, episode steps: 1131, steps per second: 133, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.015671, mae: 3.127059, mean_q: 3.765436, mean_eps: 0.100000\n",
      " 3055237/3750000: episode: 4047, duration: 9.873s, episode steps: 1323, steps per second: 134, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.015577, mae: 3.068988, mean_q: 3.697539, mean_eps: 0.100000\n",
      " 3056315/3750000: episode: 4048, duration: 8.147s, episode steps: 1078, steps per second: 132, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.014792, mae: 3.066259, mean_q: 3.690305, mean_eps: 0.100000\n",
      " 3057087/3750000: episode: 4049, duration: 5.822s, episode steps: 772, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.012591, mae: 3.115548, mean_q: 3.748847, mean_eps: 0.100000\n",
      " 3057627/3750000: episode: 4050, duration: 4.090s, episode steps: 540, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.137 [0.000, 5.000],  loss: 0.016070, mae: 3.026877, mean_q: 3.643906, mean_eps: 0.100000\n",
      " 3058431/3750000: episode: 4051, duration: 5.989s, episode steps: 804, steps per second: 134, episode reward: 27.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.011826, mae: 3.066633, mean_q: 3.688446, mean_eps: 0.100000\n",
      " 3059074/3750000: episode: 4052, duration: 4.792s, episode steps: 643, steps per second: 134, episode reward: 18.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.271 [0.000, 5.000],  loss: 0.016248, mae: 3.077150, mean_q: 3.705682, mean_eps: 0.100000\n",
      " 3059897/3750000: episode: 4053, duration: 6.176s, episode steps: 823, steps per second: 133, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.017139, mae: 3.100503, mean_q: 3.730915, mean_eps: 0.100000\n",
      " 3060410/3750000: episode: 4054, duration: 3.892s, episode steps: 513, steps per second: 132, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.021 [0.000, 5.000],  loss: 0.014903, mae: 3.139000, mean_q: 3.782284, mean_eps: 0.100000\n",
      " 3061140/3750000: episode: 4055, duration: 5.496s, episode steps: 730, steps per second: 133, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.018398, mae: 3.073482, mean_q: 3.712385, mean_eps: 0.100000\n",
      " 3061883/3750000: episode: 4056, duration: 5.669s, episode steps: 743, steps per second: 131, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.149 [0.000, 5.000],  loss: 0.015383, mae: 3.133115, mean_q: 3.780643, mean_eps: 0.100000\n",
      " 3063143/3750000: episode: 4057, duration: 9.420s, episode steps: 1260, steps per second: 134, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.016678, mae: 3.060777, mean_q: 3.686389, mean_eps: 0.100000\n",
      " 3063825/3750000: episode: 4058, duration: 5.130s, episode steps: 682, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.500 [0.000, 5.000],  loss: 0.019970, mae: 3.060690, mean_q: 3.684725, mean_eps: 0.100000\n",
      " 3065207/3750000: episode: 4059, duration: 10.252s, episode steps: 1382, steps per second: 135, episode reward: 26.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.016875, mae: 3.081519, mean_q: 3.708239, mean_eps: 0.100000\n",
      " 3066856/3750000: episode: 4060, duration: 12.153s, episode steps: 1649, steps per second: 136, episode reward: 34.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.186 [0.000, 5.000],  loss: 0.018094, mae: 3.073517, mean_q: 3.700397, mean_eps: 0.100000\n",
      " 3067749/3750000: episode: 4061, duration: 6.678s, episode steps: 893, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.681 [0.000, 5.000],  loss: 0.016117, mae: 3.116096, mean_q: 3.747702, mean_eps: 0.100000\n",
      " 3068530/3750000: episode: 4062, duration: 5.877s, episode steps: 781, steps per second: 133, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.017141, mae: 3.103037, mean_q: 3.731198, mean_eps: 0.100000\n",
      " 3069118/3750000: episode: 4063, duration: 4.360s, episode steps: 588, steps per second: 135, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.037 [0.000, 5.000],  loss: 0.016174, mae: 3.137779, mean_q: 3.773806, mean_eps: 0.100000\n",
      " 3070023/3750000: episode: 4064, duration: 6.839s, episode steps: 905, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.019171, mae: 3.103799, mean_q: 3.736767, mean_eps: 0.100000\n",
      " 3070673/3750000: episode: 4065, duration: 4.824s, episode steps: 650, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.011615, mae: 3.120922, mean_q: 3.759299, mean_eps: 0.100000\n",
      " 3071292/3750000: episode: 4066, duration: 4.622s, episode steps: 619, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.695 [0.000, 5.000],  loss: 0.013331, mae: 3.109139, mean_q: 3.744398, mean_eps: 0.100000\n",
      " 3072227/3750000: episode: 4067, duration: 7.109s, episode steps: 935, steps per second: 132, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.996 [0.000, 5.000],  loss: 0.013797, mae: 3.137545, mean_q: 3.775133, mean_eps: 0.100000\n",
      " 3072936/3750000: episode: 4068, duration: 5.246s, episode steps: 709, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.193 [0.000, 5.000],  loss: 0.019364, mae: 3.116530, mean_q: 3.748467, mean_eps: 0.100000\n",
      " 3073663/3750000: episode: 4069, duration: 5.461s, episode steps: 727, steps per second: 133, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.018331, mae: 3.153137, mean_q: 3.793381, mean_eps: 0.100000\n",
      " 3074617/3750000: episode: 4070, duration: 7.127s, episode steps: 954, steps per second: 134, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.014549, mae: 3.134437, mean_q: 3.770783, mean_eps: 0.100000\n",
      " 3075323/3750000: episode: 4071, duration: 5.349s, episode steps: 706, steps per second: 132, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.290 [0.000, 5.000],  loss: 0.017148, mae: 3.133548, mean_q: 3.773394, mean_eps: 0.100000\n",
      " 3076429/3750000: episode: 4072, duration: 8.253s, episode steps: 1106, steps per second: 134, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.388 [0.000, 5.000],  loss: 0.013343, mae: 3.199966, mean_q: 3.855742, mean_eps: 0.100000\n",
      " 3077410/3750000: episode: 4073, duration: 7.348s, episode steps: 981, steps per second: 134, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.017330, mae: 3.184493, mean_q: 3.830722, mean_eps: 0.100000\n",
      " 3078320/3750000: episode: 4074, duration: 6.854s, episode steps: 910, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.013999, mae: 3.177029, mean_q: 3.823037, mean_eps: 0.100000\n",
      " 3079163/3750000: episode: 4075, duration: 6.461s, episode steps: 843, steps per second: 130, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.012561, mae: 3.168138, mean_q: 3.815311, mean_eps: 0.100000\n",
      " 3079796/3750000: episode: 4076, duration: 4.649s, episode steps: 633, steps per second: 136, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 4.188 [0.000, 5.000],  loss: 0.021663, mae: 3.192351, mean_q: 3.847312, mean_eps: 0.100000\n",
      " 3080606/3750000: episode: 4077, duration: 6.112s, episode steps: 810, steps per second: 133, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.735 [0.000, 5.000],  loss: 0.014285, mae: 3.123221, mean_q: 3.756661, mean_eps: 0.100000\n",
      " 3081987/3750000: episode: 4078, duration: 10.240s, episode steps: 1381, steps per second: 135, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.013735, mae: 3.161226, mean_q: 3.807129, mean_eps: 0.100000\n",
      " 3083049/3750000: episode: 4079, duration: 7.929s, episode steps: 1062, steps per second: 134, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.716 [0.000, 5.000],  loss: 0.015119, mae: 3.147924, mean_q: 3.793465, mean_eps: 0.100000\n",
      " 3083757/3750000: episode: 4080, duration: 5.256s, episode steps: 708, steps per second: 135, episode reward: 23.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.012921, mae: 3.141290, mean_q: 3.787596, mean_eps: 0.100000\n",
      " 3084305/3750000: episode: 4081, duration: 4.123s, episode steps: 548, steps per second: 133, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.016101, mae: 3.124844, mean_q: 3.760705, mean_eps: 0.100000\n",
      " 3085156/3750000: episode: 4082, duration: 6.387s, episode steps: 851, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.180 [0.000, 5.000],  loss: 0.015900, mae: 3.100705, mean_q: 3.737055, mean_eps: 0.100000\n",
      " 3085792/3750000: episode: 4083, duration: 4.740s, episode steps: 636, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.302 [0.000, 5.000],  loss: 0.016746, mae: 3.194638, mean_q: 3.848455, mean_eps: 0.100000\n",
      " 3086319/3750000: episode: 4084, duration: 3.915s, episode steps: 527, steps per second: 135, episode reward: 15.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.012112, mae: 3.145409, mean_q: 3.785458, mean_eps: 0.100000\n",
      " 3087467/3750000: episode: 4085, duration: 8.670s, episode steps: 1148, steps per second: 132, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.017747, mae: 3.158472, mean_q: 3.802333, mean_eps: 0.100000\n",
      " 3088684/3750000: episode: 4086, duration: 9.106s, episode steps: 1217, steps per second: 134, episode reward: 33.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.017661, mae: 3.146380, mean_q: 3.789188, mean_eps: 0.100000\n",
      " 3089601/3750000: episode: 4087, duration: 6.951s, episode steps: 917, steps per second: 132, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.015278, mae: 3.178452, mean_q: 3.824425, mean_eps: 0.100000\n",
      " 3090408/3750000: episode: 4088, duration: 6.009s, episode steps: 807, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.010952, mae: 3.118133, mean_q: 3.752862, mean_eps: 0.100000\n",
      " 3091064/3750000: episode: 4089, duration: 4.886s, episode steps: 656, steps per second: 134, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.287 [0.000, 5.000],  loss: 0.013871, mae: 3.119414, mean_q: 3.756222, mean_eps: 0.100000\n",
      " 3091970/3750000: episode: 4090, duration: 6.789s, episode steps: 906, steps per second: 133, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.017120, mae: 3.143970, mean_q: 3.787579, mean_eps: 0.100000\n",
      " 3092605/3750000: episode: 4091, duration: 4.749s, episode steps: 635, steps per second: 134, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.016687, mae: 3.122372, mean_q: 3.754056, mean_eps: 0.100000\n",
      " 3093490/3750000: episode: 4092, duration: 6.593s, episode steps: 885, steps per second: 134, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.014710, mae: 3.173625, mean_q: 3.823984, mean_eps: 0.100000\n",
      " 3094292/3750000: episode: 4093, duration: 5.971s, episode steps: 802, steps per second: 134, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.012488, mae: 3.125363, mean_q: 3.763690, mean_eps: 0.100000\n",
      " 3095186/3750000: episode: 4094, duration: 6.709s, episode steps: 894, steps per second: 133, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.573 [0.000, 5.000],  loss: 0.017361, mae: 3.133237, mean_q: 3.768913, mean_eps: 0.100000\n",
      " 3095878/3750000: episode: 4095, duration: 5.151s, episode steps: 692, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.018579, mae: 3.067000, mean_q: 3.692596, mean_eps: 0.100000\n",
      " 3096345/3750000: episode: 4096, duration: 3.552s, episode steps: 467, steps per second: 131, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.529 [0.000, 5.000],  loss: 0.013775, mae: 3.115299, mean_q: 3.748203, mean_eps: 0.100000\n",
      " 3097119/3750000: episode: 4097, duration: 5.724s, episode steps: 774, steps per second: 135, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.016873, mae: 3.150951, mean_q: 3.788026, mean_eps: 0.100000\n",
      " 3097784/3750000: episode: 4098, duration: 5.070s, episode steps: 665, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.015028, mae: 3.108364, mean_q: 3.744621, mean_eps: 0.100000\n",
      " 3098561/3750000: episode: 4099, duration: 5.811s, episode steps: 777, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.017437, mae: 3.146688, mean_q: 3.799578, mean_eps: 0.100000\n",
      " 3099231/3750000: episode: 4100, duration: 4.943s, episode steps: 670, steps per second: 136, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.018704, mae: 3.136970, mean_q: 3.770934, mean_eps: 0.100000\n",
      " 3100502/3750000: episode: 4101, duration: 9.570s, episode steps: 1271, steps per second: 133, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.535 [0.000, 5.000],  loss: 0.012353, mae: 3.143898, mean_q: 3.780124, mean_eps: 0.100000\n",
      " 3101612/3750000: episode: 4102, duration: 8.263s, episode steps: 1110, steps per second: 134, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.014157, mae: 3.143275, mean_q: 3.783378, mean_eps: 0.100000\n",
      " 3102460/3750000: episode: 4103, duration: 6.444s, episode steps: 848, steps per second: 132, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.983 [0.000, 5.000],  loss: 0.014663, mae: 3.169176, mean_q: 3.812748, mean_eps: 0.100000\n",
      " 3103310/3750000: episode: 4104, duration: 6.367s, episode steps: 850, steps per second: 134, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.140 [0.000, 5.000],  loss: 0.016898, mae: 3.175319, mean_q: 3.819276, mean_eps: 0.100000\n",
      " 3104215/3750000: episode: 4105, duration: 6.840s, episode steps: 905, steps per second: 132, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.445 [0.000, 5.000],  loss: 0.015023, mae: 3.166541, mean_q: 3.816784, mean_eps: 0.100000\n",
      " 3105092/3750000: episode: 4106, duration: 6.565s, episode steps: 877, steps per second: 134, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.012127, mae: 3.137701, mean_q: 3.777633, mean_eps: 0.100000\n",
      " 3106171/3750000: episode: 4107, duration: 7.973s, episode steps: 1079, steps per second: 135, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 0.013684, mae: 3.135311, mean_q: 3.771354, mean_eps: 0.100000\n",
      " 3107031/3750000: episode: 4108, duration: 6.427s, episode steps: 860, steps per second: 134, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.011911, mae: 3.151699, mean_q: 3.800011, mean_eps: 0.100000\n",
      " 3108182/3750000: episode: 4109, duration: 8.643s, episode steps: 1151, steps per second: 133, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.897 [0.000, 5.000],  loss: 0.014494, mae: 3.143794, mean_q: 3.785189, mean_eps: 0.100000\n",
      " 3109420/3750000: episode: 4110, duration: 9.315s, episode steps: 1238, steps per second: 133, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.772 [0.000, 5.000],  loss: 0.013936, mae: 3.147669, mean_q: 3.790284, mean_eps: 0.100000\n",
      " 3110026/3750000: episode: 4111, duration: 4.552s, episode steps: 606, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.015358, mae: 3.112012, mean_q: 3.742674, mean_eps: 0.100000\n",
      " 3110699/3750000: episode: 4112, duration: 5.015s, episode steps: 673, steps per second: 134, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.013429, mae: 3.151465, mean_q: 3.797490, mean_eps: 0.100000\n",
      " 3111403/3750000: episode: 4113, duration: 5.298s, episode steps: 704, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.018359, mae: 3.066259, mean_q: 3.687179, mean_eps: 0.100000\n",
      " 3111919/3750000: episode: 4114, duration: 3.763s, episode steps: 516, steps per second: 137, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.014331, mae: 3.142720, mean_q: 3.787108, mean_eps: 0.100000\n",
      " 3112828/3750000: episode: 4115, duration: 6.889s, episode steps: 909, steps per second: 132, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.014166, mae: 3.144465, mean_q: 3.786197, mean_eps: 0.100000\n",
      " 3113315/3750000: episode: 4116, duration: 3.596s, episode steps: 487, steps per second: 135, episode reward: 16.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 3.119 [0.000, 5.000],  loss: 0.013647, mae: 3.102160, mean_q: 3.739365, mean_eps: 0.100000\n",
      " 3114243/3750000: episode: 4117, duration: 6.950s, episode steps: 928, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.812 [0.000, 5.000],  loss: 0.019952, mae: 3.093416, mean_q: 3.724846, mean_eps: 0.100000\n",
      " 3114950/3750000: episode: 4118, duration: 5.284s, episode steps: 707, steps per second: 134, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.289 [0.000, 5.000],  loss: 0.012133, mae: 3.080295, mean_q: 3.710992, mean_eps: 0.100000\n",
      " 3115896/3750000: episode: 4119, duration: 7.050s, episode steps: 946, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.018834, mae: 3.118648, mean_q: 3.754571, mean_eps: 0.100000\n",
      " 3116833/3750000: episode: 4120, duration: 6.983s, episode steps: 937, steps per second: 134, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.016419, mae: 3.106125, mean_q: 3.735124, mean_eps: 0.100000\n",
      " 3117380/3750000: episode: 4121, duration: 4.188s, episode steps: 547, steps per second: 131, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.014765, mae: 3.101802, mean_q: 3.738564, mean_eps: 0.100000\n",
      " 3118143/3750000: episode: 4122, duration: 5.740s, episode steps: 763, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.019340, mae: 3.065966, mean_q: 3.694028, mean_eps: 0.100000\n",
      " 3119044/3750000: episode: 4123, duration: 6.681s, episode steps: 901, steps per second: 135, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.018095, mae: 3.176755, mean_q: 3.823576, mean_eps: 0.100000\n",
      " 3119958/3750000: episode: 4124, duration: 6.771s, episode steps: 914, steps per second: 135, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.014580, mae: 3.160356, mean_q: 3.804694, mean_eps: 0.100000\n",
      " 3120846/3750000: episode: 4125, duration: 6.665s, episode steps: 888, steps per second: 133, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.678 [0.000, 5.000],  loss: 0.014156, mae: 3.146123, mean_q: 3.785545, mean_eps: 0.100000\n",
      " 3121681/3750000: episode: 4126, duration: 6.317s, episode steps: 835, steps per second: 132, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.123 [0.000, 5.000],  loss: 0.019091, mae: 3.146250, mean_q: 3.788377, mean_eps: 0.100000\n",
      " 3122493/3750000: episode: 4127, duration: 5.969s, episode steps: 812, steps per second: 136, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.193 [0.000, 5.000],  loss: 0.016006, mae: 3.126706, mean_q: 3.765752, mean_eps: 0.100000\n",
      " 3123269/3750000: episode: 4128, duration: 5.823s, episode steps: 776, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.848 [0.000, 5.000],  loss: 0.009989, mae: 3.111000, mean_q: 3.743906, mean_eps: 0.100000\n",
      " 3124126/3750000: episode: 4129, duration: 6.544s, episode steps: 857, steps per second: 131, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.017905, mae: 3.129031, mean_q: 3.763798, mean_eps: 0.100000\n",
      " 3125193/3750000: episode: 4130, duration: 7.912s, episode steps: 1067, steps per second: 135, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.017379, mae: 3.185771, mean_q: 3.836366, mean_eps: 0.100000\n",
      " 3125843/3750000: episode: 4131, duration: 4.938s, episode steps: 650, steps per second: 132, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.021014, mae: 3.164925, mean_q: 3.805222, mean_eps: 0.100000\n",
      " 3126491/3750000: episode: 4132, duration: 4.785s, episode steps: 648, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.191 [0.000, 5.000],  loss: 0.011777, mae: 3.209192, mean_q: 3.862346, mean_eps: 0.100000\n",
      " 3127109/3750000: episode: 4133, duration: 4.614s, episode steps: 618, steps per second: 134, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.017219, mae: 3.149462, mean_q: 3.793563, mean_eps: 0.100000\n",
      " 3127896/3750000: episode: 4134, duration: 5.883s, episode steps: 787, steps per second: 134, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.703 [0.000, 5.000],  loss: 0.020040, mae: 3.216509, mean_q: 3.876268, mean_eps: 0.100000\n",
      " 3128903/3750000: episode: 4135, duration: 7.596s, episode steps: 1007, steps per second: 133, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.018341, mae: 3.182769, mean_q: 3.833759, mean_eps: 0.100000\n",
      " 3129573/3750000: episode: 4136, duration: 4.967s, episode steps: 670, steps per second: 135, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.512 [0.000, 5.000],  loss: 0.020597, mae: 3.160259, mean_q: 3.804125, mean_eps: 0.100000\n",
      " 3129938/3750000: episode: 4137, duration: 2.782s, episode steps: 365, steps per second: 131, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.014816, mae: 3.187452, mean_q: 3.837084, mean_eps: 0.100000\n",
      " 3131052/3750000: episode: 4138, duration: 8.297s, episode steps: 1114, steps per second: 134, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.015044, mae: 3.127802, mean_q: 3.762592, mean_eps: 0.100000\n",
      " 3131778/3750000: episode: 4139, duration: 5.374s, episode steps: 726, steps per second: 135, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.759 [0.000, 5.000],  loss: 0.020444, mae: 3.153389, mean_q: 3.799733, mean_eps: 0.100000\n",
      " 3132792/3750000: episode: 4140, duration: 7.679s, episode steps: 1014, steps per second: 132, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.015883, mae: 3.177061, mean_q: 3.821327, mean_eps: 0.100000\n",
      " 3133864/3750000: episode: 4141, duration: 7.991s, episode steps: 1072, steps per second: 134, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.013554, mae: 3.129433, mean_q: 3.762289, mean_eps: 0.100000\n",
      " 3134901/3750000: episode: 4142, duration: 8.036s, episode steps: 1037, steps per second: 129, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.014577, mae: 3.166377, mean_q: 3.806952, mean_eps: 0.100000\n",
      " 3135400/3750000: episode: 4143, duration: 3.765s, episode steps: 499, steps per second: 133, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.011220, mae: 3.098726, mean_q: 3.728962, mean_eps: 0.100000\n",
      " 3136160/3750000: episode: 4144, duration: 5.824s, episode steps: 760, steps per second: 130, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.042 [0.000, 5.000],  loss: 0.011377, mae: 3.152341, mean_q: 3.790150, mean_eps: 0.100000\n",
      " 3136694/3750000: episode: 4145, duration: 4.030s, episode steps: 534, steps per second: 132, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.016881, mae: 3.099212, mean_q: 3.724085, mean_eps: 0.100000\n",
      " 3137400/3750000: episode: 4146, duration: 5.333s, episode steps: 706, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.977 [0.000, 5.000],  loss: 0.013679, mae: 3.128021, mean_q: 3.761545, mean_eps: 0.100000\n",
      " 3138188/3750000: episode: 4147, duration: 5.939s, episode steps: 788, steps per second: 133, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.012498, mae: 3.139624, mean_q: 3.773847, mean_eps: 0.100000\n",
      " 3139059/3750000: episode: 4148, duration: 6.516s, episode steps: 871, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.013733, mae: 3.123752, mean_q: 3.761728, mean_eps: 0.100000\n",
      " 3140444/3750000: episode: 4149, duration: 10.419s, episode steps: 1385, steps per second: 133, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.714 [0.000, 5.000],  loss: 0.013109, mae: 3.186602, mean_q: 3.832742, mean_eps: 0.100000\n",
      " 3141590/3750000: episode: 4150, duration: 8.503s, episode steps: 1146, steps per second: 135, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.016609, mae: 3.131480, mean_q: 3.770059, mean_eps: 0.100000\n",
      " 3142486/3750000: episode: 4151, duration: 6.702s, episode steps: 896, steps per second: 134, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.016212, mae: 3.123690, mean_q: 3.753720, mean_eps: 0.100000\n",
      " 3143395/3750000: episode: 4152, duration: 6.818s, episode steps: 909, steps per second: 133, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.015976, mae: 3.164790, mean_q: 3.805163, mean_eps: 0.100000\n",
      " 3143878/3750000: episode: 4153, duration: 3.579s, episode steps: 483, steps per second: 135, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.018669, mae: 3.165072, mean_q: 3.802119, mean_eps: 0.100000\n",
      " 3144887/3750000: episode: 4154, duration: 7.615s, episode steps: 1009, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.112 [0.000, 5.000],  loss: 0.017951, mae: 3.131161, mean_q: 3.763550, mean_eps: 0.100000\n",
      " 3145403/3750000: episode: 4155, duration: 3.859s, episode steps: 516, steps per second: 134, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 4.012 [0.000, 5.000],  loss: 0.019129, mae: 3.168150, mean_q: 3.817341, mean_eps: 0.100000\n",
      " 3146150/3750000: episode: 4156, duration: 5.516s, episode steps: 747, steps per second: 135, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.174 [0.000, 5.000],  loss: 0.017687, mae: 3.153226, mean_q: 3.789270, mean_eps: 0.100000\n",
      " 3147165/3750000: episode: 4157, duration: 7.661s, episode steps: 1015, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.208 [0.000, 5.000],  loss: 0.018727, mae: 3.161602, mean_q: 3.801415, mean_eps: 0.100000\n",
      " 3147914/3750000: episode: 4158, duration: 5.549s, episode steps: 749, steps per second: 135, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.491 [0.000, 5.000],  loss: 0.012035, mae: 3.123060, mean_q: 3.756017, mean_eps: 0.100000\n",
      " 3148505/3750000: episode: 4159, duration: 4.435s, episode steps: 591, steps per second: 133, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.016114, mae: 3.162235, mean_q: 3.807185, mean_eps: 0.100000\n",
      " 3149240/3750000: episode: 4160, duration: 5.558s, episode steps: 735, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014117, mae: 3.130210, mean_q: 3.766247, mean_eps: 0.100000\n",
      " 3150657/3750000: episode: 4161, duration: 10.508s, episode steps: 1417, steps per second: 135, episode reward: 33.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.015664, mae: 3.058471, mean_q: 3.678078, mean_eps: 0.100000\n",
      " 3151173/3750000: episode: 4162, duration: 3.942s, episode steps: 516, steps per second: 131, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.554 [0.000, 5.000],  loss: 0.016942, mae: 3.057552, mean_q: 3.680386, mean_eps: 0.100000\n",
      " 3151994/3750000: episode: 4163, duration: 6.203s, episode steps: 821, steps per second: 132, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.451 [0.000, 5.000],  loss: 0.017313, mae: 3.035821, mean_q: 3.658904, mean_eps: 0.100000\n",
      " 3153000/3750000: episode: 4164, duration: 7.570s, episode steps: 1006, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.011803, mae: 3.084146, mean_q: 3.711573, mean_eps: 0.100000\n",
      " 3153348/3750000: episode: 4165, duration: 2.729s, episode steps: 348, steps per second: 128, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.034 [0.000, 5.000],  loss: 0.013134, mae: 3.085331, mean_q: 3.712832, mean_eps: 0.100000\n",
      " 3154146/3750000: episode: 4166, duration: 5.944s, episode steps: 798, steps per second: 134, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.373 [0.000, 5.000],  loss: 0.013593, mae: 3.117149, mean_q: 3.748944, mean_eps: 0.100000\n",
      " 3155005/3750000: episode: 4167, duration: 6.440s, episode steps: 859, steps per second: 133, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.017554, mae: 3.059215, mean_q: 3.679393, mean_eps: 0.100000\n",
      " 3155971/3750000: episode: 4168, duration: 7.241s, episode steps: 966, steps per second: 133, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.865 [0.000, 5.000],  loss: 0.013091, mae: 3.058927, mean_q: 3.680096, mean_eps: 0.100000\n",
      " 3156956/3750000: episode: 4169, duration: 7.341s, episode steps: 985, steps per second: 134, episode reward: 30.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.011324, mae: 3.086540, mean_q: 3.713522, mean_eps: 0.100000\n",
      " 3157799/3750000: episode: 4170, duration: 6.328s, episode steps: 843, steps per second: 133, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.181 [0.000, 5.000],  loss: 0.012716, mae: 3.093528, mean_q: 3.719045, mean_eps: 0.100000\n",
      " 3158734/3750000: episode: 4171, duration: 6.992s, episode steps: 935, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.579 [0.000, 5.000],  loss: 0.018485, mae: 3.051571, mean_q: 3.672358, mean_eps: 0.100000\n",
      " 3159477/3750000: episode: 4172, duration: 5.514s, episode steps: 743, steps per second: 135, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.010144, mae: 3.115911, mean_q: 3.749780, mean_eps: 0.100000\n",
      " 3160238/3750000: episode: 4173, duration: 5.713s, episode steps: 761, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.012908, mae: 3.092444, mean_q: 3.720918, mean_eps: 0.100000\n",
      " 3161040/3750000: episode: 4174, duration: 6.026s, episode steps: 802, steps per second: 133, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.013414, mae: 3.108066, mean_q: 3.736482, mean_eps: 0.100000\n",
      " 3161956/3750000: episode: 4175, duration: 6.894s, episode steps: 916, steps per second: 133, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.017888, mae: 3.114144, mean_q: 3.747809, mean_eps: 0.100000\n",
      " 3162758/3750000: episode: 4176, duration: 5.956s, episode steps: 802, steps per second: 135, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.013032, mae: 3.065150, mean_q: 3.686900, mean_eps: 0.100000\n",
      " 3163911/3750000: episode: 4177, duration: 8.608s, episode steps: 1153, steps per second: 134, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.088 [0.000, 5.000],  loss: 0.014238, mae: 3.108116, mean_q: 3.739532, mean_eps: 0.100000\n",
      " 3164762/3750000: episode: 4178, duration: 6.434s, episode steps: 851, steps per second: 132, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.549 [0.000, 5.000],  loss: 0.013148, mae: 3.074408, mean_q: 3.699449, mean_eps: 0.100000\n",
      " 3165475/3750000: episode: 4179, duration: 5.322s, episode steps: 713, steps per second: 134, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015532, mae: 3.046551, mean_q: 3.665257, mean_eps: 0.100000\n",
      " 3165992/3750000: episode: 4180, duration: 3.875s, episode steps: 517, steps per second: 133, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.917 [0.000, 5.000],  loss: 0.021350, mae: 3.015827, mean_q: 3.629045, mean_eps: 0.100000\n",
      " 3166827/3750000: episode: 4181, duration: 6.342s, episode steps: 835, steps per second: 132, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.014830, mae: 3.044804, mean_q: 3.672790, mean_eps: 0.100000\n",
      " 3167467/3750000: episode: 4182, duration: 4.856s, episode steps: 640, steps per second: 132, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.014605, mae: 3.150595, mean_q: 3.794724, mean_eps: 0.100000\n",
      " 3168129/3750000: episode: 4183, duration: 5.050s, episode steps: 662, steps per second: 131, episode reward: 19.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.012014, mae: 3.095785, mean_q: 3.723612, mean_eps: 0.100000\n",
      " 3168957/3750000: episode: 4184, duration: 6.221s, episode steps: 828, steps per second: 133, episode reward: 24.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 0.014723, mae: 3.044057, mean_q: 3.662844, mean_eps: 0.100000\n",
      " 3169740/3750000: episode: 4185, duration: 5.954s, episode steps: 783, steps per second: 132, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.097 [0.000, 5.000],  loss: 0.017669, mae: 3.037416, mean_q: 3.654607, mean_eps: 0.100000\n",
      " 3170692/3750000: episode: 4186, duration: 7.219s, episode steps: 952, steps per second: 132, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.276 [0.000, 5.000],  loss: 0.015762, mae: 3.074766, mean_q: 3.701565, mean_eps: 0.100000\n",
      " 3171414/3750000: episode: 4187, duration: 5.357s, episode steps: 722, steps per second: 135, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.699 [0.000, 5.000],  loss: 0.017345, mae: 3.076348, mean_q: 3.706317, mean_eps: 0.100000\n",
      " 3172762/3750000: episode: 4188, duration: 10.111s, episode steps: 1348, steps per second: 133, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.013561, mae: 3.071863, mean_q: 3.694535, mean_eps: 0.100000\n",
      " 3173851/3750000: episode: 4189, duration: 8.090s, episode steps: 1089, steps per second: 135, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.122 [0.000, 5.000],  loss: 0.011422, mae: 3.092872, mean_q: 3.721427, mean_eps: 0.100000\n",
      " 3174447/3750000: episode: 4190, duration: 4.450s, episode steps: 596, steps per second: 134, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.195 [0.000, 5.000],  loss: 0.018668, mae: 3.096400, mean_q: 3.728088, mean_eps: 0.100000\n",
      " 3175390/3750000: episode: 4191, duration: 7.113s, episode steps: 943, steps per second: 133, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.932 [0.000, 5.000],  loss: 0.015957, mae: 3.041211, mean_q: 3.660997, mean_eps: 0.100000\n",
      " 3175937/3750000: episode: 4192, duration: 4.027s, episode steps: 547, steps per second: 136, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.124 [0.000, 5.000],  loss: 0.014178, mae: 3.024463, mean_q: 3.638365, mean_eps: 0.100000\n",
      " 3176564/3750000: episode: 4193, duration: 4.707s, episode steps: 627, steps per second: 133, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.014852, mae: 3.053095, mean_q: 3.671728, mean_eps: 0.100000\n",
      " 3177932/3750000: episode: 4194, duration: 10.228s, episode steps: 1368, steps per second: 134, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.411 [0.000, 5.000],  loss: 0.018727, mae: 3.031337, mean_q: 3.649027, mean_eps: 0.100000\n",
      " 3178705/3750000: episode: 4195, duration: 5.770s, episode steps: 773, steps per second: 134, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.011509, mae: 3.023423, mean_q: 3.637057, mean_eps: 0.100000\n",
      " 3179720/3750000: episode: 4196, duration: 7.645s, episode steps: 1015, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.916 [0.000, 5.000],  loss: 0.013270, mae: 3.051187, mean_q: 3.670167, mean_eps: 0.100000\n",
      " 3180403/3750000: episode: 4197, duration: 5.165s, episode steps: 683, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.013898, mae: 3.019800, mean_q: 3.638742, mean_eps: 0.100000\n",
      " 3181319/3750000: episode: 4198, duration: 6.848s, episode steps: 916, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.015790, mae: 3.068060, mean_q: 3.691290, mean_eps: 0.100000\n",
      " 3182416/3750000: episode: 4199, duration: 8.146s, episode steps: 1097, steps per second: 135, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.017109, mae: 3.117900, mean_q: 3.755952, mean_eps: 0.100000\n",
      " 3183480/3750000: episode: 4200, duration: 8.178s, episode steps: 1064, steps per second: 130, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.015879, mae: 3.125846, mean_q: 3.765580, mean_eps: 0.100000\n",
      " 3184287/3750000: episode: 4201, duration: 6.144s, episode steps: 807, steps per second: 131, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.766 [0.000, 5.000],  loss: 0.013126, mae: 3.088439, mean_q: 3.716268, mean_eps: 0.100000\n",
      " 3185011/3750000: episode: 4202, duration: 5.456s, episode steps: 724, steps per second: 133, episode reward: 22.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.016521, mae: 3.147129, mean_q: 3.788130, mean_eps: 0.100000\n",
      " 3186236/3750000: episode: 4203, duration: 9.136s, episode steps: 1225, steps per second: 134, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.860 [0.000, 5.000],  loss: 0.013439, mae: 3.100168, mean_q: 3.730539, mean_eps: 0.100000\n",
      " 3187442/3750000: episode: 4204, duration: 9.021s, episode steps: 1206, steps per second: 134, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.153 [0.000, 5.000],  loss: 0.014241, mae: 3.105855, mean_q: 3.742505, mean_eps: 0.100000\n",
      " 3188186/3750000: episode: 4205, duration: 5.547s, episode steps: 744, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.108 [0.000, 5.000],  loss: 0.014605, mae: 3.076542, mean_q: 3.703790, mean_eps: 0.100000\n",
      " 3189073/3750000: episode: 4206, duration: 6.549s, episode steps: 887, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.015027, mae: 3.140957, mean_q: 3.779758, mean_eps: 0.100000\n",
      " 3189890/3750000: episode: 4207, duration: 6.125s, episode steps: 817, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.016658, mae: 3.104513, mean_q: 3.738865, mean_eps: 0.100000\n",
      " 3190706/3750000: episode: 4208, duration: 6.076s, episode steps: 816, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.627 [0.000, 5.000],  loss: 0.018784, mae: 3.148101, mean_q: 3.786253, mean_eps: 0.100000\n",
      " 3191173/3750000: episode: 4209, duration: 3.466s, episode steps: 467, steps per second: 135, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.086 [0.000, 5.000],  loss: 0.013459, mae: 3.152219, mean_q: 3.796498, mean_eps: 0.100000\n",
      " 3192425/3750000: episode: 4210, duration: 9.415s, episode steps: 1252, steps per second: 133, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.013238, mae: 3.109601, mean_q: 3.746885, mean_eps: 0.100000\n",
      " 3193235/3750000: episode: 4211, duration: 5.977s, episode steps: 810, steps per second: 136, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.320 [0.000, 5.000],  loss: 0.018335, mae: 3.116011, mean_q: 3.745361, mean_eps: 0.100000\n",
      " 3193849/3750000: episode: 4212, duration: 4.656s, episode steps: 614, steps per second: 132, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.020113, mae: 3.183067, mean_q: 3.827986, mean_eps: 0.100000\n",
      " 3194733/3750000: episode: 4213, duration: 6.570s, episode steps: 884, steps per second: 135, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.825 [0.000, 5.000],  loss: 0.014996, mae: 3.094312, mean_q: 3.724657, mean_eps: 0.100000\n",
      " 3195670/3750000: episode: 4214, duration: 7.002s, episode steps: 937, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.010693, mae: 3.123506, mean_q: 3.759744, mean_eps: 0.100000\n",
      " 3196362/3750000: episode: 4215, duration: 5.235s, episode steps: 692, steps per second: 132, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.211 [0.000, 5.000],  loss: 0.015978, mae: 3.163569, mean_q: 3.804445, mean_eps: 0.100000\n",
      " 3197313/3750000: episode: 4216, duration: 7.003s, episode steps: 951, steps per second: 136, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.014979, mae: 3.163731, mean_q: 3.809028, mean_eps: 0.100000\n",
      " 3198178/3750000: episode: 4217, duration: 6.480s, episode steps: 865, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.206 [0.000, 5.000],  loss: 0.015318, mae: 3.096224, mean_q: 3.734249, mean_eps: 0.100000\n",
      " 3199240/3750000: episode: 4218, duration: 7.939s, episode steps: 1062, steps per second: 134, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.358 [0.000, 5.000],  loss: 0.014383, mae: 3.123060, mean_q: 3.758305, mean_eps: 0.100000\n",
      " 3199976/3750000: episode: 4219, duration: 5.554s, episode steps: 736, steps per second: 133, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.012470, mae: 3.050891, mean_q: 3.667777, mean_eps: 0.100000\n",
      " 3200778/3750000: episode: 4220, duration: 6.046s, episode steps: 802, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.938 [0.000, 5.000],  loss: 0.013935, mae: 3.094460, mean_q: 3.722100, mean_eps: 0.100000\n",
      " 3201619/3750000: episode: 4221, duration: 6.268s, episode steps: 841, steps per second: 134, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.015144, mae: 3.117867, mean_q: 3.751408, mean_eps: 0.100000\n",
      " 3202036/3750000: episode: 4222, duration: 3.135s, episode steps: 417, steps per second: 133, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.827 [0.000, 5.000],  loss: 0.016536, mae: 3.145841, mean_q: 3.785630, mean_eps: 0.100000\n",
      " 3202548/3750000: episode: 4223, duration: 3.923s, episode steps: 512, steps per second: 131, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.014773, mae: 3.072792, mean_q: 3.696789, mean_eps: 0.100000\n",
      " 3203167/3750000: episode: 4224, duration: 4.650s, episode steps: 619, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.015763, mae: 3.087676, mean_q: 3.713853, mean_eps: 0.100000\n",
      " 3203954/3750000: episode: 4225, duration: 5.796s, episode steps: 787, steps per second: 136, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.011203, mae: 3.098070, mean_q: 3.731222, mean_eps: 0.100000\n",
      " 3204793/3750000: episode: 4226, duration: 6.266s, episode steps: 839, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.013496, mae: 3.134583, mean_q: 3.774662, mean_eps: 0.100000\n",
      " 3205862/3750000: episode: 4227, duration: 7.968s, episode steps: 1069, steps per second: 134, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.015954, mae: 3.152885, mean_q: 3.790479, mean_eps: 0.100000\n",
      " 3206890/3750000: episode: 4228, duration: 7.624s, episode steps: 1028, steps per second: 135, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.547 [0.000, 5.000],  loss: 0.015067, mae: 3.115827, mean_q: 3.749697, mean_eps: 0.100000\n",
      " 3207513/3750000: episode: 4229, duration: 4.659s, episode steps: 623, steps per second: 134, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.594 [0.000, 5.000],  loss: 0.012725, mae: 3.159449, mean_q: 3.808444, mean_eps: 0.100000\n",
      " 3208200/3750000: episode: 4230, duration: 5.261s, episode steps: 687, steps per second: 131, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.013810, mae: 3.136537, mean_q: 3.776637, mean_eps: 0.100000\n",
      " 3209105/3750000: episode: 4231, duration: 6.918s, episode steps: 905, steps per second: 131, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.017400, mae: 3.142367, mean_q: 3.785608, mean_eps: 0.100000\n",
      " 3210176/3750000: episode: 4232, duration: 7.910s, episode steps: 1071, steps per second: 135, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.012611, mae: 3.162530, mean_q: 3.800799, mean_eps: 0.100000\n",
      " 3211026/3750000: episode: 4233, duration: 6.515s, episode steps: 850, steps per second: 130, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.986 [0.000, 5.000],  loss: 0.018870, mae: 3.159034, mean_q: 3.798437, mean_eps: 0.100000\n",
      " 3211826/3750000: episode: 4234, duration: 5.932s, episode steps: 800, steps per second: 135, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.013878, mae: 3.221823, mean_q: 3.875869, mean_eps: 0.100000\n",
      " 3212601/3750000: episode: 4235, duration: 5.812s, episode steps: 775, steps per second: 133, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.285 [0.000, 5.000],  loss: 0.017772, mae: 3.159826, mean_q: 3.803857, mean_eps: 0.100000\n",
      " 3213421/3750000: episode: 4236, duration: 6.153s, episode steps: 820, steps per second: 133, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.093 [0.000, 5.000],  loss: 0.019513, mae: 3.142025, mean_q: 3.781773, mean_eps: 0.100000\n",
      " 3214254/3750000: episode: 4237, duration: 6.159s, episode steps: 833, steps per second: 135, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.017082, mae: 3.157800, mean_q: 3.797687, mean_eps: 0.100000\n",
      " 3215111/3750000: episode: 4238, duration: 6.379s, episode steps: 857, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.017436, mae: 3.154889, mean_q: 3.793577, mean_eps: 0.100000\n",
      " 3215814/3750000: episode: 4239, duration: 5.351s, episode steps: 703, steps per second: 131, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.014886, mae: 3.159446, mean_q: 3.800812, mean_eps: 0.100000\n",
      " 3216622/3750000: episode: 4240, duration: 6.112s, episode steps: 808, steps per second: 132, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.011928, mae: 3.113568, mean_q: 3.743124, mean_eps: 0.100000\n",
      " 3217557/3750000: episode: 4241, duration: 6.969s, episode steps: 935, steps per second: 134, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.014237, mae: 3.113493, mean_q: 3.746793, mean_eps: 0.100000\n",
      " 3218169/3750000: episode: 4242, duration: 4.629s, episode steps: 612, steps per second: 132, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.059 [0.000, 5.000],  loss: 0.016785, mae: 3.106656, mean_q: 3.747315, mean_eps: 0.100000\n",
      " 3218767/3750000: episode: 4243, duration: 4.457s, episode steps: 598, steps per second: 134, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.289 [0.000, 5.000],  loss: 0.015220, mae: 3.097610, mean_q: 3.728496, mean_eps: 0.100000\n",
      " 3219405/3750000: episode: 4244, duration: 4.800s, episode steps: 638, steps per second: 133, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 3.047 [0.000, 5.000],  loss: 0.014220, mae: 3.128919, mean_q: 3.766117, mean_eps: 0.100000\n",
      " 3220736/3750000: episode: 4245, duration: 9.880s, episode steps: 1331, steps per second: 135, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.014314, mae: 3.103004, mean_q: 3.735499, mean_eps: 0.100000\n",
      " 3221351/3750000: episode: 4246, duration: 4.597s, episode steps: 615, steps per second: 134, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.751 [0.000, 5.000],  loss: 0.016800, mae: 3.146957, mean_q: 3.789487, mean_eps: 0.100000\n",
      " 3222206/3750000: episode: 4247, duration: 6.435s, episode steps: 855, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.020489, mae: 3.093014, mean_q: 3.721519, mean_eps: 0.100000\n",
      " 3222897/3750000: episode: 4248, duration: 5.097s, episode steps: 691, steps per second: 136, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010415, mae: 3.138386, mean_q: 3.777014, mean_eps: 0.100000\n",
      " 3223405/3750000: episode: 4249, duration: 3.813s, episode steps: 508, steps per second: 133, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.016087, mae: 3.116910, mean_q: 3.751975, mean_eps: 0.100000\n",
      " 3224609/3750000: episode: 4250, duration: 8.967s, episode steps: 1204, steps per second: 134, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.016561, mae: 3.116213, mean_q: 3.748928, mean_eps: 0.100000\n",
      " 3225500/3750000: episode: 4251, duration: 6.698s, episode steps: 891, steps per second: 133, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.016362, mae: 3.105126, mean_q: 3.732609, mean_eps: 0.100000\n",
      " 3226180/3750000: episode: 4252, duration: 5.209s, episode steps: 680, steps per second: 131, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.018761, mae: 3.090038, mean_q: 3.717575, mean_eps: 0.100000\n",
      " 3227047/3750000: episode: 4253, duration: 6.514s, episode steps: 867, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.265 [0.000, 5.000],  loss: 0.011421, mae: 3.122845, mean_q: 3.761833, mean_eps: 0.100000\n",
      " 3228386/3750000: episode: 4254, duration: 10.000s, episode steps: 1339, steps per second: 134, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.550 [0.000, 5.000],  loss: 0.015519, mae: 3.035121, mean_q: 3.650798, mean_eps: 0.100000\n",
      " 3229243/3750000: episode: 4255, duration: 6.369s, episode steps: 857, steps per second: 135, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.014 [0.000, 5.000],  loss: 0.017907, mae: 3.083285, mean_q: 3.711040, mean_eps: 0.100000\n",
      " 3229802/3750000: episode: 4256, duration: 4.152s, episode steps: 559, steps per second: 135, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.014813, mae: 3.025388, mean_q: 3.639577, mean_eps: 0.100000\n",
      " 3230906/3750000: episode: 4257, duration: 8.240s, episode steps: 1104, steps per second: 134, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.014846, mae: 3.026925, mean_q: 3.645391, mean_eps: 0.100000\n",
      " 3231663/3750000: episode: 4258, duration: 5.684s, episode steps: 757, steps per second: 133, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.485 [0.000, 5.000],  loss: 0.015360, mae: 3.125895, mean_q: 3.757428, mean_eps: 0.100000\n",
      " 3232374/3750000: episode: 4259, duration: 5.281s, episode steps: 711, steps per second: 135, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.015416, mae: 3.109514, mean_q: 3.741287, mean_eps: 0.100000\n",
      " 3233094/3750000: episode: 4260, duration: 5.411s, episode steps: 720, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.018181, mae: 3.090381, mean_q: 3.717336, mean_eps: 0.100000\n",
      " 3233897/3750000: episode: 4261, duration: 5.967s, episode steps: 803, steps per second: 135, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.021194, mae: 3.059414, mean_q: 3.679228, mean_eps: 0.100000\n",
      " 3234924/3750000: episode: 4262, duration: 7.820s, episode steps: 1027, steps per second: 131, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.198 [0.000, 5.000],  loss: 0.016430, mae: 3.076950, mean_q: 3.705797, mean_eps: 0.100000\n",
      " 3236166/3750000: episode: 4263, duration: 9.223s, episode steps: 1242, steps per second: 135, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015256, mae: 3.077930, mean_q: 3.702149, mean_eps: 0.100000\n",
      " 3237047/3750000: episode: 4264, duration: 6.668s, episode steps: 881, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.013150, mae: 3.043466, mean_q: 3.660201, mean_eps: 0.100000\n",
      " 3237739/3750000: episode: 4265, duration: 5.117s, episode steps: 692, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.107 [0.000, 5.000],  loss: 0.015449, mae: 3.091170, mean_q: 3.718263, mean_eps: 0.100000\n",
      " 3238528/3750000: episode: 4266, duration: 5.925s, episode steps: 789, steps per second: 133, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.446 [0.000, 5.000],  loss: 0.013859, mae: 3.072758, mean_q: 3.699292, mean_eps: 0.100000\n",
      " 3239265/3750000: episode: 4267, duration: 5.555s, episode steps: 737, steps per second: 133, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.020389, mae: 3.099657, mean_q: 3.726368, mean_eps: 0.100000\n",
      " 3240078/3750000: episode: 4268, duration: 6.002s, episode steps: 813, steps per second: 135, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.364 [0.000, 5.000],  loss: 0.013634, mae: 3.021490, mean_q: 3.637395, mean_eps: 0.100000\n",
      " 3240860/3750000: episode: 4269, duration: 5.932s, episode steps: 782, steps per second: 132, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.015235, mae: 3.041239, mean_q: 3.662332, mean_eps: 0.100000\n",
      " 3241727/3750000: episode: 4270, duration: 6.542s, episode steps: 867, steps per second: 133, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.013630, mae: 3.115946, mean_q: 3.755589, mean_eps: 0.100000\n",
      " 3242400/3750000: episode: 4271, duration: 5.045s, episode steps: 673, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.013857, mae: 3.088253, mean_q: 3.721783, mean_eps: 0.100000\n",
      " 3243214/3750000: episode: 4272, duration: 6.174s, episode steps: 814, steps per second: 132, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.014054, mae: 3.076001, mean_q: 3.708853, mean_eps: 0.100000\n",
      " 3243968/3750000: episode: 4273, duration: 5.639s, episode steps: 754, steps per second: 134, episode reward: 21.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.012688, mae: 3.039556, mean_q: 3.659618, mean_eps: 0.100000\n",
      " 3245341/3750000: episode: 4274, duration: 10.286s, episode steps: 1373, steps per second: 133, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.555 [0.000, 5.000],  loss: 0.016377, mae: 3.025494, mean_q: 3.645529, mean_eps: 0.100000\n",
      " 3246010/3750000: episode: 4275, duration: 4.947s, episode steps: 669, steps per second: 135, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.015447, mae: 3.021677, mean_q: 3.637685, mean_eps: 0.100000\n",
      " 3247104/3750000: episode: 4276, duration: 8.175s, episode steps: 1094, steps per second: 134, episode reward: 32.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.016083, mae: 3.024320, mean_q: 3.637774, mean_eps: 0.100000\n",
      " 3247757/3750000: episode: 4277, duration: 4.950s, episode steps: 653, steps per second: 132, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.012860, mae: 2.985942, mean_q: 3.592482, mean_eps: 0.100000\n",
      " 3248787/3750000: episode: 4278, duration: 7.771s, episode steps: 1030, steps per second: 133, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.016398, mae: 2.996560, mean_q: 3.603623, mean_eps: 0.100000\n",
      " 3249825/3750000: episode: 4279, duration: 7.780s, episode steps: 1038, steps per second: 133, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.014339, mae: 3.023373, mean_q: 3.639277, mean_eps: 0.100000\n",
      " 3250709/3750000: episode: 4280, duration: 6.731s, episode steps: 884, steps per second: 131, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 0.013209, mae: 3.000768, mean_q: 3.608970, mean_eps: 0.100000\n",
      " 3251506/3750000: episode: 4281, duration: 6.037s, episode steps: 797, steps per second: 132, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.014017, mae: 3.055879, mean_q: 3.675040, mean_eps: 0.100000\n",
      " 3252614/3750000: episode: 4282, duration: 8.240s, episode steps: 1108, steps per second: 134, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.011790, mae: 3.042174, mean_q: 3.659868, mean_eps: 0.100000\n",
      " 3253060/3750000: episode: 4283, duration: 3.372s, episode steps: 446, steps per second: 132, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.017074, mae: 3.029655, mean_q: 3.643945, mean_eps: 0.100000\n",
      " 3253658/3750000: episode: 4284, duration: 4.506s, episode steps: 598, steps per second: 133, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.013518, mae: 3.098655, mean_q: 3.731817, mean_eps: 0.100000\n",
      " 3254403/3750000: episode: 4285, duration: 5.633s, episode steps: 745, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.400 [0.000, 5.000],  loss: 0.011610, mae: 3.011650, mean_q: 3.627503, mean_eps: 0.100000\n",
      " 3255120/3750000: episode: 4286, duration: 5.350s, episode steps: 717, steps per second: 134, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.976 [0.000, 5.000],  loss: 0.012426, mae: 3.085285, mean_q: 3.710560, mean_eps: 0.100000\n",
      " 3255914/3750000: episode: 4287, duration: 6.000s, episode steps: 794, steps per second: 132, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.297 [0.000, 5.000],  loss: 0.013891, mae: 3.030710, mean_q: 3.646547, mean_eps: 0.100000\n",
      " 3256907/3750000: episode: 4288, duration: 7.439s, episode steps: 993, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.014684, mae: 3.093861, mean_q: 3.726591, mean_eps: 0.100000\n",
      " 3257564/3750000: episode: 4289, duration: 4.926s, episode steps: 657, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.960 [0.000, 5.000],  loss: 0.016733, mae: 3.118266, mean_q: 3.757277, mean_eps: 0.100000\n",
      " 3258091/3750000: episode: 4290, duration: 3.970s, episode steps: 527, steps per second: 133, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.222 [0.000, 5.000],  loss: 0.014870, mae: 3.039365, mean_q: 3.660757, mean_eps: 0.100000\n",
      " 3258754/3750000: episode: 4291, duration: 4.932s, episode steps: 663, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.262 [0.000, 5.000],  loss: 0.016815, mae: 3.098144, mean_q: 3.727690, mean_eps: 0.100000\n",
      " 3260008/3750000: episode: 4292, duration: 9.385s, episode steps: 1254, steps per second: 134, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.017237, mae: 3.141864, mean_q: 3.779527, mean_eps: 0.100000\n",
      " 3261104/3750000: episode: 4293, duration: 8.245s, episode steps: 1096, steps per second: 133, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.013702, mae: 3.102118, mean_q: 3.731465, mean_eps: 0.100000\n",
      " 3261972/3750000: episode: 4294, duration: 6.413s, episode steps: 868, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.071 [0.000, 5.000],  loss: 0.018614, mae: 3.119857, mean_q: 3.751713, mean_eps: 0.100000\n",
      " 3262942/3750000: episode: 4295, duration: 7.322s, episode steps: 970, steps per second: 132, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.013879, mae: 3.044280, mean_q: 3.663615, mean_eps: 0.100000\n",
      " 3264104/3750000: episode: 4296, duration: 8.718s, episode steps: 1162, steps per second: 133, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.018411, mae: 3.015622, mean_q: 3.625150, mean_eps: 0.100000\n",
      " 3265063/3750000: episode: 4297, duration: 7.271s, episode steps: 959, steps per second: 132, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.094 [0.000, 5.000],  loss: 0.014180, mae: 3.074808, mean_q: 3.700498, mean_eps: 0.100000\n",
      " 3265867/3750000: episode: 4298, duration: 6.002s, episode steps: 804, steps per second: 134, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.014526, mae: 3.010922, mean_q: 3.627032, mean_eps: 0.100000\n",
      " 3266710/3750000: episode: 4299, duration: 6.321s, episode steps: 843, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.734 [0.000, 5.000],  loss: 0.016796, mae: 3.012391, mean_q: 3.623235, mean_eps: 0.100000\n",
      " 3267570/3750000: episode: 4300, duration: 6.380s, episode steps: 860, steps per second: 135, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.016708, mae: 3.064822, mean_q: 3.686013, mean_eps: 0.100000\n",
      " 3268762/3750000: episode: 4301, duration: 9.061s, episode steps: 1192, steps per second: 132, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.015042, mae: 3.067981, mean_q: 3.693476, mean_eps: 0.100000\n",
      " 3269879/3750000: episode: 4302, duration: 8.286s, episode steps: 1117, steps per second: 135, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.016443, mae: 3.059876, mean_q: 3.685116, mean_eps: 0.100000\n",
      " 3270881/3750000: episode: 4303, duration: 7.593s, episode steps: 1002, steps per second: 132, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 0.015248, mae: 3.111597, mean_q: 3.748737, mean_eps: 0.100000\n",
      " 3271419/3750000: episode: 4304, duration: 3.923s, episode steps: 538, steps per second: 137, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.835 [0.000, 5.000],  loss: 0.012237, mae: 3.097958, mean_q: 3.731889, mean_eps: 0.100000\n",
      " 3272300/3750000: episode: 4305, duration: 6.661s, episode steps: 881, steps per second: 132, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.012462, mae: 3.079640, mean_q: 3.708028, mean_eps: 0.100000\n",
      " 3273489/3750000: episode: 4306, duration: 8.956s, episode steps: 1189, steps per second: 133, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.017587, mae: 3.123656, mean_q: 3.759704, mean_eps: 0.100000\n",
      " 3274390/3750000: episode: 4307, duration: 6.687s, episode steps: 901, steps per second: 135, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.055 [0.000, 5.000],  loss: 0.016067, mae: 3.166485, mean_q: 3.813878, mean_eps: 0.100000\n",
      " 3275109/3750000: episode: 4308, duration: 5.421s, episode steps: 719, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.203 [0.000, 5.000],  loss: 0.015173, mae: 3.067666, mean_q: 3.692324, mean_eps: 0.100000\n",
      " 3275747/3750000: episode: 4309, duration: 4.804s, episode steps: 638, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.015482, mae: 3.139905, mean_q: 3.782401, mean_eps: 0.100000\n",
      " 3277075/3750000: episode: 4310, duration: 9.862s, episode steps: 1328, steps per second: 135, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.013070, mae: 3.117487, mean_q: 3.754662, mean_eps: 0.100000\n",
      " 3277824/3750000: episode: 4311, duration: 5.657s, episode steps: 749, steps per second: 132, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.022241, mae: 3.084007, mean_q: 3.712109, mean_eps: 0.100000\n",
      " 3278800/3750000: episode: 4312, duration: 7.244s, episode steps: 976, steps per second: 135, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.013778, mae: 3.052729, mean_q: 3.671221, mean_eps: 0.100000\n",
      " 3279676/3750000: episode: 4313, duration: 6.644s, episode steps: 876, steps per second: 132, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.002 [0.000, 5.000],  loss: 0.011874, mae: 3.062207, mean_q: 3.682733, mean_eps: 0.100000\n",
      " 3280505/3750000: episode: 4314, duration: 6.214s, episode steps: 829, steps per second: 133, episode reward: 27.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 1.737 [0.000, 5.000],  loss: 0.016645, mae: 3.048430, mean_q: 3.664788, mean_eps: 0.100000\n",
      " 3281438/3750000: episode: 4315, duration: 6.941s, episode steps: 933, steps per second: 134, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.900 [0.000, 5.000],  loss: 0.012037, mae: 3.045274, mean_q: 3.661659, mean_eps: 0.100000\n",
      " 3282486/3750000: episode: 4316, duration: 7.899s, episode steps: 1048, steps per second: 133, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.017028, mae: 3.070572, mean_q: 3.696843, mean_eps: 0.100000\n",
      " 3283350/3750000: episode: 4317, duration: 6.506s, episode steps: 864, steps per second: 133, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.264 [0.000, 5.000],  loss: 0.016313, mae: 3.102602, mean_q: 3.736113, mean_eps: 0.100000\n",
      " 3284122/3750000: episode: 4318, duration: 5.878s, episode steps: 772, steps per second: 131, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.738 [0.000, 5.000],  loss: 0.016292, mae: 3.118219, mean_q: 3.753573, mean_eps: 0.100000\n",
      " 3284986/3750000: episode: 4319, duration: 6.378s, episode steps: 864, steps per second: 135, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.017595, mae: 3.111005, mean_q: 3.746657, mean_eps: 0.100000\n",
      " 3285681/3750000: episode: 4320, duration: 5.303s, episode steps: 695, steps per second: 131, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.921 [0.000, 5.000],  loss: 0.014379, mae: 3.076266, mean_q: 3.699329, mean_eps: 0.100000\n",
      " 3286022/3750000: episode: 4321, duration: 2.595s, episode steps: 341, steps per second: 131, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.013730, mae: 3.102385, mean_q: 3.739046, mean_eps: 0.100000\n",
      " 3287233/3750000: episode: 4322, duration: 8.956s, episode steps: 1211, steps per second: 135, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.014719, mae: 3.120517, mean_q: 3.757832, mean_eps: 0.100000\n",
      " 3288409/3750000: episode: 4323, duration: 8.869s, episode steps: 1176, steps per second: 133, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.013357, mae: 3.124462, mean_q: 3.758463, mean_eps: 0.100000\n",
      " 3289022/3750000: episode: 4324, duration: 4.600s, episode steps: 613, steps per second: 133, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.584 [0.000, 5.000],  loss: 0.011456, mae: 3.105852, mean_q: 3.737917, mean_eps: 0.100000\n",
      " 3290079/3750000: episode: 4325, duration: 7.858s, episode steps: 1057, steps per second: 135, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.015349, mae: 3.161025, mean_q: 3.806099, mean_eps: 0.100000\n",
      " 3290912/3750000: episode: 4326, duration: 6.218s, episode steps: 833, steps per second: 134, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.923 [0.000, 5.000],  loss: 0.014801, mae: 3.064790, mean_q: 3.682392, mean_eps: 0.100000\n",
      " 3291490/3750000: episode: 4327, duration: 4.366s, episode steps: 578, steps per second: 132, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.284 [0.000, 5.000],  loss: 0.008570, mae: 3.126396, mean_q: 3.769822, mean_eps: 0.100000\n",
      " 3292853/3750000: episode: 4328, duration: 10.168s, episode steps: 1363, steps per second: 134, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.067 [0.000, 5.000],  loss: 0.015703, mae: 3.135021, mean_q: 3.768819, mean_eps: 0.100000\n",
      " 3293408/3750000: episode: 4329, duration: 4.194s, episode steps: 555, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.591 [0.000, 5.000],  loss: 0.017632, mae: 3.081836, mean_q: 3.709469, mean_eps: 0.100000\n",
      " 3294528/3750000: episode: 4330, duration: 8.691s, episode steps: 1120, steps per second: 129, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.802 [0.000, 5.000],  loss: 0.013864, mae: 3.130225, mean_q: 3.767010, mean_eps: 0.100000\n",
      " 3295508/3750000: episode: 4331, duration: 7.340s, episode steps: 980, steps per second: 134, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.377 [0.000, 5.000],  loss: 0.015846, mae: 3.130349, mean_q: 3.767133, mean_eps: 0.100000\n",
      " 3296431/3750000: episode: 4332, duration: 6.973s, episode steps: 923, steps per second: 132, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.015076, mae: 3.135331, mean_q: 3.770261, mean_eps: 0.100000\n",
      " 3297360/3750000: episode: 4333, duration: 6.982s, episode steps: 929, steps per second: 133, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.326 [0.000, 5.000],  loss: 0.015775, mae: 3.195941, mean_q: 3.848049, mean_eps: 0.100000\n",
      " 3298149/3750000: episode: 4334, duration: 5.965s, episode steps: 789, steps per second: 132, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.019810, mae: 3.192935, mean_q: 3.843350, mean_eps: 0.100000\n",
      " 3299416/3750000: episode: 4335, duration: 9.474s, episode steps: 1267, steps per second: 134, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.016443, mae: 3.106677, mean_q: 3.739593, mean_eps: 0.100000\n",
      " 3300481/3750000: episode: 4336, duration: 8.018s, episode steps: 1065, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.730 [0.000, 5.000],  loss: 0.018483, mae: 3.112151, mean_q: 3.745143, mean_eps: 0.100000\n",
      " 3301421/3750000: episode: 4337, duration: 7.053s, episode steps: 940, steps per second: 133, episode reward: 29.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.012798, mae: 3.092278, mean_q: 3.719638, mean_eps: 0.100000\n",
      " 3302078/3750000: episode: 4338, duration: 4.865s, episode steps: 657, steps per second: 135, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.019842, mae: 3.093480, mean_q: 3.718908, mean_eps: 0.100000\n",
      " 3303148/3750000: episode: 4339, duration: 8.068s, episode steps: 1070, steps per second: 133, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.018385, mae: 3.085794, mean_q: 3.715069, mean_eps: 0.100000\n",
      " 3303826/3750000: episode: 4340, duration: 5.145s, episode steps: 678, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.176 [0.000, 5.000],  loss: 0.015327, mae: 3.119791, mean_q: 3.754202, mean_eps: 0.100000\n",
      " 3304220/3750000: episode: 4341, duration: 2.966s, episode steps: 394, steps per second: 133, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.012776, mae: 3.098654, mean_q: 3.735082, mean_eps: 0.100000\n",
      " 3305102/3750000: episode: 4342, duration: 6.743s, episode steps: 882, steps per second: 131, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.012926, mae: 3.116288, mean_q: 3.748574, mean_eps: 0.100000\n",
      " 3306104/3750000: episode: 4343, duration: 7.504s, episode steps: 1002, steps per second: 134, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.014551, mae: 3.113874, mean_q: 3.744709, mean_eps: 0.100000\n",
      " 3307255/3750000: episode: 4344, duration: 8.570s, episode steps: 1151, steps per second: 134, episode reward: 31.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.014090, mae: 3.145976, mean_q: 3.785990, mean_eps: 0.100000\n",
      " 3307971/3750000: episode: 4345, duration: 5.389s, episode steps: 716, steps per second: 133, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.314 [0.000, 5.000],  loss: 0.014977, mae: 3.106100, mean_q: 3.740171, mean_eps: 0.100000\n",
      " 3308574/3750000: episode: 4346, duration: 4.465s, episode steps: 603, steps per second: 135, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.139 [0.000, 5.000],  loss: 0.011320, mae: 3.102898, mean_q: 3.731497, mean_eps: 0.100000\n",
      " 3309327/3750000: episode: 4347, duration: 5.690s, episode steps: 753, steps per second: 132, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.201 [0.000, 5.000],  loss: 0.017832, mae: 3.167014, mean_q: 3.811054, mean_eps: 0.100000\n",
      " 3310172/3750000: episode: 4348, duration: 6.266s, episode steps: 845, steps per second: 135, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.013745, mae: 3.104881, mean_q: 3.734483, mean_eps: 0.100000\n",
      " 3311020/3750000: episode: 4349, duration: 6.380s, episode steps: 848, steps per second: 133, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.012742, mae: 3.210244, mean_q: 3.862388, mean_eps: 0.100000\n",
      " 3311784/3750000: episode: 4350, duration: 5.872s, episode steps: 764, steps per second: 130, episode reward: 24.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.293 [0.000, 5.000],  loss: 0.021527, mae: 3.135654, mean_q: 3.770571, mean_eps: 0.100000\n",
      " 3312760/3750000: episode: 4351, duration: 7.299s, episode steps: 976, steps per second: 134, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.471 [0.000, 5.000],  loss: 0.015087, mae: 3.155312, mean_q: 3.800780, mean_eps: 0.100000\n",
      " 3313229/3750000: episode: 4352, duration: 3.550s, episode steps: 469, steps per second: 132, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.017437, mae: 3.115723, mean_q: 3.750822, mean_eps: 0.100000\n",
      " 3314532/3750000: episode: 4353, duration: 9.761s, episode steps: 1303, steps per second: 133, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.018288, mae: 3.155303, mean_q: 3.797001, mean_eps: 0.100000\n",
      " 3314991/3750000: episode: 4354, duration: 3.437s, episode steps: 459, steps per second: 134, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 4.453 [0.000, 5.000],  loss: 0.013954, mae: 3.153420, mean_q: 3.800824, mean_eps: 0.100000\n",
      " 3315875/3750000: episode: 4355, duration: 6.652s, episode steps: 884, steps per second: 133, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.207 [0.000, 5.000],  loss: 0.013089, mae: 3.150561, mean_q: 3.791758, mean_eps: 0.100000\n",
      " 3316451/3750000: episode: 4356, duration: 4.340s, episode steps: 576, steps per second: 133, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.014917, mae: 3.211501, mean_q: 3.860850, mean_eps: 0.100000\n",
      " 3317896/3750000: episode: 4357, duration: 10.780s, episode steps: 1445, steps per second: 134, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.473 [0.000, 5.000],  loss: 0.018954, mae: 3.244673, mean_q: 3.901191, mean_eps: 0.100000\n",
      " 3318441/3750000: episode: 4358, duration: 4.149s, episode steps: 545, steps per second: 131, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.016639, mae: 3.223370, mean_q: 3.876060, mean_eps: 0.100000\n",
      " 3319241/3750000: episode: 4359, duration: 5.971s, episode steps: 800, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.015385, mae: 3.123741, mean_q: 3.756069, mean_eps: 0.100000\n",
      " 3320406/3750000: episode: 4360, duration: 8.705s, episode steps: 1165, steps per second: 134, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.642 [0.000, 5.000],  loss: 0.015140, mae: 3.156907, mean_q: 3.797504, mean_eps: 0.100000\n",
      " 3320931/3750000: episode: 4361, duration: 3.893s, episode steps: 525, steps per second: 135, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.012507, mae: 3.135922, mean_q: 3.769859, mean_eps: 0.100000\n",
      " 3321712/3750000: episode: 4362, duration: 5.854s, episode steps: 781, steps per second: 133, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 0.018608, mae: 3.174823, mean_q: 3.813284, mean_eps: 0.100000\n",
      " 3322436/3750000: episode: 4363, duration: 5.506s, episode steps: 724, steps per second: 131, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.644 [0.000, 5.000],  loss: 0.017575, mae: 3.115420, mean_q: 3.742883, mean_eps: 0.100000\n",
      " 3323385/3750000: episode: 4364, duration: 7.145s, episode steps: 949, steps per second: 133, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 0.012133, mae: 3.139434, mean_q: 3.777600, mean_eps: 0.100000\n",
      " 3324180/3750000: episode: 4365, duration: 6.014s, episode steps: 795, steps per second: 132, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.014398, mae: 3.119095, mean_q: 3.753055, mean_eps: 0.100000\n",
      " 3325694/3750000: episode: 4366, duration: 11.325s, episode steps: 1514, steps per second: 134, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.174 [0.000, 5.000],  loss: 0.016099, mae: 3.140433, mean_q: 3.779304, mean_eps: 0.100000\n",
      " 3326475/3750000: episode: 4367, duration: 5.919s, episode steps: 781, steps per second: 132, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.016537, mae: 3.114908, mean_q: 3.751820, mean_eps: 0.100000\n",
      " 3327697/3750000: episode: 4368, duration: 9.223s, episode steps: 1222, steps per second: 132, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.017636, mae: 3.171908, mean_q: 3.818938, mean_eps: 0.100000\n",
      " 3328269/3750000: episode: 4369, duration: 4.418s, episode steps: 572, steps per second: 129, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.266 [0.000, 5.000],  loss: 0.013108, mae: 3.104591, mean_q: 3.743141, mean_eps: 0.100000\n",
      " 3329089/3750000: episode: 4370, duration: 6.163s, episode steps: 820, steps per second: 133, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.020104, mae: 3.107971, mean_q: 3.738957, mean_eps: 0.100000\n",
      " 3329978/3750000: episode: 4371, duration: 6.602s, episode steps: 889, steps per second: 135, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.159 [0.000, 5.000],  loss: 0.015885, mae: 3.153479, mean_q: 3.793498, mean_eps: 0.100000\n",
      " 3331113/3750000: episode: 4372, duration: 8.557s, episode steps: 1135, steps per second: 133, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.108 [0.000, 5.000],  loss: 0.017900, mae: 3.088328, mean_q: 3.719952, mean_eps: 0.100000\n",
      " 3331863/3750000: episode: 4373, duration: 5.615s, episode steps: 750, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.014548, mae: 3.105212, mean_q: 3.739279, mean_eps: 0.100000\n",
      " 3332632/3750000: episode: 4374, duration: 5.761s, episode steps: 769, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.986 [0.000, 5.000],  loss: 0.017572, mae: 3.078454, mean_q: 3.707908, mean_eps: 0.100000\n",
      " 3333338/3750000: episode: 4375, duration: 5.268s, episode steps: 706, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.432 [0.000, 5.000],  loss: 0.013926, mae: 3.100508, mean_q: 3.733120, mean_eps: 0.100000\n",
      " 3333979/3750000: episode: 4376, duration: 4.772s, episode steps: 641, steps per second: 134, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.803 [0.000, 5.000],  loss: 0.017825, mae: 3.094581, mean_q: 3.721669, mean_eps: 0.100000\n",
      " 3335763/3750000: episode: 4377, duration: 13.407s, episode steps: 1784, steps per second: 133, episode reward: 39.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.015222, mae: 3.128187, mean_q: 3.764637, mean_eps: 0.100000\n",
      " 3336224/3750000: episode: 4378, duration: 3.459s, episode steps: 461, steps per second: 133, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.839 [0.000, 5.000],  loss: 0.013855, mae: 3.130327, mean_q: 3.774215, mean_eps: 0.100000\n",
      " 3337019/3750000: episode: 4379, duration: 5.908s, episode steps: 795, steps per second: 135, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012017, mae: 3.103336, mean_q: 3.743669, mean_eps: 0.100000\n",
      " 3337972/3750000: episode: 4380, duration: 7.114s, episode steps: 953, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.957 [0.000, 5.000],  loss: 0.011862, mae: 3.095953, mean_q: 3.728824, mean_eps: 0.100000\n",
      " 3338945/3750000: episode: 4381, duration: 7.326s, episode steps: 973, steps per second: 133, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.247 [0.000, 5.000],  loss: 0.013016, mae: 3.098411, mean_q: 3.728665, mean_eps: 0.100000\n",
      " 3339657/3750000: episode: 4382, duration: 5.261s, episode steps: 712, steps per second: 135, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.010587, mae: 3.135604, mean_q: 3.776653, mean_eps: 0.100000\n",
      " 3340587/3750000: episode: 4383, duration: 7.027s, episode steps: 930, steps per second: 132, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.013835, mae: 3.089106, mean_q: 3.716891, mean_eps: 0.100000\n",
      " 3340987/3750000: episode: 4384, duration: 2.986s, episode steps: 400, steps per second: 134, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 4.237 [0.000, 5.000],  loss: 0.022574, mae: 3.069653, mean_q: 3.691429, mean_eps: 0.100000\n",
      " 3341451/3750000: episode: 4385, duration: 3.493s, episode steps: 464, steps per second: 133, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 4.110 [0.000, 5.000],  loss: 0.013466, mae: 3.130998, mean_q: 3.767209, mean_eps: 0.100000\n",
      " 3342103/3750000: episode: 4386, duration: 4.913s, episode steps: 652, steps per second: 133, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.020608, mae: 3.100462, mean_q: 3.723710, mean_eps: 0.100000\n",
      " 3342830/3750000: episode: 4387, duration: 5.358s, episode steps: 727, steps per second: 136, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.751 [0.000, 5.000],  loss: 0.016282, mae: 3.164149, mean_q: 3.811309, mean_eps: 0.100000\n",
      " 3343497/3750000: episode: 4388, duration: 5.035s, episode steps: 667, steps per second: 132, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.016944, mae: 3.140631, mean_q: 3.782485, mean_eps: 0.100000\n",
      " 3344603/3750000: episode: 4389, duration: 8.290s, episode steps: 1106, steps per second: 133, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.015444, mae: 3.142812, mean_q: 3.781714, mean_eps: 0.100000\n",
      " 3345208/3750000: episode: 4390, duration: 4.505s, episode steps: 605, steps per second: 134, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 0.015339, mae: 3.159196, mean_q: 3.798421, mean_eps: 0.100000\n",
      " 3346086/3750000: episode: 4391, duration: 6.724s, episode steps: 878, steps per second: 131, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.017804, mae: 3.161158, mean_q: 3.805343, mean_eps: 0.100000\n",
      " 3347210/3750000: episode: 4392, duration: 8.330s, episode steps: 1124, steps per second: 135, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 0.013728, mae: 3.199790, mean_q: 3.848086, mean_eps: 0.100000\n",
      " 3347731/3750000: episode: 4393, duration: 3.949s, episode steps: 521, steps per second: 132, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.321 [0.000, 5.000],  loss: 0.015121, mae: 3.192567, mean_q: 3.839173, mean_eps: 0.100000\n",
      " 3348766/3750000: episode: 4394, duration: 7.699s, episode steps: 1035, steps per second: 134, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.014388, mae: 3.164870, mean_q: 3.805913, mean_eps: 0.100000\n",
      " 3349921/3750000: episode: 4395, duration: 8.678s, episode steps: 1155, steps per second: 133, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.012511, mae: 3.177346, mean_q: 3.828422, mean_eps: 0.100000\n",
      " 3350733/3750000: episode: 4396, duration: 5.984s, episode steps: 812, steps per second: 136, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.792 [0.000, 5.000],  loss: 0.019552, mae: 3.257296, mean_q: 3.919308, mean_eps: 0.100000\n",
      " 3351355/3750000: episode: 4397, duration: 4.658s, episode steps: 622, steps per second: 134, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.064 [0.000, 5.000],  loss: 0.015727, mae: 3.292547, mean_q: 3.962825, mean_eps: 0.100000\n",
      " 3352452/3750000: episode: 4398, duration: 8.244s, episode steps: 1097, steps per second: 133, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.971 [0.000, 5.000],  loss: 0.014382, mae: 3.282105, mean_q: 3.948060, mean_eps: 0.100000\n",
      " 3352942/3750000: episode: 4399, duration: 3.706s, episode steps: 490, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.910 [0.000, 5.000],  loss: 0.015554, mae: 3.257944, mean_q: 3.915541, mean_eps: 0.100000\n",
      " 3353519/3750000: episode: 4400, duration: 4.244s, episode steps: 577, steps per second: 136, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.953 [0.000, 5.000],  loss: 0.012467, mae: 3.280199, mean_q: 3.952255, mean_eps: 0.100000\n",
      " 3354434/3750000: episode: 4401, duration: 6.976s, episode steps: 915, steps per second: 131, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.919 [0.000, 5.000],  loss: 0.015496, mae: 3.219976, mean_q: 3.873793, mean_eps: 0.100000\n",
      " 3355220/3750000: episode: 4402, duration: 5.980s, episode steps: 786, steps per second: 131, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.186 [0.000, 5.000],  loss: 0.016359, mae: 3.238895, mean_q: 3.900815, mean_eps: 0.100000\n",
      " 3355661/3750000: episode: 4403, duration: 3.365s, episode steps: 441, steps per second: 131, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 4.000 [0.000, 5.000],  loss: 0.019678, mae: 3.120297, mean_q: 3.757437, mean_eps: 0.100000\n",
      " 3356927/3750000: episode: 4404, duration: 9.459s, episode steps: 1266, steps per second: 134, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.015573, mae: 3.178180, mean_q: 3.825882, mean_eps: 0.100000\n",
      " 3357773/3750000: episode: 4405, duration: 6.280s, episode steps: 846, steps per second: 135, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.521 [0.000, 5.000],  loss: 0.014585, mae: 3.240972, mean_q: 3.905057, mean_eps: 0.100000\n",
      " 3359059/3750000: episode: 4406, duration: 9.598s, episode steps: 1286, steps per second: 134, episode reward: 28.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.012762, mae: 3.273386, mean_q: 3.943158, mean_eps: 0.100000\n",
      " 3359701/3750000: episode: 4407, duration: 4.974s, episode steps: 642, steps per second: 129, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.014636, mae: 3.147320, mean_q: 3.789115, mean_eps: 0.100000\n",
      " 3360539/3750000: episode: 4408, duration: 6.218s, episode steps: 838, steps per second: 135, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.016613, mae: 3.243456, mean_q: 3.899311, mean_eps: 0.100000\n",
      " 3361150/3750000: episode: 4409, duration: 4.620s, episode steps: 611, steps per second: 132, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.014364, mae: 3.267396, mean_q: 3.929425, mean_eps: 0.100000\n",
      " 3361831/3750000: episode: 4410, duration: 5.063s, episode steps: 681, steps per second: 135, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.411 [0.000, 5.000],  loss: 0.015162, mae: 3.202413, mean_q: 3.853027, mean_eps: 0.100000\n",
      " 3362858/3750000: episode: 4411, duration: 7.701s, episode steps: 1027, steps per second: 133, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.020327, mae: 3.274511, mean_q: 3.939600, mean_eps: 0.100000\n",
      " 3363510/3750000: episode: 4412, duration: 4.930s, episode steps: 652, steps per second: 132, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.015927, mae: 3.255541, mean_q: 3.921521, mean_eps: 0.100000\n",
      " 3364535/3750000: episode: 4413, duration: 7.689s, episode steps: 1025, steps per second: 133, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.015129, mae: 3.280038, mean_q: 3.947378, mean_eps: 0.100000\n",
      " 3365279/3750000: episode: 4414, duration: 5.564s, episode steps: 744, steps per second: 134, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 0.017668, mae: 3.187432, mean_q: 3.844022, mean_eps: 0.100000\n",
      " 3365879/3750000: episode: 4415, duration: 4.465s, episode steps: 600, steps per second: 134, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.017527, mae: 3.265854, mean_q: 3.928073, mean_eps: 0.100000\n",
      " 3366540/3750000: episode: 4416, duration: 5.036s, episode steps: 661, steps per second: 131, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.015662, mae: 3.236090, mean_q: 3.895063, mean_eps: 0.100000\n",
      " 3367318/3750000: episode: 4417, duration: 5.866s, episode steps: 778, steps per second: 133, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.608 [0.000, 5.000],  loss: 0.014652, mae: 3.302039, mean_q: 3.984928, mean_eps: 0.100000\n",
      " 3368169/3750000: episode: 4418, duration: 6.434s, episode steps: 851, steps per second: 132, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.012516, mae: 3.310375, mean_q: 3.984973, mean_eps: 0.100000\n",
      " 3368621/3750000: episode: 4419, duration: 3.443s, episode steps: 452, steps per second: 131, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.015390, mae: 3.265335, mean_q: 3.929689, mean_eps: 0.100000\n",
      " 3369467/3750000: episode: 4420, duration: 6.354s, episode steps: 846, steps per second: 133, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.013644, mae: 3.319626, mean_q: 3.999489, mean_eps: 0.100000\n",
      " 3370085/3750000: episode: 4421, duration: 4.649s, episode steps: 618, steps per second: 133, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.581 [0.000, 5.000],  loss: 0.018196, mae: 3.335230, mean_q: 4.019878, mean_eps: 0.100000\n",
      " 3370722/3750000: episode: 4422, duration: 4.768s, episode steps: 637, steps per second: 134, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.381 [0.000, 5.000],  loss: 0.015580, mae: 3.328190, mean_q: 4.008913, mean_eps: 0.100000\n",
      " 3371637/3750000: episode: 4423, duration: 6.886s, episode steps: 915, steps per second: 133, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.018833, mae: 3.340492, mean_q: 4.022500, mean_eps: 0.100000\n",
      " 3372517/3750000: episode: 4424, duration: 6.579s, episode steps: 880, steps per second: 134, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013313, mae: 3.332805, mean_q: 4.010395, mean_eps: 0.100000\n",
      " 3373228/3750000: episode: 4425, duration: 5.407s, episode steps: 711, steps per second: 131, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.710 [0.000, 5.000],  loss: 0.014935, mae: 3.358631, mean_q: 4.040661, mean_eps: 0.100000\n",
      " 3374069/3750000: episode: 4426, duration: 6.305s, episode steps: 841, steps per second: 133, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.019953, mae: 3.304502, mean_q: 3.976430, mean_eps: 0.100000\n",
      " 3375375/3750000: episode: 4427, duration: 9.776s, episode steps: 1306, steps per second: 134, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.018648, mae: 3.283270, mean_q: 3.948522, mean_eps: 0.100000\n",
      " 3376181/3750000: episode: 4428, duration: 6.122s, episode steps: 806, steps per second: 132, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.017320, mae: 3.292092, mean_q: 3.966406, mean_eps: 0.100000\n",
      " 3377934/3750000: episode: 4429, duration: 13.048s, episode steps: 1753, steps per second: 134, episode reward: 35.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.014823, mae: 3.304793, mean_q: 3.983476, mean_eps: 0.100000\n",
      " 3378895/3750000: episode: 4430, duration: 7.181s, episode steps: 961, steps per second: 134, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.013736, mae: 3.350976, mean_q: 4.032920, mean_eps: 0.100000\n",
      " 3379232/3750000: episode: 4431, duration: 2.548s, episode steps: 337, steps per second: 132, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.902 [0.000, 5.000],  loss: 0.013771, mae: 3.305613, mean_q: 3.978475, mean_eps: 0.100000\n",
      " 3379676/3750000: episode: 4432, duration: 3.357s, episode steps: 444, steps per second: 132, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.015617, mae: 3.265229, mean_q: 3.928868, mean_eps: 0.100000\n",
      " 3380436/3750000: episode: 4433, duration: 5.715s, episode steps: 760, steps per second: 133, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.021992, mae: 3.351883, mean_q: 4.034717, mean_eps: 0.100000\n",
      " 3381463/3750000: episode: 4434, duration: 7.680s, episode steps: 1027, steps per second: 134, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.358 [0.000, 5.000],  loss: 0.015659, mae: 3.306091, mean_q: 3.979170, mean_eps: 0.100000\n",
      " 3382415/3750000: episode: 4435, duration: 7.076s, episode steps: 952, steps per second: 135, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.317 [0.000, 5.000],  loss: 0.016860, mae: 3.359129, mean_q: 4.042534, mean_eps: 0.100000\n",
      " 3382788/3750000: episode: 4436, duration: 2.809s, episode steps: 373, steps per second: 133, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 0.775 [0.000, 5.000],  loss: 0.020898, mae: 3.351253, mean_q: 4.041201, mean_eps: 0.100000\n",
      " 3383481/3750000: episode: 4437, duration: 5.190s, episode steps: 693, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.015120, mae: 3.366769, mean_q: 4.056535, mean_eps: 0.100000\n",
      " 3384194/3750000: episode: 4438, duration: 5.332s, episode steps: 713, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.012902, mae: 3.436496, mean_q: 4.134165, mean_eps: 0.100000\n",
      " 3384969/3750000: episode: 4439, duration: 5.774s, episode steps: 775, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.742 [0.000, 5.000],  loss: 0.018432, mae: 3.424908, mean_q: 4.116362, mean_eps: 0.100000\n",
      " 3386229/3750000: episode: 4440, duration: 9.477s, episode steps: 1260, steps per second: 133, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.017329, mae: 3.352288, mean_q: 4.033323, mean_eps: 0.100000\n",
      " 3386876/3750000: episode: 4441, duration: 4.776s, episode steps: 647, steps per second: 135, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.530 [0.000, 5.000],  loss: 0.020897, mae: 3.328111, mean_q: 4.012039, mean_eps: 0.100000\n",
      " 3387626/3750000: episode: 4442, duration: 5.638s, episode steps: 750, steps per second: 133, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.015229, mae: 3.353566, mean_q: 4.033213, mean_eps: 0.100000\n",
      " 3388245/3750000: episode: 4443, duration: 4.712s, episode steps: 619, steps per second: 131, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.011499, mae: 3.319257, mean_q: 3.990053, mean_eps: 0.100000\n",
      " 3389303/3750000: episode: 4444, duration: 7.862s, episode steps: 1058, steps per second: 135, episode reward: 32.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.194 [0.000, 5.000],  loss: 0.015235, mae: 3.354686, mean_q: 4.034357, mean_eps: 0.100000\n",
      " 3390364/3750000: episode: 4445, duration: 7.956s, episode steps: 1061, steps per second: 133, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.117 [0.000, 5.000],  loss: 0.015684, mae: 3.374671, mean_q: 4.058719, mean_eps: 0.100000\n",
      " 3390979/3750000: episode: 4446, duration: 4.509s, episode steps: 615, steps per second: 136, episode reward: 19.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 3.605 [0.000, 5.000],  loss: 0.014592, mae: 3.412011, mean_q: 4.109319, mean_eps: 0.100000\n",
      " 3391639/3750000: episode: 4447, duration: 4.969s, episode steps: 660, steps per second: 133, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.495 [0.000, 5.000],  loss: 0.016279, mae: 3.436907, mean_q: 4.137113, mean_eps: 0.100000\n",
      " 3392286/3750000: episode: 4448, duration: 4.961s, episode steps: 647, steps per second: 130, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.013368, mae: 3.435837, mean_q: 4.134933, mean_eps: 0.100000\n",
      " 3393090/3750000: episode: 4449, duration: 6.005s, episode steps: 804, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.020123, mae: 3.477548, mean_q: 4.189017, mean_eps: 0.100000\n",
      " 3393604/3750000: episode: 4450, duration: 3.879s, episode steps: 514, steps per second: 133, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 0.016145, mae: 3.426305, mean_q: 4.120835, mean_eps: 0.100000\n",
      " 3394234/3750000: episode: 4451, duration: 4.611s, episode steps: 630, steps per second: 137, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.149 [0.000, 5.000],  loss: 0.016993, mae: 3.404074, mean_q: 4.104190, mean_eps: 0.100000\n",
      " 3395063/3750000: episode: 4452, duration: 6.312s, episode steps: 829, steps per second: 131, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.022664, mae: 3.514712, mean_q: 4.232189, mean_eps: 0.100000\n",
      " 3396190/3750000: episode: 4453, duration: 8.390s, episode steps: 1127, steps per second: 134, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.020583, mae: 3.480550, mean_q: 4.191009, mean_eps: 0.100000\n",
      " 3397014/3750000: episode: 4454, duration: 6.184s, episode steps: 824, steps per second: 133, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.019771, mae: 3.482896, mean_q: 4.191934, mean_eps: 0.100000\n",
      " 3397428/3750000: episode: 4455, duration: 3.149s, episode steps: 414, steps per second: 131, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 0.018367, mae: 3.542639, mean_q: 4.268528, mean_eps: 0.100000\n",
      " 3398215/3750000: episode: 4456, duration: 5.872s, episode steps: 787, steps per second: 134, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 0.016719, mae: 3.503200, mean_q: 4.214632, mean_eps: 0.100000\n",
      " 3398913/3750000: episode: 4457, duration: 5.263s, episode steps: 698, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.016640, mae: 3.468135, mean_q: 4.170460, mean_eps: 0.100000\n",
      " 3399712/3750000: episode: 4458, duration: 5.978s, episode steps: 799, steps per second: 134, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.019526, mae: 3.428898, mean_q: 4.125693, mean_eps: 0.100000\n",
      " 3400374/3750000: episode: 4459, duration: 4.909s, episode steps: 662, steps per second: 135, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.284 [0.000, 5.000],  loss: 0.014016, mae: 3.417425, mean_q: 4.114447, mean_eps: 0.100000\n",
      " 3401208/3750000: episode: 4460, duration: 6.511s, episode steps: 834, steps per second: 128, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.543 [0.000, 5.000],  loss: 0.021575, mae: 3.397980, mean_q: 4.095168, mean_eps: 0.100000\n",
      " 3401975/3750000: episode: 4461, duration: 5.691s, episode steps: 767, steps per second: 135, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.069 [0.000, 5.000],  loss: 0.015973, mae: 3.471244, mean_q: 4.189026, mean_eps: 0.100000\n",
      " 3402656/3750000: episode: 4462, duration: 5.057s, episode steps: 681, steps per second: 135, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.012837, mae: 3.423384, mean_q: 4.120251, mean_eps: 0.100000\n",
      " 3403196/3750000: episode: 4463, duration: 4.108s, episode steps: 540, steps per second: 131, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.889 [0.000, 5.000],  loss: 0.015286, mae: 3.465592, mean_q: 4.174023, mean_eps: 0.100000\n",
      " 3403831/3750000: episode: 4464, duration: 4.766s, episode steps: 635, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.176 [0.000, 5.000],  loss: 0.019892, mae: 3.466493, mean_q: 4.172201, mean_eps: 0.100000\n",
      " 3404534/3750000: episode: 4465, duration: 5.238s, episode steps: 703, steps per second: 134, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.986 [0.000, 5.000],  loss: 0.018038, mae: 3.421400, mean_q: 4.114078, mean_eps: 0.100000\n",
      " 3405199/3750000: episode: 4466, duration: 5.004s, episode steps: 665, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.301 [0.000, 5.000],  loss: 0.023348, mae: 3.420725, mean_q: 4.120166, mean_eps: 0.100000\n",
      " 3405673/3750000: episode: 4467, duration: 3.571s, episode steps: 474, steps per second: 133, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 4.354 [0.000, 5.000],  loss: 0.013519, mae: 3.482102, mean_q: 4.199172, mean_eps: 0.100000\n",
      " 3406285/3750000: episode: 4468, duration: 4.626s, episode steps: 612, steps per second: 132, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.752 [0.000, 5.000],  loss: 0.016026, mae: 3.415846, mean_q: 4.113275, mean_eps: 0.100000\n",
      " 3406930/3750000: episode: 4469, duration: 4.818s, episode steps: 645, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.015275, mae: 3.445318, mean_q: 4.143835, mean_eps: 0.100000\n",
      " 3407830/3750000: episode: 4470, duration: 6.941s, episode steps: 900, steps per second: 130, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.618 [0.000, 5.000],  loss: 0.018224, mae: 3.447111, mean_q: 4.149525, mean_eps: 0.100000\n",
      " 3408388/3750000: episode: 4471, duration: 4.199s, episode steps: 558, steps per second: 133, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.891 [0.000, 5.000],  loss: 0.023599, mae: 3.479108, mean_q: 4.181430, mean_eps: 0.100000\n",
      " 3409595/3750000: episode: 4472, duration: 9.066s, episode steps: 1207, steps per second: 133, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.012304, mae: 3.504467, mean_q: 4.218397, mean_eps: 0.100000\n",
      " 3410776/3750000: episode: 4473, duration: 8.738s, episode steps: 1181, steps per second: 135, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.206 [0.000, 5.000],  loss: 0.019547, mae: 3.484268, mean_q: 4.194390, mean_eps: 0.100000\n",
      " 3411736/3750000: episode: 4474, duration: 7.235s, episode steps: 960, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.401 [0.000, 5.000],  loss: 0.014941, mae: 3.490252, mean_q: 4.201119, mean_eps: 0.100000\n",
      " 3412470/3750000: episode: 4475, duration: 5.531s, episode steps: 734, steps per second: 133, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.012761, mae: 3.501347, mean_q: 4.213499, mean_eps: 0.100000\n",
      " 3413103/3750000: episode: 4476, duration: 4.802s, episode steps: 633, steps per second: 132, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.894 [0.000, 5.000],  loss: 0.017861, mae: 3.511689, mean_q: 4.220156, mean_eps: 0.100000\n",
      " 3414091/3750000: episode: 4477, duration: 7.446s, episode steps: 988, steps per second: 133, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.016285, mae: 3.494347, mean_q: 4.204401, mean_eps: 0.100000\n",
      " 3415104/3750000: episode: 4478, duration: 7.580s, episode steps: 1013, steps per second: 134, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.786 [0.000, 5.000],  loss: 0.015685, mae: 3.489324, mean_q: 4.194428, mean_eps: 0.100000\n",
      " 3416021/3750000: episode: 4479, duration: 6.910s, episode steps: 917, steps per second: 133, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.018764, mae: 3.465657, mean_q: 4.162736, mean_eps: 0.100000\n",
      " 3416525/3750000: episode: 4480, duration: 3.757s, episode steps: 504, steps per second: 134, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.488 [0.000, 5.000],  loss: 0.023605, mae: 3.500089, mean_q: 4.216216, mean_eps: 0.100000\n",
      " 3417267/3750000: episode: 4481, duration: 5.498s, episode steps: 742, steps per second: 135, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.780 [0.000, 5.000],  loss: 0.012633, mae: 3.402134, mean_q: 4.106774, mean_eps: 0.100000\n",
      " 3418379/3750000: episode: 4482, duration: 8.313s, episode steps: 1112, steps per second: 134, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.477 [0.000, 5.000],  loss: 0.013009, mae: 3.459176, mean_q: 4.169794, mean_eps: 0.100000\n",
      " 3418765/3750000: episode: 4483, duration: 2.944s, episode steps: 386, steps per second: 131, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.280 [0.000, 5.000],  loss: 0.016985, mae: 3.473165, mean_q: 4.179317, mean_eps: 0.100000\n",
      " 3420124/3750000: episode: 4484, duration: 10.291s, episode steps: 1359, steps per second: 132, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.017374, mae: 3.440978, mean_q: 4.139015, mean_eps: 0.100000\n",
      " 3420781/3750000: episode: 4485, duration: 4.930s, episode steps: 657, steps per second: 133, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.831 [0.000, 5.000],  loss: 0.015043, mae: 3.440820, mean_q: 4.137016, mean_eps: 0.100000\n",
      " 3421298/3750000: episode: 4486, duration: 3.794s, episode steps: 517, steps per second: 136, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 4.033 [0.000, 5.000],  loss: 0.014990, mae: 3.503472, mean_q: 4.221588, mean_eps: 0.100000\n",
      " 3422034/3750000: episode: 4487, duration: 5.527s, episode steps: 736, steps per second: 133, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.216 [0.000, 5.000],  loss: 0.020064, mae: 3.496180, mean_q: 4.204350, mean_eps: 0.100000\n",
      " 3422488/3750000: episode: 4488, duration: 3.470s, episode steps: 454, steps per second: 131, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.018437, mae: 3.462400, mean_q: 4.158617, mean_eps: 0.100000\n",
      " 3423028/3750000: episode: 4489, duration: 4.045s, episode steps: 540, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.013721, mae: 3.513412, mean_q: 4.228626, mean_eps: 0.100000\n",
      " 3423597/3750000: episode: 4490, duration: 4.276s, episode steps: 569, steps per second: 133, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.015575, mae: 3.480126, mean_q: 4.190777, mean_eps: 0.100000\n",
      " 3424232/3750000: episode: 4491, duration: 4.911s, episode steps: 635, steps per second: 129, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.578 [0.000, 5.000],  loss: 0.017450, mae: 3.477178, mean_q: 4.184860, mean_eps: 0.100000\n",
      " 3424860/3750000: episode: 4492, duration: 4.762s, episode steps: 628, steps per second: 132, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.015023, mae: 3.469023, mean_q: 4.180192, mean_eps: 0.100000\n",
      " 3425875/3750000: episode: 4493, duration: 7.611s, episode steps: 1015, steps per second: 133, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.016302, mae: 3.532514, mean_q: 4.259970, mean_eps: 0.100000\n",
      " 3426444/3750000: episode: 4494, duration: 4.343s, episode steps: 569, steps per second: 131, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.016598, mae: 3.467081, mean_q: 4.172748, mean_eps: 0.100000\n",
      " 3427713/3750000: episode: 4495, duration: 9.384s, episode steps: 1269, steps per second: 135, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.014333, mae: 3.493530, mean_q: 4.199709, mean_eps: 0.100000\n",
      " 3428371/3750000: episode: 4496, duration: 4.907s, episode steps: 658, steps per second: 134, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.015709, mae: 3.494367, mean_q: 4.201899, mean_eps: 0.100000\n",
      " 3429626/3750000: episode: 4497, duration: 9.385s, episode steps: 1255, steps per second: 134, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.020752, mae: 3.475130, mean_q: 4.176927, mean_eps: 0.100000\n",
      " 3430299/3750000: episode: 4498, duration: 4.984s, episode steps: 673, steps per second: 135, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.664 [0.000, 5.000],  loss: 0.020878, mae: 3.497726, mean_q: 4.208118, mean_eps: 0.100000\n",
      " 3430912/3750000: episode: 4499, duration: 4.670s, episode steps: 613, steps per second: 131, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.011421, mae: 3.423840, mean_q: 4.120278, mean_eps: 0.100000\n",
      " 3431943/3750000: episode: 4500, duration: 7.973s, episode steps: 1031, steps per second: 129, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.282 [0.000, 5.000],  loss: 0.015356, mae: 3.466111, mean_q: 4.166829, mean_eps: 0.100000\n",
      " 3432574/3750000: episode: 4501, duration: 4.868s, episode steps: 631, steps per second: 130, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.144 [0.000, 5.000],  loss: 0.013322, mae: 3.474361, mean_q: 4.177738, mean_eps: 0.100000\n",
      " 3434135/3750000: episode: 4502, duration: 11.708s, episode steps: 1561, steps per second: 133, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.017841, mae: 3.454003, mean_q: 4.155224, mean_eps: 0.100000\n",
      " 3434819/3750000: episode: 4503, duration: 5.108s, episode steps: 684, steps per second: 134, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.102 [0.000, 5.000],  loss: 0.016217, mae: 3.370617, mean_q: 4.056166, mean_eps: 0.100000\n",
      " 3435359/3750000: episode: 4504, duration: 4.097s, episode steps: 540, steps per second: 132, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.346 [0.000, 5.000],  loss: 0.014073, mae: 3.410224, mean_q: 4.105931, mean_eps: 0.100000\n",
      " 3435989/3750000: episode: 4505, duration: 4.766s, episode steps: 630, steps per second: 132, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.986 [0.000, 5.000],  loss: 0.013354, mae: 3.384203, mean_q: 4.072360, mean_eps: 0.100000\n",
      " 3436653/3750000: episode: 4506, duration: 4.942s, episode steps: 664, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.017896, mae: 3.384185, mean_q: 4.071023, mean_eps: 0.100000\n",
      " 3437242/3750000: episode: 4507, duration: 4.498s, episode steps: 589, steps per second: 131, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.655 [0.000, 5.000],  loss: 0.014028, mae: 3.406234, mean_q: 4.100681, mean_eps: 0.100000\n",
      " 3438095/3750000: episode: 4508, duration: 6.355s, episode steps: 853, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.016214, mae: 3.416751, mean_q: 4.108315, mean_eps: 0.100000\n",
      " 3438981/3750000: episode: 4509, duration: 6.617s, episode steps: 886, steps per second: 134, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.016183, mae: 3.313409, mean_q: 3.983436, mean_eps: 0.100000\n",
      " 3439954/3750000: episode: 4510, duration: 7.343s, episode steps: 973, steps per second: 133, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.011251, mae: 3.336982, mean_q: 4.015657, mean_eps: 0.100000\n",
      " 3441011/3750000: episode: 4511, duration: 7.879s, episode steps: 1057, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.387 [0.000, 5.000],  loss: 0.016164, mae: 3.353585, mean_q: 4.035101, mean_eps: 0.100000\n",
      " 3441846/3750000: episode: 4512, duration: 6.355s, episode steps: 835, steps per second: 131, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.013127, mae: 3.327986, mean_q: 4.001410, mean_eps: 0.100000\n",
      " 3443021/3750000: episode: 4513, duration: 8.719s, episode steps: 1175, steps per second: 135, episode reward: 34.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.278 [0.000, 5.000],  loss: 0.015887, mae: 3.348209, mean_q: 4.028971, mean_eps: 0.100000\n",
      " 3443949/3750000: episode: 4514, duration: 6.959s, episode steps: 928, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.419 [0.000, 5.000],  loss: 0.012485, mae: 3.312610, mean_q: 3.985387, mean_eps: 0.100000\n",
      " 3445280/3750000: episode: 4515, duration: 9.914s, episode steps: 1331, steps per second: 134, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.013109, mae: 3.300541, mean_q: 3.972280, mean_eps: 0.100000\n",
      " 3445634/3750000: episode: 4516, duration: 2.768s, episode steps: 354, steps per second: 128, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.017880, mae: 3.277215, mean_q: 3.948683, mean_eps: 0.100000\n",
      " 3446254/3750000: episode: 4517, duration: 4.603s, episode steps: 620, steps per second: 135, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.017096, mae: 3.304296, mean_q: 3.971524, mean_eps: 0.100000\n",
      " 3446792/3750000: episode: 4518, duration: 4.006s, episode steps: 538, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.013667, mae: 3.325236, mean_q: 3.998164, mean_eps: 0.100000\n",
      " 3447653/3750000: episode: 4519, duration: 6.463s, episode steps: 861, steps per second: 133, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.538 [0.000, 5.000],  loss: 0.015273, mae: 3.279828, mean_q: 3.946579, mean_eps: 0.100000\n",
      " 3448336/3750000: episode: 4520, duration: 5.126s, episode steps: 683, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.158 [0.000, 5.000],  loss: 0.014374, mae: 3.237425, mean_q: 3.891282, mean_eps: 0.100000\n",
      " 3449019/3750000: episode: 4521, duration: 5.086s, episode steps: 683, steps per second: 134, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.014216, mae: 3.210447, mean_q: 3.866304, mean_eps: 0.100000\n",
      " 3450193/3750000: episode: 4522, duration: 8.904s, episode steps: 1174, steps per second: 132, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.014768, mae: 3.258749, mean_q: 3.922197, mean_eps: 0.100000\n",
      " 3451043/3750000: episode: 4523, duration: 6.374s, episode steps: 850, steps per second: 133, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.921 [0.000, 5.000],  loss: 0.014675, mae: 3.302867, mean_q: 3.971106, mean_eps: 0.100000\n",
      " 3452584/3750000: episode: 4524, duration: 11.451s, episode steps: 1541, steps per second: 135, episode reward: 34.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.465 [0.000, 5.000],  loss: 0.014248, mae: 3.210339, mean_q: 3.862580, mean_eps: 0.100000\n",
      " 3453503/3750000: episode: 4525, duration: 6.830s, episode steps: 919, steps per second: 135, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.014377, mae: 3.197581, mean_q: 3.843166, mean_eps: 0.100000\n",
      " 3454479/3750000: episode: 4526, duration: 7.510s, episode steps: 976, steps per second: 130, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.016754, mae: 3.185552, mean_q: 3.833037, mean_eps: 0.100000\n",
      " 3455105/3750000: episode: 4527, duration: 4.727s, episode steps: 626, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.663 [0.000, 5.000],  loss: 0.019004, mae: 3.218861, mean_q: 3.878025, mean_eps: 0.100000\n",
      " 3455854/3750000: episode: 4528, duration: 5.637s, episode steps: 749, steps per second: 133, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.013708, mae: 3.165117, mean_q: 3.812899, mean_eps: 0.100000\n",
      " 3456853/3750000: episode: 4529, duration: 7.473s, episode steps: 999, steps per second: 134, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.929 [0.000, 5.000],  loss: 0.014109, mae: 3.209507, mean_q: 3.860769, mean_eps: 0.100000\n",
      " 3458186/3750000: episode: 4530, duration: 9.998s, episode steps: 1333, steps per second: 133, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.012908, mae: 3.221083, mean_q: 3.875584, mean_eps: 0.100000\n",
      " 3459001/3750000: episode: 4531, duration: 6.126s, episode steps: 815, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012813, mae: 3.226353, mean_q: 3.887978, mean_eps: 0.100000\n",
      " 3459486/3750000: episode: 4532, duration: 3.591s, episode steps: 485, steps per second: 135, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.012459, mae: 3.209850, mean_q: 3.860615, mean_eps: 0.100000\n",
      " 3460626/3750000: episode: 4533, duration: 8.543s, episode steps: 1140, steps per second: 133, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.978 [0.000, 5.000],  loss: 0.012227, mae: 3.205878, mean_q: 3.858417, mean_eps: 0.100000\n",
      " 3461315/3750000: episode: 4534, duration: 5.093s, episode steps: 689, steps per second: 135, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.570 [0.000, 5.000],  loss: 0.016835, mae: 3.263577, mean_q: 3.925709, mean_eps: 0.100000\n",
      " 3462008/3750000: episode: 4535, duration: 5.207s, episode steps: 693, steps per second: 133, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.024730, mae: 3.276865, mean_q: 3.940988, mean_eps: 0.100000\n",
      " 3462469/3750000: episode: 4536, duration: 3.445s, episode steps: 461, steps per second: 134, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 4.095 [0.000, 5.000],  loss: 0.011218, mae: 3.304705, mean_q: 3.981229, mean_eps: 0.100000\n",
      " 3463539/3750000: episode: 4537, duration: 7.980s, episode steps: 1070, steps per second: 134, episode reward: 31.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.850 [0.000, 5.000],  loss: 0.018027, mae: 3.260842, mean_q: 3.926035, mean_eps: 0.100000\n",
      " 3464228/3750000: episode: 4538, duration: 5.190s, episode steps: 689, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.015884, mae: 3.287649, mean_q: 3.956227, mean_eps: 0.100000\n",
      " 3465067/3750000: episode: 4539, duration: 6.314s, episode steps: 839, steps per second: 133, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 0.017138, mae: 3.261643, mean_q: 3.928081, mean_eps: 0.100000\n",
      " 3465994/3750000: episode: 4540, duration: 6.856s, episode steps: 927, steps per second: 135, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.014997, mae: 3.203840, mean_q: 3.855187, mean_eps: 0.100000\n",
      " 3467493/3750000: episode: 4541, duration: 11.144s, episode steps: 1499, steps per second: 135, episode reward: 34.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.017671, mae: 3.236273, mean_q: 3.895604, mean_eps: 0.100000\n",
      " 3468011/3750000: episode: 4542, duration: 3.886s, episode steps: 518, steps per second: 133, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.689 [0.000, 5.000],  loss: 0.016317, mae: 3.228144, mean_q: 3.881248, mean_eps: 0.100000\n",
      " 3469334/3750000: episode: 4543, duration: 9.785s, episode steps: 1323, steps per second: 135, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.017253, mae: 3.241533, mean_q: 3.901093, mean_eps: 0.100000\n",
      " 3470213/3750000: episode: 4544, duration: 6.540s, episode steps: 879, steps per second: 134, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.028 [0.000, 5.000],  loss: 0.014059, mae: 3.221918, mean_q: 3.880432, mean_eps: 0.100000\n",
      " 3470912/3750000: episode: 4545, duration: 5.210s, episode steps: 699, steps per second: 134, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.575 [0.000, 5.000],  loss: 0.015403, mae: 3.271238, mean_q: 3.936681, mean_eps: 0.100000\n",
      " 3471449/3750000: episode: 4546, duration: 4.163s, episode steps: 537, steps per second: 129, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.009443, mae: 3.165331, mean_q: 3.814014, mean_eps: 0.100000\n",
      " 3472158/3750000: episode: 4547, duration: 5.274s, episode steps: 709, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.017595, mae: 3.229079, mean_q: 3.889090, mean_eps: 0.100000\n",
      " 3473054/3750000: episode: 4548, duration: 6.715s, episode steps: 896, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.872 [0.000, 5.000],  loss: 0.019629, mae: 3.244042, mean_q: 3.901052, mean_eps: 0.100000\n",
      " 3473988/3750000: episode: 4549, duration: 7.042s, episode steps: 934, steps per second: 133, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.015902, mae: 3.196190, mean_q: 3.856527, mean_eps: 0.100000\n",
      " 3474679/3750000: episode: 4550, duration: 5.124s, episode steps: 691, steps per second: 135, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.233 [0.000, 5.000],  loss: 0.012743, mae: 3.322427, mean_q: 3.999385, mean_eps: 0.100000\n",
      " 3475579/3750000: episode: 4551, duration: 6.819s, episode steps: 900, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.017705, mae: 3.233109, mean_q: 3.888764, mean_eps: 0.100000\n",
      " 3476631/3750000: episode: 4552, duration: 7.872s, episode steps: 1052, steps per second: 134, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.014831, mae: 3.225117, mean_q: 3.883412, mean_eps: 0.100000\n",
      " 3477394/3750000: episode: 4553, duration: 5.699s, episode steps: 763, steps per second: 134, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.013743, mae: 3.243918, mean_q: 3.901689, mean_eps: 0.100000\n",
      " 3478686/3750000: episode: 4554, duration: 9.650s, episode steps: 1292, steps per second: 134, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.017753, mae: 3.228090, mean_q: 3.882619, mean_eps: 0.100000\n",
      " 3480231/3750000: episode: 4555, duration: 11.518s, episode steps: 1545, steps per second: 134, episode reward: 41.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.016965, mae: 3.183512, mean_q: 3.833706, mean_eps: 0.100000\n",
      " 3480758/3750000: episode: 4556, duration: 3.876s, episode steps: 527, steps per second: 136, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 0.011325, mae: 3.204927, mean_q: 3.858718, mean_eps: 0.100000\n",
      " 3481488/3750000: episode: 4557, duration: 5.475s, episode steps: 730, steps per second: 133, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.382 [0.000, 5.000],  loss: 0.019987, mae: 3.188016, mean_q: 3.838038, mean_eps: 0.100000\n",
      " 3482004/3750000: episode: 4558, duration: 3.939s, episode steps: 516, steps per second: 131, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.070 [0.000, 5.000],  loss: 0.015762, mae: 3.214972, mean_q: 3.864229, mean_eps: 0.100000\n",
      " 3482821/3750000: episode: 4559, duration: 6.079s, episode steps: 817, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.015566, mae: 3.152567, mean_q: 3.792920, mean_eps: 0.100000\n",
      " 3484064/3750000: episode: 4560, duration: 9.316s, episode steps: 1243, steps per second: 133, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.020797, mae: 3.185382, mean_q: 3.832219, mean_eps: 0.100000\n",
      " 3484996/3750000: episode: 4561, duration: 6.880s, episode steps: 932, steps per second: 135, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.802 [0.000, 5.000],  loss: 0.013933, mae: 3.175129, mean_q: 3.826518, mean_eps: 0.100000\n",
      " 3486248/3750000: episode: 4562, duration: 9.451s, episode steps: 1252, steps per second: 132, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.578 [0.000, 5.000],  loss: 0.015583, mae: 3.159121, mean_q: 3.799732, mean_eps: 0.100000\n",
      " 3486776/3750000: episode: 4563, duration: 3.921s, episode steps: 528, steps per second: 135, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 4.371 [0.000, 5.000],  loss: 0.012191, mae: 3.127161, mean_q: 3.767850, mean_eps: 0.100000\n",
      " 3487545/3750000: episode: 4564, duration: 5.865s, episode steps: 769, steps per second: 131, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.014385, mae: 3.104121, mean_q: 3.739318, mean_eps: 0.100000\n",
      " 3488178/3750000: episode: 4565, duration: 4.694s, episode steps: 633, steps per second: 135, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012838, mae: 3.166821, mean_q: 3.806952, mean_eps: 0.100000\n",
      " 3488820/3750000: episode: 4566, duration: 4.926s, episode steps: 642, steps per second: 130, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.822 [0.000, 5.000],  loss: 0.024169, mae: 3.120565, mean_q: 3.754366, mean_eps: 0.100000\n",
      " 3490176/3750000: episode: 4567, duration: 10.101s, episode steps: 1356, steps per second: 134, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.016304, mae: 3.092134, mean_q: 3.722308, mean_eps: 0.100000\n",
      " 3490803/3750000: episode: 4568, duration: 4.799s, episode steps: 627, steps per second: 131, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.017587, mae: 3.112489, mean_q: 3.747515, mean_eps: 0.100000\n",
      " 3491514/3750000: episode: 4569, duration: 5.292s, episode steps: 711, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.288 [0.000, 5.000],  loss: 0.018371, mae: 3.113140, mean_q: 3.748591, mean_eps: 0.100000\n",
      " 3492688/3750000: episode: 4570, duration: 8.810s, episode steps: 1174, steps per second: 133, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.016633, mae: 3.094539, mean_q: 3.725385, mean_eps: 0.100000\n",
      " 3493495/3750000: episode: 4571, duration: 5.967s, episode steps: 807, steps per second: 135, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.014336, mae: 3.101354, mean_q: 3.733104, mean_eps: 0.100000\n",
      " 3494474/3750000: episode: 4572, duration: 7.228s, episode steps: 979, steps per second: 135, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.397 [0.000, 5.000],  loss: 0.015389, mae: 3.095314, mean_q: 3.724575, mean_eps: 0.100000\n",
      " 3494883/3750000: episode: 4573, duration: 3.144s, episode steps: 409, steps per second: 130, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.692 [0.000, 5.000],  loss: 0.013193, mae: 3.134898, mean_q: 3.773881, mean_eps: 0.100000\n",
      " 3495990/3750000: episode: 4574, duration: 8.305s, episode steps: 1107, steps per second: 133, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.143 [0.000, 5.000],  loss: 0.015396, mae: 3.025249, mean_q: 3.645924, mean_eps: 0.100000\n",
      " 3496634/3750000: episode: 4575, duration: 4.811s, episode steps: 644, steps per second: 134, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.311 [0.000, 5.000],  loss: 0.016891, mae: 3.109447, mean_q: 3.748739, mean_eps: 0.100000\n",
      " 3497414/3750000: episode: 4576, duration: 5.883s, episode steps: 780, steps per second: 133, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.018877, mae: 3.051508, mean_q: 3.674425, mean_eps: 0.100000\n",
      " 3498506/3750000: episode: 4577, duration: 8.165s, episode steps: 1092, steps per second: 134, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.021057, mae: 3.072239, mean_q: 3.695972, mean_eps: 0.100000\n",
      " 3499403/3750000: episode: 4578, duration: 6.809s, episode steps: 897, steps per second: 132, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.388 [0.000, 5.000],  loss: 0.015968, mae: 3.099413, mean_q: 3.733503, mean_eps: 0.100000\n",
      " 3500289/3750000: episode: 4579, duration: 6.642s, episode steps: 886, steps per second: 133, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.015741, mae: 3.110073, mean_q: 3.741157, mean_eps: 0.100000\n",
      " 3501076/3750000: episode: 4580, duration: 5.856s, episode steps: 787, steps per second: 134, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.010 [0.000, 5.000],  loss: 0.021083, mae: 3.104062, mean_q: 3.732303, mean_eps: 0.100000\n",
      " 3502111/3750000: episode: 4581, duration: 7.769s, episode steps: 1035, steps per second: 133, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.303 [0.000, 5.000],  loss: 0.016445, mae: 3.087057, mean_q: 3.718909, mean_eps: 0.100000\n",
      " 3502967/3750000: episode: 4582, duration: 6.390s, episode steps: 856, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.141 [0.000, 5.000],  loss: 0.017737, mae: 3.102307, mean_q: 3.733113, mean_eps: 0.100000\n",
      " 3503884/3750000: episode: 4583, duration: 7.026s, episode steps: 917, steps per second: 131, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.250 [0.000, 5.000],  loss: 0.016396, mae: 3.122851, mean_q: 3.756960, mean_eps: 0.100000\n",
      " 3504526/3750000: episode: 4584, duration: 4.783s, episode steps: 642, steps per second: 134, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.014899, mae: 3.164151, mean_q: 3.809231, mean_eps: 0.100000\n",
      " 3505576/3750000: episode: 4585, duration: 7.884s, episode steps: 1050, steps per second: 133, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.019100, mae: 3.113535, mean_q: 3.745405, mean_eps: 0.100000\n",
      " 3506608/3750000: episode: 4586, duration: 7.725s, episode steps: 1032, steps per second: 134, episode reward: 29.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.017144, mae: 3.107975, mean_q: 3.742597, mean_eps: 0.100000\n",
      " 3507470/3750000: episode: 4587, duration: 6.456s, episode steps: 862, steps per second: 134, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.578 [0.000, 5.000],  loss: 0.016835, mae: 3.112998, mean_q: 3.750010, mean_eps: 0.100000\n",
      " 3507996/3750000: episode: 4588, duration: 3.933s, episode steps: 526, steps per second: 134, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.359 [0.000, 5.000],  loss: 0.012754, mae: 3.103000, mean_q: 3.733431, mean_eps: 0.100000\n",
      " 3508950/3750000: episode: 4589, duration: 7.129s, episode steps: 954, steps per second: 134, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.016995, mae: 3.115366, mean_q: 3.751951, mean_eps: 0.100000\n",
      " 3509651/3750000: episode: 4590, duration: 5.285s, episode steps: 701, steps per second: 133, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.018877, mae: 3.068774, mean_q: 3.690806, mean_eps: 0.100000\n",
      " 3510545/3750000: episode: 4591, duration: 6.684s, episode steps: 894, steps per second: 134, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.550 [0.000, 5.000],  loss: 0.016491, mae: 3.029298, mean_q: 3.652695, mean_eps: 0.100000\n",
      " 3512102/3750000: episode: 4592, duration: 11.725s, episode steps: 1557, steps per second: 133, episode reward: 31.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.017247, mae: 3.079572, mean_q: 3.706590, mean_eps: 0.100000\n",
      " 3513163/3750000: episode: 4593, duration: 7.929s, episode steps: 1061, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.013986, mae: 3.091858, mean_q: 3.722950, mean_eps: 0.100000\n",
      " 3514218/3750000: episode: 4594, duration: 7.843s, episode steps: 1055, steps per second: 135, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.255 [0.000, 5.000],  loss: 0.014315, mae: 3.062847, mean_q: 3.696316, mean_eps: 0.100000\n",
      " 3514966/3750000: episode: 4595, duration: 5.596s, episode steps: 748, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.654 [0.000, 5.000],  loss: 0.015340, mae: 3.110724, mean_q: 3.749713, mean_eps: 0.100000\n",
      " 3515623/3750000: episode: 4596, duration: 4.933s, episode steps: 657, steps per second: 133, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.461 [0.000, 5.000],  loss: 0.017254, mae: 3.087736, mean_q: 3.719889, mean_eps: 0.100000\n",
      " 3516583/3750000: episode: 4597, duration: 7.196s, episode steps: 960, steps per second: 133, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.015093, mae: 3.091137, mean_q: 3.717741, mean_eps: 0.100000\n",
      " 3517574/3750000: episode: 4598, duration: 7.507s, episode steps: 991, steps per second: 132, episode reward: 29.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.014536, mae: 3.121287, mean_q: 3.755108, mean_eps: 0.100000\n",
      " 3518411/3750000: episode: 4599, duration: 6.380s, episode steps: 837, steps per second: 131, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.015320, mae: 3.140060, mean_q: 3.783705, mean_eps: 0.100000\n",
      " 3519361/3750000: episode: 4600, duration: 7.170s, episode steps: 950, steps per second: 133, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.020353, mae: 3.141593, mean_q: 3.780536, mean_eps: 0.100000\n",
      " 3520616/3750000: episode: 4601, duration: 9.453s, episode steps: 1255, steps per second: 133, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.963 [0.000, 5.000],  loss: 0.014602, mae: 3.056356, mean_q: 3.676221, mean_eps: 0.100000\n",
      " 3521454/3750000: episode: 4602, duration: 6.358s, episode steps: 838, steps per second: 132, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.013088, mae: 3.111125, mean_q: 3.741435, mean_eps: 0.100000\n",
      " 3522074/3750000: episode: 4603, duration: 4.679s, episode steps: 620, steps per second: 133, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.015746, mae: 3.094476, mean_q: 3.721446, mean_eps: 0.100000\n",
      " 3523433/3750000: episode: 4604, duration: 10.251s, episode steps: 1359, steps per second: 133, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.017004, mae: 3.104314, mean_q: 3.738430, mean_eps: 0.100000\n",
      " 3524278/3750000: episode: 4605, duration: 6.446s, episode steps: 845, steps per second: 131, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.015892, mae: 3.076443, mean_q: 3.705012, mean_eps: 0.100000\n",
      " 3525035/3750000: episode: 4606, duration: 5.788s, episode steps: 757, steps per second: 131, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.601 [0.000, 5.000],  loss: 0.018290, mae: 3.142530, mean_q: 3.785082, mean_eps: 0.100000\n",
      " 3526330/3750000: episode: 4607, duration: 9.672s, episode steps: 1295, steps per second: 134, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.014789, mae: 3.097218, mean_q: 3.728375, mean_eps: 0.100000\n",
      " 3527245/3750000: episode: 4608, duration: 6.936s, episode steps: 915, steps per second: 132, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.013916, mae: 3.109600, mean_q: 3.743102, mean_eps: 0.100000\n",
      " 3528223/3750000: episode: 4609, duration: 7.354s, episode steps: 978, steps per second: 133, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.018233, mae: 3.133241, mean_q: 3.772657, mean_eps: 0.100000\n",
      " 3528750/3750000: episode: 4610, duration: 3.951s, episode steps: 527, steps per second: 133, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.213 [0.000, 5.000],  loss: 0.013136, mae: 3.064989, mean_q: 3.691971, mean_eps: 0.100000\n",
      " 3529880/3750000: episode: 4611, duration: 8.557s, episode steps: 1130, steps per second: 132, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.910 [0.000, 5.000],  loss: 0.014559, mae: 3.110506, mean_q: 3.743783, mean_eps: 0.100000\n",
      " 3530424/3750000: episode: 4612, duration: 4.154s, episode steps: 544, steps per second: 131, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.397 [0.000, 5.000],  loss: 0.017149, mae: 3.151345, mean_q: 3.794822, mean_eps: 0.100000\n",
      " 3531224/3750000: episode: 4613, duration: 6.185s, episode steps: 800, steps per second: 129, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.011681, mae: 3.164671, mean_q: 3.808079, mean_eps: 0.100000\n",
      " 3532493/3750000: episode: 4614, duration: 9.482s, episode steps: 1269, steps per second: 134, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.015305, mae: 3.127711, mean_q: 3.765494, mean_eps: 0.100000\n",
      " 3532870/3750000: episode: 4615, duration: 2.837s, episode steps: 377, steps per second: 133, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.788 [0.000, 5.000],  loss: 0.015981, mae: 3.167457, mean_q: 3.823858, mean_eps: 0.100000\n",
      " 3533744/3750000: episode: 4616, duration: 6.707s, episode steps: 874, steps per second: 130, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.018527, mae: 3.122612, mean_q: 3.759220, mean_eps: 0.100000\n",
      " 3534348/3750000: episode: 4617, duration: 4.478s, episode steps: 604, steps per second: 135, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.677 [0.000, 5.000],  loss: 0.015131, mae: 3.138999, mean_q: 3.777484, mean_eps: 0.100000\n",
      " 3535001/3750000: episode: 4618, duration: 4.912s, episode steps: 653, steps per second: 133, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.360 [0.000, 5.000],  loss: 0.009839, mae: 3.136874, mean_q: 3.777770, mean_eps: 0.100000\n",
      " 3536084/3750000: episode: 4619, duration: 8.254s, episode steps: 1083, steps per second: 131, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.012825, mae: 3.164222, mean_q: 3.806865, mean_eps: 0.100000\n",
      " 3536951/3750000: episode: 4620, duration: 6.413s, episode steps: 867, steps per second: 135, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.514 [0.000, 5.000],  loss: 0.019352, mae: 3.166061, mean_q: 3.815090, mean_eps: 0.100000\n",
      " 3537842/3750000: episode: 4621, duration: 6.761s, episode steps: 891, steps per second: 132, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.011772, mae: 3.160660, mean_q: 3.811658, mean_eps: 0.100000\n",
      " 3538819/3750000: episode: 4622, duration: 7.189s, episode steps: 977, steps per second: 136, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.013653, mae: 3.200859, mean_q: 3.854382, mean_eps: 0.100000\n",
      " 3539525/3750000: episode: 4623, duration: 5.401s, episode steps: 706, steps per second: 131, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.018554, mae: 3.189383, mean_q: 3.842705, mean_eps: 0.100000\n",
      " 3540359/3750000: episode: 4624, duration: 6.161s, episode steps: 834, steps per second: 135, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.089 [0.000, 5.000],  loss: 0.015201, mae: 3.194559, mean_q: 3.848406, mean_eps: 0.100000\n",
      " 3541568/3750000: episode: 4625, duration: 9.107s, episode steps: 1209, steps per second: 133, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.993 [0.000, 5.000],  loss: 0.017959, mae: 3.219564, mean_q: 3.878124, mean_eps: 0.100000\n",
      " 3542726/3750000: episode: 4626, duration: 8.639s, episode steps: 1158, steps per second: 134, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.018766, mae: 3.217528, mean_q: 3.870568, mean_eps: 0.100000\n",
      " 3543444/3750000: episode: 4627, duration: 5.367s, episode steps: 718, steps per second: 134, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.624 [0.000, 5.000],  loss: 0.012126, mae: 3.218711, mean_q: 3.884204, mean_eps: 0.100000\n",
      " 3544389/3750000: episode: 4628, duration: 7.112s, episode steps: 945, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.014917, mae: 3.262788, mean_q: 3.927393, mean_eps: 0.100000\n",
      " 3545286/3750000: episode: 4629, duration: 6.703s, episode steps: 897, steps per second: 134, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.017238, mae: 3.218092, mean_q: 3.869502, mean_eps: 0.100000\n",
      " 3545792/3750000: episode: 4630, duration: 3.809s, episode steps: 506, steps per second: 133, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.660 [0.000, 5.000],  loss: 0.021026, mae: 3.214681, mean_q: 3.868091, mean_eps: 0.100000\n",
      " 3546633/3750000: episode: 4631, duration: 6.298s, episode steps: 841, steps per second: 134, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.015152, mae: 3.197218, mean_q: 3.852559, mean_eps: 0.100000\n",
      " 3547939/3750000: episode: 4632, duration: 9.674s, episode steps: 1306, steps per second: 135, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015887, mae: 3.218601, mean_q: 3.871382, mean_eps: 0.100000\n",
      " 3548726/3750000: episode: 4633, duration: 5.960s, episode steps: 787, steps per second: 132, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014850, mae: 3.215114, mean_q: 3.871945, mean_eps: 0.100000\n",
      " 3549757/3750000: episode: 4634, duration: 7.648s, episode steps: 1031, steps per second: 135, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.018358, mae: 3.192752, mean_q: 3.838972, mean_eps: 0.100000\n",
      " 3550259/3750000: episode: 4635, duration: 3.780s, episode steps: 502, steps per second: 133, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.015599, mae: 3.129422, mean_q: 3.764953, mean_eps: 0.100000\n",
      " 3551196/3750000: episode: 4636, duration: 7.018s, episode steps: 937, steps per second: 134, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.019285, mae: 3.187450, mean_q: 3.835542, mean_eps: 0.100000\n",
      " 3552320/3750000: episode: 4637, duration: 8.530s, episode steps: 1124, steps per second: 132, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.022582, mae: 3.172510, mean_q: 3.818902, mean_eps: 0.100000\n",
      " 3553115/3750000: episode: 4638, duration: 5.945s, episode steps: 795, steps per second: 134, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.013098, mae: 3.175595, mean_q: 3.822980, mean_eps: 0.100000\n",
      " 3554156/3750000: episode: 4639, duration: 7.726s, episode steps: 1041, steps per second: 135, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.284 [0.000, 5.000],  loss: 0.014437, mae: 3.161631, mean_q: 3.809171, mean_eps: 0.100000\n",
      " 3554723/3750000: episode: 4640, duration: 4.331s, episode steps: 567, steps per second: 131, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.356 [0.000, 5.000],  loss: 0.011756, mae: 3.149621, mean_q: 3.789143, mean_eps: 0.100000\n",
      " 3555686/3750000: episode: 4641, duration: 7.256s, episode steps: 963, steps per second: 133, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.014134, mae: 3.190732, mean_q: 3.844455, mean_eps: 0.100000\n",
      " 3556427/3750000: episode: 4642, duration: 5.554s, episode steps: 741, steps per second: 133, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.017408, mae: 3.170632, mean_q: 3.820891, mean_eps: 0.100000\n",
      " 3557556/3750000: episode: 4643, duration: 8.467s, episode steps: 1129, steps per second: 133, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.043 [0.000, 5.000],  loss: 0.016862, mae: 3.146645, mean_q: 3.789754, mean_eps: 0.100000\n",
      " 3558661/3750000: episode: 4644, duration: 8.362s, episode steps: 1105, steps per second: 132, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.017419, mae: 3.177616, mean_q: 3.828731, mean_eps: 0.100000\n",
      " 3559463/3750000: episode: 4645, duration: 5.982s, episode steps: 802, steps per second: 134, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.998 [0.000, 5.000],  loss: 0.023749, mae: 3.169811, mean_q: 3.821137, mean_eps: 0.100000\n",
      " 3559983/3750000: episode: 4646, duration: 3.854s, episode steps: 520, steps per second: 135, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.015357, mae: 3.211253, mean_q: 3.863685, mean_eps: 0.100000\n",
      " 3560981/3750000: episode: 4647, duration: 7.480s, episode steps: 998, steps per second: 133, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.016792, mae: 3.172097, mean_q: 3.817569, mean_eps: 0.100000\n",
      " 3561867/3750000: episode: 4648, duration: 6.593s, episode steps: 886, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.604 [0.000, 5.000],  loss: 0.012617, mae: 3.219562, mean_q: 3.881707, mean_eps: 0.100000\n",
      " 3562354/3750000: episode: 4649, duration: 3.638s, episode steps: 487, steps per second: 134, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.852 [0.000, 5.000],  loss: 0.015902, mae: 3.183090, mean_q: 3.829741, mean_eps: 0.100000\n",
      " 3563284/3750000: episode: 4650, duration: 6.959s, episode steps: 930, steps per second: 134, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.951 [0.000, 5.000],  loss: 0.014331, mae: 3.230736, mean_q: 3.888098, mean_eps: 0.100000\n",
      " 3564307/3750000: episode: 4651, duration: 7.609s, episode steps: 1023, steps per second: 134, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.017492, mae: 3.193122, mean_q: 3.841014, mean_eps: 0.100000\n",
      " 3565574/3750000: episode: 4652, duration: 9.421s, episode steps: 1267, steps per second: 134, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.808 [0.000, 5.000],  loss: 0.013184, mae: 3.177320, mean_q: 3.823295, mean_eps: 0.100000\n",
      " 3566488/3750000: episode: 4653, duration: 6.822s, episode steps: 914, steps per second: 134, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.015335, mae: 3.205785, mean_q: 3.855805, mean_eps: 0.100000\n",
      " 3567512/3750000: episode: 4654, duration: 7.758s, episode steps: 1024, steps per second: 132, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.017501, mae: 3.176099, mean_q: 3.819560, mean_eps: 0.100000\n",
      " 3568297/3750000: episode: 4655, duration: 5.886s, episode steps: 785, steps per second: 133, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.014717, mae: 3.234641, mean_q: 3.892650, mean_eps: 0.100000\n",
      " 3568914/3750000: episode: 4656, duration: 4.622s, episode steps: 617, steps per second: 133, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.656 [0.000, 5.000],  loss: 0.018257, mae: 3.208291, mean_q: 3.861946, mean_eps: 0.100000\n",
      " 3570036/3750000: episode: 4657, duration: 8.384s, episode steps: 1122, steps per second: 134, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.017485, mae: 3.196355, mean_q: 3.854776, mean_eps: 0.100000\n",
      " 3570520/3750000: episode: 4658, duration: 3.689s, episode steps: 484, steps per second: 131, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.015718, mae: 3.220566, mean_q: 3.877162, mean_eps: 0.100000\n",
      " 3571080/3750000: episode: 4659, duration: 4.275s, episode steps: 560, steps per second: 131, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.462 [0.000, 5.000],  loss: 0.023129, mae: 3.238083, mean_q: 3.891420, mean_eps: 0.100000\n",
      " 3572006/3750000: episode: 4660, duration: 7.021s, episode steps: 926, steps per second: 132, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.630 [0.000, 5.000],  loss: 0.012123, mae: 3.237066, mean_q: 3.900324, mean_eps: 0.100000\n",
      " 3573591/3750000: episode: 4661, duration: 11.759s, episode steps: 1585, steps per second: 135, episode reward: 35.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.015065, mae: 3.174606, mean_q: 3.819432, mean_eps: 0.100000\n",
      " 3574490/3750000: episode: 4662, duration: 6.697s, episode steps: 899, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.015680, mae: 3.221779, mean_q: 3.887291, mean_eps: 0.100000\n",
      " 3575419/3750000: episode: 4663, duration: 6.868s, episode steps: 929, steps per second: 135, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.016652, mae: 3.196356, mean_q: 3.844523, mean_eps: 0.100000\n",
      " 3576167/3750000: episode: 4664, duration: 5.668s, episode steps: 748, steps per second: 132, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.845 [0.000, 5.000],  loss: 0.017795, mae: 3.243013, mean_q: 3.907153, mean_eps: 0.100000\n",
      " 3576877/3750000: episode: 4665, duration: 5.282s, episode steps: 710, steps per second: 134, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.958 [0.000, 5.000],  loss: 0.015159, mae: 3.259725, mean_q: 3.929893, mean_eps: 0.100000\n",
      " 3577508/3750000: episode: 4666, duration: 4.759s, episode steps: 631, steps per second: 133, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.016046, mae: 3.216524, mean_q: 3.873731, mean_eps: 0.100000\n",
      " 3578111/3750000: episode: 4667, duration: 4.549s, episode steps: 603, steps per second: 133, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.018323, mae: 3.235045, mean_q: 3.891509, mean_eps: 0.100000\n",
      " 3578996/3750000: episode: 4668, duration: 6.598s, episode steps: 885, steps per second: 134, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.015323, mae: 3.270808, mean_q: 3.938082, mean_eps: 0.100000\n",
      " 3580227/3750000: episode: 4669, duration: 9.237s, episode steps: 1231, steps per second: 133, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.017031, mae: 3.235701, mean_q: 3.896922, mean_eps: 0.100000\n",
      " 3581142/3750000: episode: 4670, duration: 6.822s, episode steps: 915, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.013697, mae: 3.246231, mean_q: 3.912003, mean_eps: 0.100000\n",
      " 3581933/3750000: episode: 4671, duration: 5.815s, episode steps: 791, steps per second: 136, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: 0.018895, mae: 3.269846, mean_q: 3.938946, mean_eps: 0.100000\n",
      " 3582517/3750000: episode: 4672, duration: 4.387s, episode steps: 584, steps per second: 133, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.011542, mae: 3.275251, mean_q: 3.941979, mean_eps: 0.100000\n",
      " 3583393/3750000: episode: 4673, duration: 6.616s, episode steps: 876, steps per second: 132, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.016868, mae: 3.218990, mean_q: 3.873614, mean_eps: 0.100000\n",
      " 3584461/3750000: episode: 4674, duration: 8.085s, episode steps: 1068, steps per second: 132, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.015305, mae: 3.226276, mean_q: 3.882253, mean_eps: 0.100000\n",
      " 3585231/3750000: episode: 4675, duration: 5.735s, episode steps: 770, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.013073, mae: 3.261921, mean_q: 3.929426, mean_eps: 0.100000\n",
      " 3586477/3750000: episode: 4676, duration: 9.295s, episode steps: 1246, steps per second: 134, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.015573, mae: 3.257800, mean_q: 3.920344, mean_eps: 0.100000\n",
      " 3587152/3750000: episode: 4677, duration: 5.080s, episode steps: 675, steps per second: 133, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.018867, mae: 3.307645, mean_q: 3.980762, mean_eps: 0.100000\n",
      " 3588130/3750000: episode: 4678, duration: 7.398s, episode steps: 978, steps per second: 132, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.014456, mae: 3.275489, mean_q: 3.939683, mean_eps: 0.100000\n",
      " 3589205/3750000: episode: 4679, duration: 8.155s, episode steps: 1075, steps per second: 132, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.023 [0.000, 5.000],  loss: 0.020471, mae: 3.303707, mean_q: 3.972753, mean_eps: 0.100000\n",
      " 3590074/3750000: episode: 4680, duration: 6.436s, episode steps: 869, steps per second: 135, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.067 [0.000, 5.000],  loss: 0.017171, mae: 3.295626, mean_q: 3.967204, mean_eps: 0.100000\n",
      " 3591012/3750000: episode: 4681, duration: 7.060s, episode steps: 938, steps per second: 133, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.014970, mae: 3.324796, mean_q: 4.005808, mean_eps: 0.100000\n",
      " 3591806/3750000: episode: 4682, duration: 5.989s, episode steps: 794, steps per second: 133, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.014373, mae: 3.315209, mean_q: 3.989880, mean_eps: 0.100000\n",
      " 3592313/3750000: episode: 4683, duration: 3.764s, episode steps: 507, steps per second: 135, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.148 [0.000, 5.000],  loss: 0.014350, mae: 3.306708, mean_q: 3.989178, mean_eps: 0.100000\n",
      " 3593030/3750000: episode: 4684, duration: 5.394s, episode steps: 717, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 0.017536, mae: 3.307533, mean_q: 3.985915, mean_eps: 0.100000\n",
      " 3593871/3750000: episode: 4685, duration: 6.254s, episode steps: 841, steps per second: 134, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 1.737 [0.000, 5.000],  loss: 0.018016, mae: 3.348905, mean_q: 4.034377, mean_eps: 0.100000\n",
      " 3594382/3750000: episode: 4686, duration: 3.858s, episode steps: 511, steps per second: 132, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.014615, mae: 3.346929, mean_q: 4.032393, mean_eps: 0.100000\n",
      " 3594859/3750000: episode: 4687, duration: 3.483s, episode steps: 477, steps per second: 137, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.616 [0.000, 5.000],  loss: 0.023205, mae: 3.351107, mean_q: 4.032074, mean_eps: 0.100000\n",
      " 3595801/3750000: episode: 4688, duration: 7.203s, episode steps: 942, steps per second: 131, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.645 [0.000, 5.000],  loss: 0.023338, mae: 3.376671, mean_q: 4.063440, mean_eps: 0.100000\n",
      " 3597236/3750000: episode: 4689, duration: 10.683s, episode steps: 1435, steps per second: 134, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: 0.016938, mae: 3.416658, mean_q: 4.111457, mean_eps: 0.100000\n",
      " 3597729/3750000: episode: 4690, duration: 3.717s, episode steps: 493, steps per second: 133, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 0.511 [0.000, 5.000],  loss: 0.016078, mae: 3.388136, mean_q: 4.087768, mean_eps: 0.100000\n",
      " 3598421/3750000: episode: 4691, duration: 5.223s, episode steps: 692, steps per second: 132, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.021709, mae: 3.426771, mean_q: 4.124162, mean_eps: 0.100000\n",
      " 3599472/3750000: episode: 4692, duration: 7.849s, episode steps: 1051, steps per second: 134, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.165 [0.000, 5.000],  loss: 0.016008, mae: 3.407868, mean_q: 4.095171, mean_eps: 0.100000\n",
      " 3600157/3750000: episode: 4693, duration: 5.101s, episode steps: 685, steps per second: 134, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.304 [0.000, 5.000],  loss: 0.013240, mae: 3.337908, mean_q: 4.020320, mean_eps: 0.100000\n",
      " 3600905/3750000: episode: 4694, duration: 5.636s, episode steps: 748, steps per second: 133, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.016940, mae: 3.383240, mean_q: 4.070001, mean_eps: 0.100000\n",
      " 3601836/3750000: episode: 4695, duration: 6.973s, episode steps: 931, steps per second: 134, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.013035, mae: 3.375542, mean_q: 4.060031, mean_eps: 0.100000\n",
      " 3602673/3750000: episode: 4696, duration: 6.240s, episode steps: 837, steps per second: 134, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.013138, mae: 3.348990, mean_q: 4.030473, mean_eps: 0.100000\n",
      " 3603779/3750000: episode: 4697, duration: 8.373s, episode steps: 1106, steps per second: 132, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015101, mae: 3.366922, mean_q: 4.050880, mean_eps: 0.100000\n",
      " 3605213/3750000: episode: 4698, duration: 10.703s, episode steps: 1434, steps per second: 134, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.018333, mae: 3.396050, mean_q: 4.091750, mean_eps: 0.100000\n",
      " 3605920/3750000: episode: 4699, duration: 5.397s, episode steps: 707, steps per second: 131, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.876 [0.000, 5.000],  loss: 0.014610, mae: 3.365834, mean_q: 4.054348, mean_eps: 0.100000\n",
      " 3606641/3750000: episode: 4700, duration: 5.450s, episode steps: 721, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.014422, mae: 3.406573, mean_q: 4.101822, mean_eps: 0.100000\n",
      " 3607122/3750000: episode: 4701, duration: 3.657s, episode steps: 481, steps per second: 132, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.019095, mae: 3.366454, mean_q: 4.051239, mean_eps: 0.100000\n",
      " 3607672/3750000: episode: 4702, duration: 4.130s, episode steps: 550, steps per second: 133, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 0.809 [0.000, 5.000],  loss: 0.018995, mae: 3.415872, mean_q: 4.112027, mean_eps: 0.100000\n",
      " 3608540/3750000: episode: 4703, duration: 6.598s, episode steps: 868, steps per second: 132, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.615 [0.000, 5.000],  loss: 0.018047, mae: 3.379528, mean_q: 4.060465, mean_eps: 0.100000\n",
      " 3609346/3750000: episode: 4704, duration: 6.119s, episode steps: 806, steps per second: 132, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.488 [0.000, 5.000],  loss: 0.017498, mae: 3.399141, mean_q: 4.086694, mean_eps: 0.100000\n",
      " 3609998/3750000: episode: 4705, duration: 4.906s, episode steps: 652, steps per second: 133, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.016578, mae: 3.369192, mean_q: 4.054044, mean_eps: 0.100000\n",
      " 3610867/3750000: episode: 4706, duration: 6.535s, episode steps: 869, steps per second: 133, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.589 [0.000, 5.000],  loss: 0.011713, mae: 3.378848, mean_q: 4.072356, mean_eps: 0.100000\n",
      " 3611616/3750000: episode: 4707, duration: 5.591s, episode steps: 749, steps per second: 134, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.013748, mae: 3.379672, mean_q: 4.061317, mean_eps: 0.100000\n",
      " 3613104/3750000: episode: 4708, duration: 11.153s, episode steps: 1488, steps per second: 133, episode reward: 36.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.192 [0.000, 5.000],  loss: 0.015215, mae: 3.341853, mean_q: 4.021484, mean_eps: 0.100000\n",
      " 3614206/3750000: episode: 4709, duration: 8.410s, episode steps: 1102, steps per second: 131, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.016348, mae: 3.372293, mean_q: 4.052596, mean_eps: 0.100000\n",
      " 3614874/3750000: episode: 4710, duration: 5.027s, episode steps: 668, steps per second: 133, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.310 [0.000, 5.000],  loss: 0.015946, mae: 3.326041, mean_q: 4.000304, mean_eps: 0.100000\n",
      " 3615727/3750000: episode: 4711, duration: 6.465s, episode steps: 853, steps per second: 132, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.015885, mae: 3.343090, mean_q: 4.022245, mean_eps: 0.100000\n",
      " 3616886/3750000: episode: 4712, duration: 8.668s, episode steps: 1159, steps per second: 134, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.017341, mae: 3.399627, mean_q: 4.087274, mean_eps: 0.100000\n",
      " 3617787/3750000: episode: 4713, duration: 6.728s, episode steps: 901, steps per second: 134, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.020908, mae: 3.361959, mean_q: 4.042400, mean_eps: 0.100000\n",
      " 3618734/3750000: episode: 4714, duration: 7.073s, episode steps: 947, steps per second: 134, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.017746, mae: 3.404668, mean_q: 4.095947, mean_eps: 0.100000\n",
      " 3619567/3750000: episode: 4715, duration: 6.240s, episode steps: 833, steps per second: 133, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.391 [0.000, 5.000],  loss: 0.022063, mae: 3.370105, mean_q: 4.054859, mean_eps: 0.100000\n",
      " 3620080/3750000: episode: 4716, duration: 3.889s, episode steps: 513, steps per second: 132, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.020648, mae: 3.351398, mean_q: 4.025554, mean_eps: 0.100000\n",
      " 3620670/3750000: episode: 4717, duration: 4.528s, episode steps: 590, steps per second: 130, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.017447, mae: 3.374923, mean_q: 4.063546, mean_eps: 0.100000\n",
      " 3621640/3750000: episode: 4718, duration: 7.295s, episode steps: 970, steps per second: 133, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.375 [0.000, 5.000],  loss: 0.020573, mae: 3.345764, mean_q: 4.025726, mean_eps: 0.100000\n",
      " 3622587/3750000: episode: 4719, duration: 7.183s, episode steps: 947, steps per second: 132, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.968 [0.000, 5.000],  loss: 0.014952, mae: 3.321659, mean_q: 3.998768, mean_eps: 0.100000\n",
      " 3623708/3750000: episode: 4720, duration: 8.350s, episode steps: 1121, steps per second: 134, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.601 [0.000, 5.000],  loss: 0.016185, mae: 3.310601, mean_q: 3.982111, mean_eps: 0.100000\n",
      " 3624546/3750000: episode: 4721, duration: 6.263s, episode steps: 838, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.010987, mae: 3.321364, mean_q: 3.998139, mean_eps: 0.100000\n",
      " 3625086/3750000: episode: 4722, duration: 4.100s, episode steps: 540, steps per second: 132, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.011706, mae: 3.354927, mean_q: 4.034428, mean_eps: 0.100000\n",
      " 3625897/3750000: episode: 4723, duration: 5.976s, episode steps: 811, steps per second: 136, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.602 [0.000, 5.000],  loss: 0.020490, mae: 3.342661, mean_q: 4.024225, mean_eps: 0.100000\n",
      " 3627182/3750000: episode: 4724, duration: 9.670s, episode steps: 1285, steps per second: 133, episode reward: 36.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.014952, mae: 3.290032, mean_q: 3.964871, mean_eps: 0.100000\n",
      " 3627918/3750000: episode: 4725, duration: 5.439s, episode steps: 736, steps per second: 135, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.018707, mae: 3.288372, mean_q: 3.955792, mean_eps: 0.100000\n",
      " 3628668/3750000: episode: 4726, duration: 5.704s, episode steps: 750, steps per second: 131, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.693 [0.000, 5.000],  loss: 0.011247, mae: 3.342585, mean_q: 4.022593, mean_eps: 0.100000\n",
      " 3630383/3750000: episode: 4727, duration: 12.821s, episode steps: 1715, steps per second: 134, episode reward: 35.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.012280, mae: 3.300486, mean_q: 3.972568, mean_eps: 0.100000\n",
      " 3631608/3750000: episode: 4728, duration: 9.244s, episode steps: 1225, steps per second: 133, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.011946, mae: 3.337607, mean_q: 4.015860, mean_eps: 0.100000\n",
      " 3632539/3750000: episode: 4729, duration: 6.918s, episode steps: 931, steps per second: 135, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.016340, mae: 3.356370, mean_q: 4.038878, mean_eps: 0.100000\n",
      " 3633066/3750000: episode: 4730, duration: 4.041s, episode steps: 527, steps per second: 130, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.970 [0.000, 5.000],  loss: 0.018592, mae: 3.340939, mean_q: 4.022047, mean_eps: 0.100000\n",
      " 3634024/3750000: episode: 4731, duration: 7.264s, episode steps: 958, steps per second: 132, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.920 [0.000, 5.000],  loss: 0.013100, mae: 3.333178, mean_q: 4.012173, mean_eps: 0.100000\n",
      " 3634623/3750000: episode: 4732, duration: 4.540s, episode steps: 599, steps per second: 132, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.019140, mae: 3.314541, mean_q: 3.986297, mean_eps: 0.100000\n",
      " 3635725/3750000: episode: 4733, duration: 8.328s, episode steps: 1102, steps per second: 132, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.127 [0.000, 5.000],  loss: 0.015056, mae: 3.356575, mean_q: 4.041596, mean_eps: 0.100000\n",
      " 3636658/3750000: episode: 4734, duration: 6.922s, episode steps: 933, steps per second: 135, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.014453, mae: 3.329813, mean_q: 4.014516, mean_eps: 0.100000\n",
      " 3637423/3750000: episode: 4735, duration: 5.802s, episode steps: 765, steps per second: 132, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.352 [0.000, 5.000],  loss: 0.013506, mae: 3.338787, mean_q: 4.020831, mean_eps: 0.100000\n",
      " 3638038/3750000: episode: 4736, duration: 4.623s, episode steps: 615, steps per second: 133, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.239 [0.000, 5.000],  loss: 0.013980, mae: 3.333336, mean_q: 4.012979, mean_eps: 0.100000\n",
      " 3639047/3750000: episode: 4737, duration: 7.561s, episode steps: 1009, steps per second: 133, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.014553, mae: 3.361442, mean_q: 4.055986, mean_eps: 0.100000\n",
      " 3640469/3750000: episode: 4738, duration: 10.577s, episode steps: 1422, steps per second: 134, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.016816, mae: 3.423392, mean_q: 4.130113, mean_eps: 0.100000\n",
      " 3641356/3750000: episode: 4739, duration: 6.603s, episode steps: 887, steps per second: 134, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.796 [0.000, 5.000],  loss: 0.016651, mae: 3.406084, mean_q: 4.101956, mean_eps: 0.100000\n",
      " 3642124/3750000: episode: 4740, duration: 5.852s, episode steps: 768, steps per second: 131, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.915 [0.000, 5.000],  loss: 0.018506, mae: 3.430848, mean_q: 4.127893, mean_eps: 0.100000\n",
      " 3642762/3750000: episode: 4741, duration: 4.759s, episode steps: 638, steps per second: 134, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.016139, mae: 3.345317, mean_q: 4.023251, mean_eps: 0.100000\n",
      " 3643458/3750000: episode: 4742, duration: 5.118s, episode steps: 696, steps per second: 136, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.960 [0.000, 5.000],  loss: 0.013707, mae: 3.380445, mean_q: 4.064347, mean_eps: 0.100000\n",
      " 3644603/3750000: episode: 4743, duration: 8.618s, episode steps: 1145, steps per second: 133, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.815 [0.000, 5.000],  loss: 0.012410, mae: 3.367344, mean_q: 4.051860, mean_eps: 0.100000\n",
      " 3645271/3750000: episode: 4744, duration: 4.951s, episode steps: 668, steps per second: 135, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.017978, mae: 3.387691, mean_q: 4.073231, mean_eps: 0.100000\n",
      " 3646128/3750000: episode: 4745, duration: 6.466s, episode steps: 857, steps per second: 133, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.828 [0.000, 5.000],  loss: 0.016576, mae: 3.397487, mean_q: 4.090033, mean_eps: 0.100000\n",
      " 3646776/3750000: episode: 4746, duration: 4.812s, episode steps: 648, steps per second: 135, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.185 [0.000, 5.000],  loss: 0.016542, mae: 3.353785, mean_q: 4.033255, mean_eps: 0.100000\n",
      " 3647761/3750000: episode: 4747, duration: 7.599s, episode steps: 985, steps per second: 130, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.016068, mae: 3.292995, mean_q: 3.958207, mean_eps: 0.100000\n",
      " 3648398/3750000: episode: 4748, duration: 4.799s, episode steps: 637, steps per second: 133, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.014640, mae: 3.365625, mean_q: 4.051609, mean_eps: 0.100000\n",
      " 3648990/3750000: episode: 4749, duration: 4.492s, episode steps: 592, steps per second: 132, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.152 [0.000, 5.000],  loss: 0.016840, mae: 3.304741, mean_q: 3.973517, mean_eps: 0.100000\n",
      " 3650017/3750000: episode: 4750, duration: 7.707s, episode steps: 1027, steps per second: 133, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.019139, mae: 3.304046, mean_q: 3.976327, mean_eps: 0.100000\n",
      " 3650917/3750000: episode: 4751, duration: 6.766s, episode steps: 900, steps per second: 133, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.014493, mae: 3.339213, mean_q: 4.024368, mean_eps: 0.100000\n",
      " 3652103/3750000: episode: 4752, duration: 9.037s, episode steps: 1186, steps per second: 131, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014579, mae: 3.294812, mean_q: 3.962310, mean_eps: 0.100000\n",
      " 3652585/3750000: episode: 4753, duration: 4.075s, episode steps: 482, steps per second: 118, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.017375, mae: 3.351039, mean_q: 4.026332, mean_eps: 0.100000\n",
      " 3653918/3750000: episode: 4754, duration: 9.915s, episode steps: 1333, steps per second: 134, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.012961, mae: 3.327138, mean_q: 4.003766, mean_eps: 0.100000\n",
      " 3655142/3750000: episode: 4755, duration: 9.283s, episode steps: 1224, steps per second: 132, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.015488, mae: 3.353847, mean_q: 4.034133, mean_eps: 0.100000\n",
      " 3655849/3750000: episode: 4756, duration: 5.251s, episode steps: 707, steps per second: 135, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.294 [0.000, 5.000],  loss: 0.009798, mae: 3.362869, mean_q: 4.049623, mean_eps: 0.100000\n",
      " 3657442/3750000: episode: 4757, duration: 11.920s, episode steps: 1593, steps per second: 134, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.078 [0.000, 5.000],  loss: 0.016851, mae: 3.354280, mean_q: 4.037865, mean_eps: 0.100000\n",
      " 3658367/3750000: episode: 4758, duration: 6.893s, episode steps: 925, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.014848, mae: 3.312769, mean_q: 3.985179, mean_eps: 0.100000\n",
      " 3659048/3750000: episode: 4759, duration: 5.155s, episode steps: 681, steps per second: 132, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.014082, mae: 3.338505, mean_q: 4.021034, mean_eps: 0.100000\n",
      " 3659956/3750000: episode: 4760, duration: 6.800s, episode steps: 908, steps per second: 134, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.268 [0.000, 5.000],  loss: 0.015653, mae: 3.320456, mean_q: 4.000490, mean_eps: 0.100000\n",
      " 3661345/3750000: episode: 4761, duration: 10.421s, episode steps: 1389, steps per second: 133, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.012736, mae: 3.286821, mean_q: 3.953274, mean_eps: 0.100000\n",
      " 3661865/3750000: episode: 4762, duration: 3.878s, episode steps: 520, steps per second: 134, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.367 [0.000, 5.000],  loss: 0.011357, mae: 3.323391, mean_q: 4.000481, mean_eps: 0.100000\n",
      " 3662560/3750000: episode: 4763, duration: 5.186s, episode steps: 695, steps per second: 134, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.015590, mae: 3.318459, mean_q: 3.994562, mean_eps: 0.100000\n",
      " 3663085/3750000: episode: 4764, duration: 4.099s, episode steps: 525, steps per second: 128, episode reward: 13.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.012591, mae: 3.244648, mean_q: 3.910717, mean_eps: 0.100000\n",
      " 3664468/3750000: episode: 4765, duration: 10.397s, episode steps: 1383, steps per second: 133, episode reward: 35.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.016256, mae: 3.324169, mean_q: 4.001266, mean_eps: 0.100000\n",
      " 3665829/3750000: episode: 4766, duration: 10.252s, episode steps: 1361, steps per second: 133, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.003 [0.000, 5.000],  loss: 0.013883, mae: 3.311137, mean_q: 3.986289, mean_eps: 0.100000\n",
      " 3666755/3750000: episode: 4767, duration: 6.852s, episode steps: 926, steps per second: 135, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.014165, mae: 3.259290, mean_q: 3.919818, mean_eps: 0.100000\n",
      " 3667563/3750000: episode: 4768, duration: 6.184s, episode steps: 808, steps per second: 131, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.013989, mae: 3.319250, mean_q: 3.996501, mean_eps: 0.100000\n",
      " 3668277/3750000: episode: 4769, duration: 5.263s, episode steps: 714, steps per second: 136, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.961 [0.000, 5.000],  loss: 0.017057, mae: 3.286983, mean_q: 3.953733, mean_eps: 0.100000\n",
      " 3669257/3750000: episode: 4770, duration: 7.273s, episode steps: 980, steps per second: 135, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.555 [0.000, 5.000],  loss: 0.015092, mae: 3.326907, mean_q: 4.007861, mean_eps: 0.100000\n",
      " 3670081/3750000: episode: 4771, duration: 6.249s, episode steps: 824, steps per second: 132, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.004 [0.000, 5.000],  loss: 0.019107, mae: 3.328449, mean_q: 4.006528, mean_eps: 0.100000\n",
      " 3671092/3750000: episode: 4772, duration: 7.491s, episode steps: 1011, steps per second: 135, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.013025, mae: 3.351907, mean_q: 4.032389, mean_eps: 0.100000\n",
      " 3671580/3750000: episode: 4773, duration: 3.733s, episode steps: 488, steps per second: 131, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.107 [0.000, 5.000],  loss: 0.011346, mae: 3.291247, mean_q: 3.969689, mean_eps: 0.100000\n",
      " 3672283/3750000: episode: 4774, duration: 5.313s, episode steps: 703, steps per second: 132, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.017838, mae: 3.360769, mean_q: 4.046812, mean_eps: 0.100000\n",
      " 3673033/3750000: episode: 4775, duration: 5.526s, episode steps: 750, steps per second: 136, episode reward: 22.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017425, mae: 3.359032, mean_q: 4.040511, mean_eps: 0.100000\n",
      " 3673958/3750000: episode: 4776, duration: 6.921s, episode steps: 925, steps per second: 134, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 0.014896, mae: 3.247467, mean_q: 3.908504, mean_eps: 0.100000\n",
      " 3674441/3750000: episode: 4777, duration: 3.683s, episode steps: 483, steps per second: 131, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.807 [0.000, 5.000],  loss: 0.012271, mae: 3.326898, mean_q: 4.001605, mean_eps: 0.100000\n",
      " 3675370/3750000: episode: 4778, duration: 6.859s, episode steps: 929, steps per second: 135, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.016177, mae: 3.298301, mean_q: 3.965255, mean_eps: 0.100000\n",
      " 3676213/3750000: episode: 4779, duration: 6.312s, episode steps: 843, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.046 [0.000, 5.000],  loss: 0.014561, mae: 3.327484, mean_q: 4.004662, mean_eps: 0.100000\n",
      " 3677142/3750000: episode: 4780, duration: 6.944s, episode steps: 929, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.477 [0.000, 5.000],  loss: 0.012728, mae: 3.300152, mean_q: 3.971583, mean_eps: 0.100000\n",
      " 3677737/3750000: episode: 4781, duration: 4.420s, episode steps: 595, steps per second: 135, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.017128, mae: 3.350674, mean_q: 4.028003, mean_eps: 0.100000\n",
      " 3678943/3750000: episode: 4782, duration: 9.083s, episode steps: 1206, steps per second: 133, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.015126, mae: 3.331720, mean_q: 4.009408, mean_eps: 0.100000\n",
      " 3679596/3750000: episode: 4783, duration: 4.852s, episode steps: 653, steps per second: 135, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.795 [0.000, 5.000],  loss: 0.010989, mae: 3.242483, mean_q: 3.906915, mean_eps: 0.100000\n",
      " 3680161/3750000: episode: 4784, duration: 4.353s, episode steps: 565, steps per second: 130, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.112 [0.000, 5.000],  loss: 0.012628, mae: 3.340690, mean_q: 4.016379, mean_eps: 0.100000\n",
      " 3681327/3750000: episode: 4785, duration: 8.639s, episode steps: 1166, steps per second: 135, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.017197, mae: 3.299396, mean_q: 3.971674, mean_eps: 0.100000\n",
      " 3682797/3750000: episode: 4786, duration: 10.966s, episode steps: 1470, steps per second: 134, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.859 [0.000, 5.000],  loss: 0.016382, mae: 3.372443, mean_q: 4.061847, mean_eps: 0.100000\n",
      " 3683393/3750000: episode: 4787, duration: 4.476s, episode steps: 596, steps per second: 133, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.089 [0.000, 5.000],  loss: 0.016226, mae: 3.288734, mean_q: 3.958962, mean_eps: 0.100000\n",
      " 3684106/3750000: episode: 4788, duration: 5.351s, episode steps: 713, steps per second: 133, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.084 [0.000, 5.000],  loss: 0.018749, mae: 3.312476, mean_q: 3.984875, mean_eps: 0.100000\n",
      " 3685049/3750000: episode: 4789, duration: 7.048s, episode steps: 943, steps per second: 134, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.012432, mae: 3.306795, mean_q: 3.978653, mean_eps: 0.100000\n",
      " 3686015/3750000: episode: 4790, duration: 7.167s, episode steps: 966, steps per second: 135, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.014195, mae: 3.277983, mean_q: 3.942277, mean_eps: 0.100000\n",
      " 3686436/3750000: episode: 4791, duration: 3.154s, episode steps: 421, steps per second: 133, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.981 [0.000, 5.000],  loss: 0.017810, mae: 3.326771, mean_q: 4.014540, mean_eps: 0.100000\n",
      " 3686969/3750000: episode: 4792, duration: 4.062s, episode steps: 533, steps per second: 131, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.308 [0.000, 5.000],  loss: 0.013152, mae: 3.267940, mean_q: 3.932297, mean_eps: 0.100000\n",
      " 3687826/3750000: episode: 4793, duration: 6.433s, episode steps: 857, steps per second: 133, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.017850, mae: 3.267356, mean_q: 3.932727, mean_eps: 0.100000\n",
      " 3688749/3750000: episode: 4794, duration: 6.970s, episode steps: 923, steps per second: 132, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.015525, mae: 3.331399, mean_q: 4.011712, mean_eps: 0.100000\n",
      " 3689860/3750000: episode: 4795, duration: 8.321s, episode steps: 1111, steps per second: 134, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.733 [0.000, 5.000],  loss: 0.018418, mae: 3.282908, mean_q: 3.954941, mean_eps: 0.100000\n",
      " 3690694/3750000: episode: 4796, duration: 6.204s, episode steps: 834, steps per second: 134, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.014818, mae: 3.331760, mean_q: 4.013714, mean_eps: 0.100000\n",
      " 3691423/3750000: episode: 4797, duration: 5.541s, episode steps: 729, steps per second: 132, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.966 [0.000, 5.000],  loss: 0.012245, mae: 3.293611, mean_q: 3.964752, mean_eps: 0.100000\n",
      " 3692628/3750000: episode: 4798, duration: 8.881s, episode steps: 1205, steps per second: 136, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.124 [0.000, 5.000],  loss: 0.013311, mae: 3.292852, mean_q: 3.962583, mean_eps: 0.100000\n",
      " 3693120/3750000: episode: 4799, duration: 3.771s, episode steps: 492, steps per second: 130, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.020493, mae: 3.241663, mean_q: 3.901166, mean_eps: 0.100000\n",
      " 3693742/3750000: episode: 4800, duration: 4.711s, episode steps: 622, steps per second: 132, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.243 [0.000, 5.000],  loss: 0.011653, mae: 3.243306, mean_q: 3.912987, mean_eps: 0.100000\n",
      " 3694531/3750000: episode: 4801, duration: 5.901s, episode steps: 789, steps per second: 134, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.017790, mae: 3.307672, mean_q: 3.979507, mean_eps: 0.100000\n",
      " 3695648/3750000: episode: 4802, duration: 8.465s, episode steps: 1117, steps per second: 132, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.013848, mae: 3.283685, mean_q: 3.953637, mean_eps: 0.100000\n",
      " 3696832/3750000: episode: 4803, duration: 8.774s, episode steps: 1184, steps per second: 135, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.014014, mae: 3.316241, mean_q: 3.990650, mean_eps: 0.100000\n",
      " 3698164/3750000: episode: 4804, duration: 10.024s, episode steps: 1332, steps per second: 133, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.016771, mae: 3.256231, mean_q: 3.916088, mean_eps: 0.100000\n",
      " 3698680/3750000: episode: 4805, duration: 3.827s, episode steps: 516, steps per second: 135, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.855 [0.000, 5.000],  loss: 0.014291, mae: 3.277027, mean_q: 3.944352, mean_eps: 0.100000\n",
      " 3699791/3750000: episode: 4806, duration: 8.336s, episode steps: 1111, steps per second: 133, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.019060, mae: 3.242150, mean_q: 3.900515, mean_eps: 0.100000\n",
      " 3701076/3750000: episode: 4807, duration: 9.498s, episode steps: 1285, steps per second: 135, episode reward: 33.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.888 [0.000, 5.000],  loss: 0.014600, mae: 3.207121, mean_q: 3.857781, mean_eps: 0.100000\n",
      " 3702374/3750000: episode: 4808, duration: 9.675s, episode steps: 1298, steps per second: 134, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.092 [0.000, 5.000],  loss: 0.013847, mae: 3.200416, mean_q: 3.855564, mean_eps: 0.100000\n",
      " 3703238/3750000: episode: 4809, duration: 6.391s, episode steps: 864, steps per second: 135, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.483 [0.000, 5.000],  loss: 0.012298, mae: 3.234103, mean_q: 3.895746, mean_eps: 0.100000\n",
      " 3703909/3750000: episode: 4810, duration: 5.100s, episode steps: 671, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.018567, mae: 3.291816, mean_q: 3.955915, mean_eps: 0.100000\n",
      " 3705305/3750000: episode: 4811, duration: 10.374s, episode steps: 1396, steps per second: 135, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.123 [0.000, 5.000],  loss: 0.015206, mae: 3.257282, mean_q: 3.926878, mean_eps: 0.100000\n",
      " 3705824/3750000: episode: 4812, duration: 3.905s, episode steps: 519, steps per second: 133, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.218 [0.000, 5.000],  loss: 0.010284, mae: 3.245572, mean_q: 3.903096, mean_eps: 0.100000\n",
      " 3706817/3750000: episode: 4813, duration: 7.300s, episode steps: 993, steps per second: 136, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.016534, mae: 3.212199, mean_q: 3.865045, mean_eps: 0.100000\n",
      " 3707716/3750000: episode: 4814, duration: 6.678s, episode steps: 899, steps per second: 135, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.534 [0.000, 5.000],  loss: 0.015957, mae: 3.228493, mean_q: 3.887843, mean_eps: 0.100000\n",
      " 3708386/3750000: episode: 4815, duration: 5.058s, episode steps: 670, steps per second: 132, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.167 [0.000, 5.000],  loss: 0.016964, mae: 3.299622, mean_q: 3.965652, mean_eps: 0.100000\n",
      " 3709418/3750000: episode: 4816, duration: 7.626s, episode steps: 1032, steps per second: 135, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.013195, mae: 3.273415, mean_q: 3.936797, mean_eps: 0.100000\n",
      " 3710292/3750000: episode: 4817, duration: 6.633s, episode steps: 874, steps per second: 132, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.626 [0.000, 5.000],  loss: 0.014230, mae: 3.241328, mean_q: 3.903594, mean_eps: 0.100000\n",
      " 3711265/3750000: episode: 4818, duration: 7.301s, episode steps: 973, steps per second: 133, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.017731, mae: 3.249679, mean_q: 3.906684, mean_eps: 0.100000\n",
      " 3711943/3750000: episode: 4819, duration: 5.083s, episode steps: 678, steps per second: 133, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.086 [0.000, 5.000],  loss: 0.014486, mae: 3.211029, mean_q: 3.859283, mean_eps: 0.100000\n",
      " 3713244/3750000: episode: 4820, duration: 9.723s, episode steps: 1301, steps per second: 134, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.017417, mae: 3.249188, mean_q: 3.906124, mean_eps: 0.100000\n",
      " 3714332/3750000: episode: 4821, duration: 8.023s, episode steps: 1088, steps per second: 136, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.015325, mae: 3.221760, mean_q: 3.876846, mean_eps: 0.100000\n",
      " 3714962/3750000: episode: 4822, duration: 4.769s, episode steps: 630, steps per second: 132, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.012685, mae: 3.183191, mean_q: 3.828556, mean_eps: 0.100000\n",
      " 3716094/3750000: episode: 4823, duration: 8.404s, episode steps: 1132, steps per second: 135, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.014572, mae: 3.253242, mean_q: 3.913311, mean_eps: 0.100000\n",
      " 3716931/3750000: episode: 4824, duration: 6.333s, episode steps: 837, steps per second: 132, episode reward:  6.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.849 [0.000, 5.000],  loss: 0.019434, mae: 3.190567, mean_q: 3.841630, mean_eps: 0.100000\n",
      " 3718352/3750000: episode: 4825, duration: 10.732s, episode steps: 1421, steps per second: 132, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015586, mae: 3.243778, mean_q: 3.904321, mean_eps: 0.100000\n",
      " 3718896/3750000: episode: 4826, duration: 4.093s, episode steps: 544, steps per second: 133, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.019225, mae: 3.228960, mean_q: 3.886661, mean_eps: 0.100000\n",
      " 3720153/3750000: episode: 4827, duration: 9.401s, episode steps: 1257, steps per second: 134, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.772 [0.000, 5.000],  loss: 0.017358, mae: 3.203540, mean_q: 3.864128, mean_eps: 0.100000\n",
      " 3721308/3750000: episode: 4828, duration: 8.681s, episode steps: 1155, steps per second: 133, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.016346, mae: 3.259876, mean_q: 3.923436, mean_eps: 0.100000\n",
      " 3721930/3750000: episode: 4829, duration: 4.632s, episode steps: 622, steps per second: 134, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013711, mae: 3.242368, mean_q: 3.899030, mean_eps: 0.100000\n",
      " 3722670/3750000: episode: 4830, duration: 5.528s, episode steps: 740, steps per second: 134, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.696 [0.000, 5.000],  loss: 0.010488, mae: 3.253072, mean_q: 3.914739, mean_eps: 0.100000\n",
      " 3724288/3750000: episode: 4831, duration: 12.083s, episode steps: 1618, steps per second: 134, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.016456, mae: 3.219220, mean_q: 3.871550, mean_eps: 0.100000\n",
      " 3725305/3750000: episode: 4832, duration: 7.645s, episode steps: 1017, steps per second: 133, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.646 [0.000, 5.000],  loss: 0.013727, mae: 3.241134, mean_q: 3.904254, mean_eps: 0.100000\n",
      " 3726055/3750000: episode: 4833, duration: 5.520s, episode steps: 750, steps per second: 136, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.018637, mae: 3.268365, mean_q: 3.930768, mean_eps: 0.100000\n",
      " 3726575/3750000: episode: 4834, duration: 3.859s, episode steps: 520, steps per second: 135, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 0.938 [0.000, 5.000],  loss: 0.015443, mae: 3.258316, mean_q: 3.925323, mean_eps: 0.100000\n",
      " 3727139/3750000: episode: 4835, duration: 4.318s, episode steps: 564, steps per second: 131, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.257 [0.000, 5.000],  loss: 0.018935, mae: 3.143220, mean_q: 3.778777, mean_eps: 0.100000\n",
      " 3727736/3750000: episode: 4836, duration: 4.626s, episode steps: 597, steps per second: 129, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.407 [0.000, 5.000],  loss: 0.016726, mae: 3.254430, mean_q: 3.918396, mean_eps: 0.100000\n",
      " 3728497/3750000: episode: 4837, duration: 5.682s, episode steps: 761, steps per second: 134, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 3.702 [0.000, 5.000],  loss: 0.014136, mae: 3.172949, mean_q: 3.824375, mean_eps: 0.100000\n",
      " 3729157/3750000: episode: 4838, duration: 4.936s, episode steps: 660, steps per second: 134, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.168 [0.000, 5.000],  loss: 0.011629, mae: 3.220034, mean_q: 3.873462, mean_eps: 0.100000\n",
      " 3729501/3750000: episode: 4839, duration: 2.714s, episode steps: 344, steps per second: 127, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.012728, mae: 3.169671, mean_q: 3.810610, mean_eps: 0.100000\n",
      " 3730711/3750000: episode: 4840, duration: 8.945s, episode steps: 1210, steps per second: 135, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.012067, mae: 3.199841, mean_q: 3.847591, mean_eps: 0.100000\n",
      " 3731586/3750000: episode: 4841, duration: 6.564s, episode steps: 875, steps per second: 133, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.771 [0.000, 5.000],  loss: 0.021989, mae: 3.216593, mean_q: 3.869872, mean_eps: 0.100000\n",
      " 3732031/3750000: episode: 4842, duration: 3.290s, episode steps: 445, steps per second: 135, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.015441, mae: 3.232035, mean_q: 3.886991, mean_eps: 0.100000\n",
      " 3732419/3750000: episode: 4843, duration: 2.872s, episode steps: 388, steps per second: 135, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 0.912 [0.000, 5.000],  loss: 0.014615, mae: 3.252644, mean_q: 3.910521, mean_eps: 0.100000\n",
      " 3733900/3750000: episode: 4844, duration: 11.169s, episode steps: 1481, steps per second: 133, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.016682, mae: 3.236305, mean_q: 3.891704, mean_eps: 0.100000\n",
      " 3734857/3750000: episode: 4845, duration: 7.133s, episode steps: 957, steps per second: 134, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.016393, mae: 3.207928, mean_q: 3.858488, mean_eps: 0.100000\n",
      " 3735363/3750000: episode: 4846, duration: 3.847s, episode steps: 506, steps per second: 132, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.040 [0.000, 5.000],  loss: 0.009328, mae: 3.191609, mean_q: 3.840640, mean_eps: 0.100000\n",
      " 3736657/3750000: episode: 4847, duration: 9.585s, episode steps: 1294, steps per second: 135, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.013929, mae: 3.176805, mean_q: 3.823383, mean_eps: 0.100000\n",
      " 3737339/3750000: episode: 4848, duration: 5.098s, episode steps: 682, steps per second: 134, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.015033, mae: 3.207296, mean_q: 3.866272, mean_eps: 0.100000\n",
      " 3738555/3750000: episode: 4849, duration: 9.116s, episode steps: 1216, steps per second: 133, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.015073, mae: 3.188504, mean_q: 3.836315, mean_eps: 0.100000\n",
      " 3739236/3750000: episode: 4850, duration: 5.074s, episode steps: 681, steps per second: 134, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.329 [0.000, 5.000],  loss: 0.014604, mae: 3.201569, mean_q: 3.852419, mean_eps: 0.100000\n",
      " 3740666/3750000: episode: 4851, duration: 10.769s, episode steps: 1430, steps per second: 133, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.014444, mae: 3.157355, mean_q: 3.795099, mean_eps: 0.100000\n",
      " 3741475/3750000: episode: 4852, duration: 5.976s, episode steps: 809, steps per second: 135, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.013549, mae: 3.133798, mean_q: 3.777813, mean_eps: 0.100000\n",
      " 3741919/3750000: episode: 4853, duration: 3.306s, episode steps: 444, steps per second: 134, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.525 [0.000, 5.000],  loss: 0.012489, mae: 3.151256, mean_q: 3.789683, mean_eps: 0.100000\n",
      " 3742302/3750000: episode: 4854, duration: 3.024s, episode steps: 383, steps per second: 127, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.014398, mae: 3.165362, mean_q: 3.809338, mean_eps: 0.100000\n",
      " 3743693/3750000: episode: 4855, duration: 10.454s, episode steps: 1391, steps per second: 133, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.192 [0.000, 5.000],  loss: 0.013144, mae: 3.163772, mean_q: 3.806059, mean_eps: 0.100000\n",
      " 3744237/3750000: episode: 4856, duration: 4.082s, episode steps: 544, steps per second: 133, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.241 [0.000, 5.000],  loss: 0.014644, mae: 3.195704, mean_q: 3.853522, mean_eps: 0.100000\n",
      " 3745874/3750000: episode: 4857, duration: 12.271s, episode steps: 1637, steps per second: 133, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.017199, mae: 3.185906, mean_q: 3.835406, mean_eps: 0.100000\n",
      " 3746285/3750000: episode: 4858, duration: 3.115s, episode steps: 411, steps per second: 132, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.015839, mae: 3.154653, mean_q: 3.794878, mean_eps: 0.100000\n",
      " 3746986/3750000: episode: 4859, duration: 5.284s, episode steps: 701, steps per second: 133, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.869 [0.000, 5.000],  loss: 0.011059, mae: 3.158031, mean_q: 3.798695, mean_eps: 0.100000\n",
      " 3748365/3750000: episode: 4860, duration: 10.239s, episode steps: 1379, steps per second: 135, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.012318, mae: 3.200852, mean_q: 3.853852, mean_eps: 0.100000\n",
      " 3749508/3750000: episode: 4861, duration: 8.584s, episode steps: 1143, steps per second: 133, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.711 [0.000, 5.000],  loss: 0.013897, mae: 3.180522, mean_q: 3.827455, mean_eps: 0.100000\n",
      "done, took 27955.415 seconds\n"
     ]
    }
   ],
   "source": [
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=3750000,verbose=2)\n",
    "\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 27.000, steps: 829\n",
      "Episode 2: reward: 21.000, steps: 732\n",
      "Episode 3: reward: 9.000, steps: 383\n",
      "Episode 4: reward: 18.000, steps: 625\n",
      "Episode 5: reward: 30.000, steps: 1183\n",
      "Episode 6: reward: 22.000, steps: 810\n",
      "Episode 7: reward: 21.000, steps: 807\n",
      "Episode 8: reward: 20.000, steps: 768\n",
      "Episode 9: reward: 32.000, steps: 1239\n",
      "Episode 10: reward: 18.000, steps: 790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2481934ef40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este experimento se ha entrenado el modelo durante 3.750.000 steps, de los cuales 2.500.000 han correspondido a la fase de exploración bajando epsilon desde 1 hasta 0,1. El resto de steps corresponden a la fase de explotación. El modelo se actualiza cada 1.000 steps ya que en el caso de ser con un número mayor de steps la recompensa por episodio es menor, siendo los 50.000 primeros de calentamiento. Se utiliza una memoria de 100.000 steps. Por último el factor gamma se establece en 0,99.\n",
    "\n",
    "En cuanto a la red neuronal, se utiliza una típica para los problemas de atary, con tres capas convolucionales con activación 'relu', después una capa Flatten y dos Dense, la última con activación linear.\n",
    "\n",
    "En el test del experimento se obtiene una media por encima de 20, siendo 7 test superiores y 3 inferiores. La puntuación máxima en el test es 32 y la mínima 9\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
