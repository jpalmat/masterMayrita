{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "missing-projector",
   "metadata": {},
   "source": [
    "### Cuestionario sobre vídeo teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-motorcycle",
   "metadata": {},
   "source": [
    "Esta actividad consiste en responder las siguientes preguntas teóricas relacionadas con el vídeo de *dotcsv* sobre uno de los juegos de Atari más complicados de resolver utilizando técnicas de Aprendizaje por refuerzo, *Montezuma´s Revenge*.\n",
    "\n",
    "Enlaces de interés para la actividad:\n",
    "\n",
    "Montezuma's Revenge - ¿Hito del Aprendizaje Reforzado? | Data Coffee 8: https://www.youtube.com/watch?v=DBJh4cfq0ro\n",
    "\n",
    "Uber Go Explore: https://eng.uber.com/go-explore/\n",
    "                 https://github.com/uber-research/go-explore\n",
    "\n",
    "Para otros vídeos de DotCSV, Canal de DotCSV: https://www.youtube.com/c/DotCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-plasma",
   "metadata": {},
   "source": [
    "### Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-rhythm",
   "metadata": {},
   "source": [
    "1) En Aprendizaje por refuerzo, a qué nos referimos por recompensa intrínseca?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-clerk",
   "metadata": {},
   "source": [
    "La recompensa intrínseca se refiere a que el agente inteligente no solo se encuentre recompensado cuando el entorno lo considere porque puede ser una recompensa muy dispersa, sino una recompensa por el hecho de lanzarse a explorar o tener curiosidad de encontrar nuevos estados dentro de la simulación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-attendance",
   "metadata": {},
   "source": [
    "2) En el algoritmo de Go-Explore, cómo se representa el estado? de cuántas fases se compone el algoritmo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-match",
   "metadata": {},
   "source": [
    "El estado se lo representa como que el agente primero debe recordar que hay una zona sin explorar y debe volver a visitarla y luego la selecciona para explorarla.\n",
    "El algoritmo tiene 2 fases: la exploración hasta que se resuelva y la fase de robustificación para que se haga más robusto frente a posibles perturbaciones que se puedan dar en el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-broad",
   "metadata": {},
   "source": [
    "3) Por qué genera controversia el *downsampling* utilizado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-recipe",
   "metadata": {},
   "source": [
    "Genera controversia por lo rudimentario o simple del método, ya que realizan un downgrade bajando la calidad de pixeles ya que el sistema usado para la codificación en un Atari puede que sirva reduciendo la intesindad de píxeles pero en simuladores más complejos no será eficaz ya que no es un sistema más generalizable. También han realizado varios experimentos y han introducido información extra para que el sistema pueda aprender mejor, como coordenadas, número de llaves, número de niveles, lo que podría hacer que el agente aprenda en ese sistema pero no en otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-compensation",
   "metadata": {},
   "source": [
    "4) Qué información extra se considera en el estado que se encuentra el agente? Cómo crees que puede afectar en la toma de decisiones del agente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-transcription",
   "metadata": {},
   "source": [
    "Se considera las zonas sin explorar con lo cual para la toma de decisiones del agente se debe volver al estado se debe entrenar al agente haciendo memorizar y rehacer movimientos que lo han llevado de un estado a otro, o resetear el entorno de similación del estado que quieren alcanzar sin simular todo el recorrido de vuelta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-prediction",
   "metadata": {},
   "source": [
    "5) En Aprendizaje por refuerzo, en qué consisten las *Sticky actions*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-configuration",
   "metadata": {},
   "source": [
    "Las acciones pegajosas son mecanismos que introducen una pequeña probabilidad aleatoria de que el agente inteligente vuelva a repetir la última acción que ha hecho, haciendo que el comportamiento ya no sea determinista."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
